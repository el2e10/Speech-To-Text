{"audio_filepath": "Data/Preprocessed/Hierarchical softmax_82.wav", "duration": 765.0, "text": "the next one is a bit tricky so the third solution is to use something known as h ierarchical s oftmax this is a bit counterintuitive in the sense it is a very smart trick but it is not something which is very obvious so just pay a bit attention on this it is a neat way of handling this large vocabulary th ing and this i think used in various nlp applications where speed is important not often but wherever speed is important so this is what our original network was this was the either you take it as a skip gram model or you take it as a continuous bag of words model right let us take it as a continuous bag of words model you had a word as the input and then you had this larg e prediction and you had this s oftmax computation which gives you the probability and you are try ing to maximize this probability for the correct word right where v w is the correct word now in stead of this the hierarchical s oftmax says that you construct a binary tree s uch that your tree has how many nodes v nodes  it has one node corresponding to every word ok and there exist a unique path from the root node to every leaf node every leaf node corresponds to a word and there is a unique path from the root node to leaf node of course there will be overlapping thing s for example for this word the path is these nodes and for this word also the path is like there is some overlap in the path but for every word there is a unique pat h how many if you get that set up now let lw one lw two up to lw p be the nodes on this pa th so i am calling this as lw one  lw two  lw three sorry sorry sorry sorry yeah actually it is so actually this is l on one l on two l on three that means the third node on the path of on the second node on the path of on and so on right that is how it is going to be and let pi w be a binary vector  so what is the size of pi w actually binary tree log of v right so the size of pi w vector is going to be log of v so if there are eight leaf nodes you will have three nodes as the size of the vector so for each of these things this vector takes on a value one so here the value would be one because the path branches to the left if the path branches to right then the value is going to be zero  right so for every node or every word i have t his way of uniquely defining it is path i can say that the path is one zero zero is that fine for the word on the path is one zero zero if i consider some other word the path would be different is that fine and of course i have assumed there are only eight words here right that is why this holds if there are either otherwise i would have a vector whose size is log v right now my v is eight so it is just three f inally each of these internal nodes is associated with a vector ok so i have u one u two u three so how many of these would i have if there are v nodes at the leaf how many non leaf nodes do you have in the binary tree v you all know this right so if you have v nodes at the leaf then you will have v nodes internally so for each interna l node i have a vector associated with it so how many vectors do i have in all u v and my input side is still the same right i have this w word or w context depending on whether it is a skip gram or by or continuous bag of words model so how many p arameters does this model have is it same as the bag of words model or less than the bag of words model or more than the bag of words model this is how you will think you will see how many input parameters do the poo l two models have how many output para meters to the two models are input parameters same output parameters how many vectors do you have u one to uv each of size k same as the original model right it is just as an original model i had put everything inside as w context which was k cross v  rig ht so it is the same number of parameters so the total number of parameters in the network is the same now for a given pair w comma c which is the correct pair  we are interested in the probabi lity p of w given vc nothing great about this it is the same as i have been saying always that we want the pa probability of w given c what we are going to model as w g iven vc because v c is the representation of c and we model this probability now as the following thing why does this make sense you just assume this is on and these are on k s  right so on one on two on three why does this make sense i will get the word on at the output only if the first element on the path was pi on one and the second elem ent on t he path was pi on two up to the k th element on the path was pi on k how many forget that please raise your hands ok right so that is how we are modeling it is it but what about pi on one pi on two pi on k how do you model that at least this f orm is clear to everyone right if it is not let me know because then you not understand the rest of the stuff yeah ok so now see that modeling part is always in your hands right you know that you want you are interested in a certain probability it depends on you how to model it so now what you have done is you have con constructed a binary tree now i am interested in p of on given some word vc right or some word vector vc now i can say that but the way i am thinking about this is that i get t he word on only if the first if i started from the root node the first vector took on the value one or the first branch took on the value one the second branch took on the value zero and the third branch took on the value zero so that is exactly what i am sayin g here it is a probability that the first turn that i took was a left turn then a right turn then a right turn yeah the path is you have constructed the binary tree and the path is fixed now for all the words how to construct the binary tree is a sepa rate thing but the binary tree has been constructed and every word has a unique path associated with that so that word will occur only if that path is executed right so i am just trying to find the probability of that path being executed now i need to tell you what does each so how many terms are there in this product k terms right how do i estimate each of these k terms is what i need to tell you ok  c an you think of it how would i model each of these probabilities remember that every node h as a vector associated with it how many if you can think of an answer i hope i are you saying what i think you are saying so this is what i will do so as i said for the on example this is what you want this is the path th at you want to be executed and i am going to model it as this so getting a left turn i model it using this that dot product between the original word vector which was the input word vector which was vc and the node represen tation of the node associated with that particular node does this make sense so i will tell you what we are trying to do so this path was clear that the probability is going to be a product of these probabilities now i want how do i get each of thes e probabilities so that is again in my hand right i am going to say that i am going to train my parameters vc and ui where ui is the parameter corresponding to every node i am going to train it in a such a way that whenever i want this to take on the value one this should be close to one ok  b ecause i will set up my loss function accordingly we will see the loss function but i am saying that whenever i want the probability to be equal to one i am going to use this to computed and alternately when i wa nt the probability to be zero i am going to take one minus that which is just this is that fine okay let us go ahead a bit and then we will come back if you are still lost so what does this actually ensure this ensures that the representation of a contex t word vc will have a very high similarity with the node ui if the path takes a left turn there and it will have a very low similarity with the node ui if the path takes a right turn their how many if you get this part based on if you assume that this i s how we are going to model it when is this going to be high when the dot product between vc and ui is high when is this going to be low when the dot product between these two is low right there is a negative yeah so we ok sorry i or rather when is this going to be low right so you get that so it is coming so the word representation which is vc which is this guy would come to the come close to all these representations or move away from them depending on whether you want to take a left t urn there or a right turn there now what would happen to words which appear in similar context the same thing that we have been discussing so far right t hey will come close to the node representations which are along the path right is that fine so this is the context representation right this is actually you are representing every context word by these three representations now if a word appears in the same context it is representation is going to either come close or move away from these representations  right so words appear in the same context if you have cat and you had sleep here then cat has to come close to this it has to move away from this and it has to move away from this is that clear that is how we have set up the probabilities now i nstead if i had dog and again you had the context word as sleep now the representation of dog also has to go close to this it has to move away from this and it has to move away from this so in effect again the same thing is happening that the represen tation of cat and dog are moving in the same directions so they will eventually come close to each other how many if you get this intuition and how many computations do you need now to compute the probability of this so ea rlier you acquired that complex s oftmax computation how many computations do you need now you definitely need these many computations and each of these computations require s a sigmoid over or dot product  right so that is much much lesser than so you just need these many dot products as compared to your expensive s oftmax computation earlier so you see how you get the savings using the hierarchical s oftmax so this is as i said this is not very intuitive it is like a really smart trick and it takes time to get your head around it but i am sure if you go back and look at the slides you will get it right if it is if you have just got fifty percent of the idea here that is typically how it happens every time but and i probably not figured out a better way of teaching this but once you go back i am pretty sure that you will get to unde rstand what is happening so now the question is how do we construct a binary tree anyone has any thoughts on that do we need to ensure certain things while constructin g the binary tree okay i will ask this as a quiz question just note that there is some subtlety here ah in practice this is what is done you just randomly arrange the nodes on the leaf nodes and then you just construct a binary tree from there  righ t so you have distributed all your leaf nodes randomly and on top of that you have constructed a binary tree my question is there a problem in doing that which i will ask you on"}
{"audio_filepath": "Data/Preprocessed/Attention Mechanism (Contd.)_115.wav", "duration": 113.0, "text": "so let us start s l ast lecture we are looking at encoder decoder mo dels and we saw that a bunch of problems from different domains and different modalities images text videos and so on and even this cross modal or multi modal applications where you are taking a video and trying to describe it so video is one modality  description texts is another modality and so on we were able to propose modals for all of these using this encoder decoder architecture and then we motivated this attention mechanism where we said that encoder decoder is trying to do this silly thin g where it tries to encode the entire input once and that is what how humans do it he do this back and forth thing where at every time step if we are trying to produce a translation or a single word in the translation we just focus on certain words in the input sentence and kind of ignore the other so the attention mechanism which is this bunch of equations that you see here that allowed you a neural way of modelling attention and the key thing to note here is a there was a supervision for the attention no one actually tells us that this is the portion of the text which is important at time step t but they still works better because this is the better modelling choice and i give you that bicycle analogy and also it is a better modelling choice we are ab le to no one has given you these supervisions but you are still have more parameters in the model to learn this kind of a behaviour and then we also saw that we could actually visualise these attention based and from some exp eriments on some papers we saw that actually learn some meaningful attentions in the particular case on the figure on the on the right hand side so the one that clearly shows that for a monotonic kind of a translation scenario between english and frenc h most of the attentions weights are along a diagram and that is exactly what you would expect right so that is where we end it"}
{"audio_filepath": "Data/Preprocessed/Deep Dream_100.wav", "duration": 634.0, "text": "so we will startso we werein the threerd lectureon cnnswherewe werelookingat different visualization tools for understanding what your convolutional neural network is learning and we did a bunch of things and now you move on to the next module where we talk aboutsomethingknown asdeepdreamveryinterestinglytitledbut i am sure most of you have already seen this or read about this so here is the idea right so far we were seeing that if we start from a blank image then we could suitably modify it by constructing an optimization problem whose parameters are the pixels o f the image and we can modify the image so that it starts looking like a certain class of interest right but now suppose instead of starting with a blank image i start with a natural image right say a sky or any image that you have in your dataset i s tart with this and then i focus on neurons in some layer of the convolutional neural network i am focusing on these neurons say any one of these neurons i am focusing on and i want to change the image so that these neurons so when i say neurons i actuall y mean only a single neuron but for illustration i will show multiple neurons so i want to change the image so that this neuron fires even more so how would we achieve this what will we do so say this is the neuron which i want to fire even more so what is my optimization problem first of all what are the parameters of the optimization problem student refer time onefiftyone the pixels of the image that is clear now i want this to fire even more so what is the objective function what you are going to maximize lets call this neuron as h ij  what you are going to maximize student refer time twoeighteen sorry student no refer time twotwentyone no i want this neuron to fire more student refer time twotwentyseven maximize hij right i mean that is i mean why so that sort of a thing ok but of course we will do something so that it is a neat differentiable thing and so on so you want to maximize the activation of one such neuron h ij  so we could just formulate the following optimization problem t hat i want to maximize h ij two  ok and of course the parameters of the optimization are the image pixels and if i consider one such pixel in the image then i essentially need to compute this gradient the gradient of the loss function which is h ij two with re spect to this image pixel and i can do it in these two parts the lead ability of the loss function with respect to h ij and the derivative of the h ij with respect to the image pixel this we have seen a million times while doing back propagation of course you are not gone all the way back to i mn but we saw last time that it is just one more term in the chain rule and this again looks straightforward right the derivative of the loss function with respect to h ij looks straight forward so i have a very si mple way of computing the derivative of the loss function with respect to any pixel of the image so now i can apply gradient descent and i can update the image so i started and now remember that the my original i mn was no t blank or random or zero or anything it is actually the sky image so maybe it was blue or cloudy or whatever pixel that i have in my original image and that pixel i am changing so i have started with the sky image i have changed a bit based on this grad ient update rule gradient descent update rule and now i feed it back to the network what will happen what will happen to h ij student fire a bit it will fire a bit more because that is exactly how you have changed the image with exactly that objecti ve function right and now if i keep doing this what will happen so remember what does h ij actually capture now this is where so if you understand this right you will really understand and appreciate everything about convolutional neural network and i will be sure that you are actually understood the details and not just these boxed architectures right so what if this happens right then what does actually h ij capture it captures certain student patterns patterns in the imag e right now if h ij i s firing that means these patterns have started student refer time fourfiftyseven appearing in the image we started with a sky image and now hij is firing more and more that means it is now the image is suddenly becoming more and more or containing more and more patterns for which h ij should fire does that make sense ok yeah so let us run this algorithm we will start with this image and we will run this algorithm so i will run it before that i want some guesses what kind of patterns do you think will start appearing here and this is deep dream is the title right ok so fine so let us see so i will run this algorithm so what i am doing is i am starting with this image and running exactly what i showed you that i will compute the gradient with res pect to one of the neurons and i will keep updating the image so that it becomes more and more like the patterns that i am trying to capture so lets run this and observe carefully it is almost a magic trick i hope this does not disappoint what do you see student refer time fivefiftyone most of them are what student refer time fivefiftythree they are dreaming so they are literally building castles in the air right so what is happening why is this happening everyone sees castles right that is the first thing otherwise student laughter ok good why is this happening have you seen the disney logo the castle what does it have in back background how many of you find this interesting how many think this is ok expected ok why is this happening th ink about training data think about what would have happen or you missed the magic show so what is the convolutional neural network actually trying to do student refer time sixthirtytwo i will give you a hint its being over enthusiastic how many of you get that ok so here is what is happening right should i explain it or no i am not going to ask you a quiz question i am just saying that i have some more images to show ok i will explain it first so this is what is happening right so in the trainin g data whenever the castle appears it is typically has the sky as the background ok so now the convolutional neural network started drawing these correlations so whenever it sees a sky it is trying to find a castle somewhere but because it knows that m ost of the times whenever i see a sky there is a castle in the foreground so those neurons are firing a bit and then now you are trying to fire them even more and more so that keep trying to change the image till this castle actually appears on the ima ge how do you how many if you get this explanation please raise your hands ok so let us see some more examples right so now guess what will happen here ships ok again a generation which thinks of student refer time seventhirtyeight a ships is ok i sh ouldnt comment on that student birds fishesrefer time sevenfortythree student birds birds what else but there are also mountains studentrefer time sevenfortyseven ice ok interesting studentrefer time sevenfifty now our expectations are increase l et me just run this and see what happens oops oh no student refer time sevenfiftysix i have my final trick ok student laughter the prestige is gone ok yeah so what do you see here so actually if you go back and look at it carefully right this is ve ry interesting a lot of fish eyes actually start appearing here and some shapes like fishes actually start appearing here go back and look at it carefully and all on the mountains and the green regions a lot of birds and animals start appearing right whic h is again expected because in your data set you would have seen birds and animals with a green or this kind of a background right whatever you call it a mix of green and brown background right so now it is trying to find those things even though they d o not exist and as it try to force it more and more it starts creating those images as you start asking to dream more and more right and since this is about dreams i could not let this go it has to had inception in that so what will happen here now stud ent refer time eightfiftyone there is actually nothing interesting is this for my own sake that i put this unfortunately nothing interesting happened with this student oscars wow student laughter if only but thats the point right this is so data se t specific that it cant really generalize it cannot dream beyond the data set actually nothing interesting happens it is just a lot of these men are wearing brown suits and in the data set unfortunately all brown things were dogs laughter so this is w hat will happen we will start seeing dogs appear everywhere you see one here student laughter you see many here actually student refer time ninethirtyone it is like a few more and this would have turned into laughter something unpleasant right so that is what is happening actually see a lot of dogs here here in many places right so this is its still running so what exactly is happening here the same thing that i had detected right the network has been trained to detect certain patterns dogs cats birds etcetera which appear frequently in the imagenet data and with these backgrounds that i am trying to do or these textures that i have in my images it starts seeing these patterns even when they hardly exist and now as i start focusing on thes e neurons which are firing and try to modify the image to make them fire even more it will start producing these pixels or these images in the original image right so you can read this explanation which is from the google blog on this they have some rea lly some code and something on this so you can just read this explanation if a cloud looks a little bit like a bird so that will make it look more like a bird this in turn will make the network recognize the bird even more strongly on the next pass an d so forth until out of nowhere a bird actually starts appearing in the image right so that is exactly what is happening so this is deep dream"}
{"audio_filepath": "Data/Preprocessed/Adding Noise to the outputs_65.wav", "duration": 246.0, "text": "so now going on to the next module which is adding noise to the outputs re fer slide time zeronineteen so here when you are given some training data this is the label vector that you have been given right where one of these elements is one so  these are like zero to nine eight where which digit it is and in this case it happens to be digi t two so that element is one right that is the true training data given to you so what you could do is actually and actually what you try to do is minimize this quantity p i log q i where what is p i p i is the vector which w as given and what is q i the predicted probabilities ok so now when you try to add noise to the output what you actually do is you see that i do not trust the true labels they may be noisy whatever data you have given to me that is one way of look ing at it that i do not trust it i will just say that it is noisy the other way of looking at it is that in some way i am ensuring that i do not try to over fit to this label right because now my true whatever i am trying to optimize let me just go to that and let us see so instead what we will do is we will use soft targets so this is what i mean by soft target assume that there was some epsilon noise in your labels  so  instead of treating this as one and all zeros trea t the true label as one minus epsilon and divide that among the remaining nine entities right that probability mass divided among the remaining nine entities so now when you are trying to minimize this what is p i this soft distribution right and q i is the pre dicted distribution so you see why this acts as a regularization why does it act as a regularization what is the aim of regularization do not over fit on the training data right to over fit on the training data what should it have done it should ha ve treated only the correct label now if i am giving it this information then i am not allowing it to over fit on the training data right because now with this distribution this quantity will not get minimized when q i is equal to the one hour distributi on where all the masses on two do you get that so in some sense we are making sure that now if it tries to over fit on the training data it will not get the minimized error right so you have this corrupted the outputs of it everyone gets this is ok the trainer no that is the whole point student refer time twoforty no so that is thing right s o  some of these are heuristics based so now we have started with this whole derivation where we try to show the relations between trainer error tested o r not but things that we have seen some of these things right even whatever unfortunately i tried to prove on the previous slide the weight decay thing even that is only for these neat networks where you do not have any hidden layer and so on right so  most of these are just heuristics you are just saying that the principle is that you will not allow the true training error as computed from the training data to go to zero if you do that you know that you are going to over fit so try whatever you can to avoid that ok that is the idea do you agree that doing this is going in that direction student refer time threetwentyfive training data the hope is that if you do not do that then it will not under fit on the test it right there is no i mean i have you are you looking for a proof where i say that doing this we will ensure that a training error does not go to zero but the test error comes close to the training error there is no such proof right just a heuristic it is going by the principle that if i do not allow the training error to go to zero then hopefully i will over fit i will not over it as much as i would have otherwise right so that you can think of it as this way right so  this is the curve that you are seeing it this was a training curve this was your test curve you are preventing from entering this region where the error is zero that means you will end up somewhere here right and you know that that is a more preferred point as compared to this that is the intuition that you are going ri ght is that"}
{"audio_filepath": "Data/Preprocessed/Backpropagation through time_105.wav", "duration": 819.0, "text": "so that was recurrent neural networks now whenever we propose a network what do we do nexttrainingrightso whatwe will look atit backpropagationthroughtime this is not the tit le of a ficti on and movie or anythingthis is an algorithm that we will see so before we proceed right let us look at the dimension of the parameters that we have and i expect you to tell me the dimensions so i will define somethings for you which are very hard so x i belongs to r n  so let us be clear about that s i belongs to r d that means the s i is a d dimensional vecto r and y i belongs to r k which has k classes ok so now what is u what is v d   k is it d   k i am asking soham now i mean we have be written it as d   k and w is student refer time onefive d  t sure everyone sure ok right so these are the dimensions why am i talking about these dimensions whenever we talk about gradients what we talk about partial derivatives or gradient or something we need to know what is the size of the parameter with respect to which we are taking the gradient because that is what the size of the gradient matrix is going to be right that is why i am asking you to focus on this now how do we train this network title of the module student backpropagation backpropagation ok how why do i have a module if i am only going to tell you about backpropagation do you see any problem with this why cannot you just apply the standard backpr opagation algorithm ok so we will try to understand this with the help of a concrete example and we will go back to our example of predicting characters ok so this is the auto completion task and for simplicity we will as sume that english has only these three characters d e p and then a stop to indicate that the word has been completed ok this is what you are going to consider that my vocabulary size is just four that means i can only predict one of these k four classes k is e qual to four ok and at each time say i want to predict one of these things what is the suitable output function for this task can everyone say with probability ninetyninenine percent student half max half max ok what is the suitable loss function for this tas k small pleasures in life that is all i get ok suppose we initialize u v w randomly and networks predicts the following probabilities ok so let us understand what is happening i fed it d as the input i have just started training so my u w and v are all some randomly initialized weight matrices right now and so it has predicted this as my probability distribution this is the predictions that i have got from the network and i also know what is the true probability distribution what is the true probability distribution for the first time step zero one zero zero and so on right you can see it second times that is also zero one zero zero third is zero zero one zero and the last one should have been zero zero so given the situation and before i talk about learning algorithms what is the first thing that i need to define objective function right so what is the objective function here how many errors do i have i mean i can make my errors at four places whether i making an error or not is the separa te case but i can have four loss functions so then these are the two questions that i am interested in what is the total loss made by the model and how do we back propagate this loss and update the parameters of the model as usual i am ignoring the biases which is w u and v so we can answer these two questions then we are done right if you can do this then we are done so the total loss what is the total loss actually take a guess sum of all the loss right good so jus t going to be the sum of the loss over the times steps that you are i mean very logical and what else would it be and we know that the loss at every time step is so this is the loss at time step t hence y t and what is c actually the true class at time step t right so it is would be e at first time step e at second time step then p and then stop ok so that what c is so this is we all comfortable with is this is the cross into p loss and i am going to sum at over all the t time setup that i have  now for back propagation what we need is we need to be able to compute the gradient of this loss function with respect to w u v if i give you your formula for the gradient the rest is straight forward you will just apply gradient as well ok so let us look at each of these parameters we will look at the easy one first which is v so what is the derivative of the lost function with respect to v have you ever done this in life student yes yes when student refer t ime fourfiftyfive now i am asking the date ok so you have done this when you doing backpropagation this is the gradient of the loss function with respect to the weights in the output layer and we know how to do that right that is very straightforward and there is no complication there and you will see what i mean by complication later on so all i need to do is take this loss function and compute its gradient with respect to v it is very simple chain rule which i can update there apply there and i can compute it separately for all these guys and i can just sum it up right so this is the easy part this is very straight forward so where one parameter we are all set we know how to do that right we can just add up all these gradients the some los e notation here this is actually an addition of four matrices right each of this i hope is a matrix is that a matrix or a scalar or a vector or a tensor student matrix matrix ok so we have already seen how to do this back propagation and this is a s mallest chain possible in the back propagation and we have enough confidence in doing this now let us considered the derivative of the loss function with respect to w just take a minute and see if it is complicated or if it is straight forward to see a lot of ws in the figure ok so let us see how to do that right so again the loss with respect to w or the derivative with respect to the loss derivative of the loss with respect to w is going to just be the sum of these four or t derivatives and by changed of derivatives we can just sum the derivative across all the paths which lead from the loss function to w is that fine right whenever you want to compute the derivative of the loss function with respect to any paramet er a recipes to look at all the paths which go from the loss function to that parameter and some of the gradients across those paths how many if have fine with this what are the paths which are actually connecting the loss function to w student refer time sixfiftyone there will be t paths good so  let us see we will consider l four   this is the last time step so l four  actually depends on s four  s four depends on what w and s three s three depends on what w and s two  s two depends on what an s one depends on w and s zero  always assume there is s zero what kind of a network is this what kind of a function is this what did i ask to revise this is not an order derivative what kind of function is this so we have an ordered network with i will give it to you and it is not be to say in an ordered network each state is computed one at a time right so we will first compute s one then we will compute s two because s two depend on s one there i s no other way we can compute s two  then s three  s four and then finally the loss function so now we have the following situation that the derivative of l four  with respect to w can be written using this chain rule w hich is the derivative with respect to s four and then the derivative of s four with respect to w and that is that looks manageable there is nothing fancy here or is it i see a lot if people that looks manageable right everyone is not refer time eightone s tudent refer time eightone even though you have done the assignment everyone is not even though you have revise the assignment everyone is not refer time eightsix so this part we have already seen this is not the tricky part l four  s four is straight forward because it only depends on this v and so its fine that part we have seen this is same as computing the gradient of the loss function with respect to the hidden layer but now let look at the derivative of s four with respect to w what is s four actually four ws b    so now if i want to compute  s four by let me just remove the sigma right i mean we can always get back the nonlinearity so i want to compute  s four  w  so it will just be s three  s three is again student depend on w depend on w right so that is the problem with an ordered network in such an ordered network you cannot compute the gradient of a s four wi th respect to w assuming that s three is a constant s three is not a constant its again a function of w and w is the parameter with respect to a computing the derivative right that is the problem here so in such networks the total derivative has two parts wha t are these two parts okay how many if you have revise this what are the two parts called explicit and i mean at least your language model should be fine at explicit and what else can it be think on at least have that much smartness either you do n ot read its fine so that is going to be explicit and implicit what do we do in the explicit case if you can read the slide we treat all the other inputs as constant right an implicit is summing ove r all the indirect paths from s four to w so let us ac tually try to derive this whole thing right so this is what the total derivative looks like all of you are comfortable with this right i mean this is all we have done this in the assignments i will not go into the theory a nd all that you should be comfortable if you have not revised you have to be blamed sorry for that but i cannot go into the details of that but i still derive the whole thing so this is what it looks like the plus here indicates that we are going to treat everything else as a constant and just take the derivative with respect to w and then the implicit part would be this we are going to sum across all the paths so this is a path ok now here again we have a total derivative  s three  w  so what am i going to do for that again explicit and implicit again i have this  s two by two by  w which is again explicit plus implicit again  s one  w is that fine and then this is finite because s one does depends on s zero which has no connection to w so this is what your entire formula looks like now this sum slide abuse of notation here because what is each o f these actually scalar vector matrix student refer time tenfortyone s four is student s four is vector vector w is student matrix matrix the derivative of a vector with respect to a matrix is student tensor tensor you cannot do this in your head is it these three sentences is one after the other ok so for simplicity what i am going to do is i am going to short circuit some of these paths right so let us i will just tell you what i am going to short circuit so i am going to write just for eas e of coming up with the generic formula the first term i am going to write as this and this is fair because this is just one right the second term also is fine the third term i am going to short circuit this path i am just going to write as  s four  s two and then  s two  w  and again i am going to short circuit these paths and just write it as  s four  s one and then this the reason i am doing this then i can write it as a very simple s ummation where i have s four by s k  where k goes from one two three four and then i just ha ve the explicit derivative of s k with respect to w just stare at this for the minute and not a minute actually just ten second or something if you have any problems with this let me know i will use my standard trick if you do not understand this you will not understand anything afterwards no one is falling for that ok everyone is comfortable with this ok so we have a formula for  s four  w  and we have dealt with the tricky situation where we have these multiple paths in an ordered network and hence we are to split into explicit and implicit derivatives so we have done all that refer time twelvetwenty math and you have come up with the simplified formula for this ok so finally this is what we have you noting it down right student refer time twelvetwentyeight laughe r i do not see you noting it down ok so now let us look at  s four  w  that is exactly what we have derived on the previous slide and that was a summation of t terms and for us t is e qual to four ok an d in general l t by the this was for l four so in general if i want to do l t then it is going to be this which i am replaced by t and this which i have replaced by this formula everyone is fine with this what were this means everyone is fine with this form ula right this is generic formula with respect to any time step the only thing is that on the previous slide w e are derived with respect to s four  now i have just come up with the generic formula ok so this algorithm is called backpropagation through ti me because now we have taken care of this ordered network and you have a way of computing this gradient once you have this gradient your life is simple because now we can just supply the gradient descent update ok so we have dealt with v we have dealt with w and as the name suggest who will deal with u you ok fine so you will to find out what it is for u ok by its going to be something very similar and i do not want to do it because that is not i mean going to be something very similar you can d o it on your own but i want to focus on something which is important"}
{"audio_filepath": "Data/Preprocessed/Dataset augmentation_62.wav", "duration": 338.0, "text": "so how with that heavy math i will just interline with this something very simple which is something known as dataset augmentation refer slide time zerotwentyone so what is dataset augmentation mean so you always given some training data  so in the case of mnist you had this training data where you are given these digits images o f digits and you wanted to train some classifier  so in dataset augmentation what we do is  so now we have what is happening here right conceptual is that there some seeing some training data and try to build a classifier and what you doing actuall y is minimizing the empi rical train error that mean it will ensure that whatever you have seen in training is going to look it is going to be perfectly classified whatever we have seen in training that is going to look very good it is going to be the t raining error o n that is the error of those training examples  it i s going to be very easy now my question is this if a training time you are see ing all this two s which are roughly vertically drawn right and a test time you se e at two which is written l ike this which is slightly tilted what would happen it will not be able to do a g ood job on that  that means your model is not think of terms that you have used in this lecture not generalizing c an you think of a simple trick based on your domain know ledge of how people right digits to kind of overcome overcome this problem you get the question right i am telling you that it i s possible that someone writes to in a very tilted manner can you prepare for eventuality eventuality the title of this modul e was dataset augmentation  so what would happen is are given some training data you can always generate for training data from that see here is an other training instant that i have created i have just rotate it to two by some random angle i took this image i just rotate it and this is a simple operation that all pixels are moving by a certain angle i c ould have rotate it more i could have shifted it vertically that means in all my image the two was actually exactly at the centre i just shifted at a bit vertically so i am  so think that you are reading one of those kyc forms or bank forms most people would write at the center of the block provided  but some people could write to the extreme right or extreme left right  so you are preparing for that they saying that ok all my data the digits are well written at the centre  but let me just shift them bit so that i can also deal with people who write it at the corner l eft align or right align instead of center align i could have even shift i t horizontally most people would write at the center  but some people would write at the top or at the bottom i could blur the image  but someone has taken a photo and send it to me and the photo is not very clear or i could just change some pixels rand omly right i could add noise all of this is dataset augmentation with the hope that i am capturing with these variations i am capturing enough variations in the data so that i have a better chance of doing something better on the test data is that f ine this is all still training data mind you i am still going to compute the empherical train error it is just that now i have blown up my data  but much more than what i had initially do you all see by doing this you could have done better on the mnis t assignment you could have done better again i am not asking you to do this so now i will do this then i will have supervised data because i know that by this small variations the label i s not going to change and what am i using there i am usin g my domain knowledge right i cannot do this always right i hope you appreciated that suppose that changes the domain a bit and i am given images of defects of motor parts right where i have taken a image and there is a black spot somewhere which indi cates defect i cannot go about doing the same thing there i cannot change some other pixels it will just means that the defects is at a different location right  but in many cases you can do that so if you are given picture because of dogs and cats because the entire world case about classifying cats and dogs then you could do some rotation s you could blur them a bit you could occlude certain questions of the picture and s o on and still generate training data right a nd what you are trying to do is trying to take care of cases that you would end up dealing at test refer time fourtwentyseven  right is that clear ok and please be aware that we are exp l o i ting some domain knowledge here refer slide time fourthirtytwo typically more data is better learning w orks well for image classification in object rec o gnition these are the task where this is already been tried out and they have shown to work very well in the se tasks also shown to work well for speech where the people have some speech training data the y try to augment it for some task it may not be very easy to generate such data right so you could think of various nlp applications whereas given you a data document right because always do what joe does in that friends episode do you remember what i am talking you see what i am talking about see you wants to write a recommendation letter for monika and chandler ok and he has a letter written any replaces every word by it s best synonyms from the thesaurus refer time fivetwenty  right that says a w ay of generating noisy data and in that case it was actually noisy right so you could think of doing here  but as happened in that case it will not result in very good transformation n ext for example i remember something right they are very warm he arted people got translated as they have some war m cardiograph or something like that which do not make sense  so it i s not very easy in almost all application should do it  but in some application s typically in vision application this is easy to do an d you would gain a lot by doing this right"}
{"audio_filepath": "Data/Preprocessed/Deep Art_101.wav", "duration": 304.0, "text": "ok now we will go to deep art now here any questions on that ok so now here is what here is a again an iq test right so what will happen ok so this is deep art ok someone wanted to try this that if you take natural images or camera images and if you have art from various famous artists and i want to render this original image in this art form and how can i do so i will explain this the bit of a leap of faith in what is happening here but just indulge me right so let us see so to design a network which can actually do this we design we first define two quantities one is the content targets so i call this image as the content image because this is the content that you are interested in right i want my final content to look like this for the content we would want the following thing that i f i am able to create a new image when i pass it through the same convolutional neural network we want these hidden representations to be equal right because that is the assumption here is that the hidden representation actually captured the essence of the image which is this face and it is various attributes right so if i create a new image in a different style still this content should be present in it and my way of ensuring that or rather the way of the authors way of ensuring this was to make sure that the embeddings that i learn for the new image and the original image are the same ok so i want these to be equal and i have just shown one for illustration but you could have the same objective function for all the representations right rem ember that we learn multiple representations and a convolutional neural network so this is what my objective function would be for the content i would want that this tensor which is the volume ijk every pixel or every feature value in the tensor for th e original image should be the same as the generated image ok and again my optimization problem is with respect to what image i am going to change the image and this is the loss function that i am interested in is that fine ok fine so i think x is m y original image and p is the new image which i am going to create right now next and here is where there is a bit of leap of faith we want the style of the generated image to be the same as the style image so i gave you one content image and one style image so for content the loss function is clear now for style how do you capture the style of the image so the explanation given here and i am not very sure about this but maybe it comes from some traditional computer vis ion literature but i just take it on faith that if you have this volume here which is say sixtyfour  two hundred and fiftysix  two hundred and fiftysix or any other dimension right then v t v which is a sixtyfour  sixtyfour dimensiona l image or matrix captures the style of the image so this is what has been written in the original paper i am not really dug deep but my feeling is it comes from some of the traditional literature from computer vision right so that is not important we will just take it for granted that that gives the image and here is the illustration for that as you go deeper and deeper so this is if you plot the sixtyfour cross sixtyfour image that you got then you get different styles as you go deeper and deeper you get a b etter representation of the style of the original image right so that is the argument made in the original paper now if you assume that this is correct then can you design a loss function for the style part of it i want the style of the created image to be the same as the style of the style image so how would i do that so this is the content image this is the actual oh sorry this is a style image correction ok so i would just want that this v t v which captures the s tyle and i could do it for any one of the layers or all layers depending on what i want to do i just want that this style should be as close to each other so i can have a similar matrix squared error kind of a function right so that is what this is try ing to capture these are the style gram so this is v t v for the style image and this is v t v for the generated image if i pass it through the convolutional neural network i want both of these to match so i want the conte nt to match i want the style to match so then what is my total objective function going to be student sum of these sum of these right so this is what my total objective function is going to be i want the content to match and i also want the style to match so i will use an objective function which tries to balance between these two and alpha and beta are some hyper parameters ok and if you do this and train the algorithm and try to modify the pixels along with some other bunch of tricks then you wil l get this gandalf rendered in this style that you have given right so this is again some code is available for this you can go and try it out and it is interesting it is in a very interesting idea that you could have taken these two things and now you could be imaginative right you could do all sorts of things with if you have two different images how do you want to combine them and so on right so that is the basic key idea here"}
{"audio_filepath": "Data/Preprocessed/Linear Algebra : Basic Definitions_43.wav", "duration": 633.0, "text": "nowfrom here on we will go on to somethin g even more ba sic we will star t defining some ba sic definiti ons fr om linea r algebra a nd these are aga in importa nt for something that i need in the next lecture so let us start with this i mean in the process we all just see why the eigen vectors ar e important for us in this course so how many of you know what a basis is so a set of vectors belonging to r n is called a basis if they are linearly independent right and every vector in r n can be expressed as a linear combination of these vectors so a set of n vectors vone to vn is linearly independent if no vector in the set can be expressed as a linear combination of the remaining n minus one vector  so a more weird we are stating it that so that everyone get confus e is that if you take this linear combination the only solution to this is all the cis is equal to zero and that make perfect sense right that is that same as that linear combination linear independence and all that thus is make sense to everyone ok so what does linear independence mean that any vector from this set cannot be expressed as the linear combination of the other set other vectors in the set and a more formal way of saying that is this everyone gets this what is linear independence refer slide time onethirtyfive now let us consider some very stupid examples again the space r two and we consider these two vectors one zero and zero one are they linearly independent yes ok they cannot be expressed as a multiple of each other right now any vector ab bel onging to r square can be expressed as a linear combination of these two vectors ok and x and y are linearly independent the only solution is c one x plus oh sorry c one x plus c two y is c one and y equal to zero what about if i move to r three one zero zero zero one zero and zero zero one  s o x y and z axis right are the unit vector so in that x and y turns to be unit vectors in the direction of the coordinate axis and we are used to representing every point in r two as a linear combination of these two vector is that exactly what i what we do so when we say that i have a point two co m ma three i am actually telling you that the point is two one zero plus three zero one right i am expressing at are the linear combination of the coordinate axis but now th is nothing sacrosanct about x and y right i could have cho sen just about any other axis  so in particular we could have chosen this as our basis are these two vectors linearly independent can any vector and r two be expressed as a linear combination of the se two vectors sure so i give you a vector a b how do you going to express it as a linear combination of these two vectors so you will do it this way right how will you find that values are the x one and x two so  other linear system o f linear equations right so this is what you will do i know all are good in doing this and what do we actually do when we do this what is the algorithm that we use how do we solve this what is the algorithm that you use solving this studen t gaussian elimination gaussian elimination right in two variables of course we do not call it an algorithm that is what we did in eight standard or something but when we come to engineering we call it gaussian elimination right so the same algorithm ref er slide time threetwentynine so in general given a set of linearly independent vectors we can express any vector that belonging to rn as a linear combination of these vectors right i can say z is equal to alpha one u one plus alpha two u two and so on given alpha one to alpha n are linearly independent ok so that means any vector in rn can be expressed using these vectors which form the bas is of rn does that make sense  that is why call the basis vector because anything else these are the fundamental vectors using these anything else can be expressed i n that space it is that clear so this is how it will be how do i write this in matrix notation a there are lot of these and these thing i do not really understand what you mean by that  yeah good so this is what you mean so that we writing same in matrix notation and now this is again a dash a system of linear equation there was a lot of space to fill and one dash good so system of linear equation and again you can solve them using student gaussian eliminat ion gaussian elimination what is the complexity of gaussian elimination let us see options right n n square n cube fl n cube right the gaussian elimination the complexity is o n cube right and i am not doing all this just to the sake of time pass i h ave a point of make which i will make on the next slide right so now this was for any basis that means if you have any n linear independent vectors now i will consider a special basis where instead of n linearly independent vectors in addition thes e vectors are also orthogonal ok orthogonal vectors are linearly independent ok so a set of orthogonal vectors are linearly independent but the co nverse is not all this right so now let us see what if we have an orthonorma l basis that means a basis consisting of orthonormal vectors so orthonormal is combination of two words ortho means the two vectors are orthogonal and normal means all the vectors are unit vectors that means i am normalized them by their magnitude so what is the condition that holds ui transpose uj is equal to zero if i is not equal to j and ui transpose ui is equal to one ok now what happens in this special case so we have this again we can express any vectors z as a linear combination of these n ow l et me try to do this i am just pre multiplying this equation by u one transpose what happens on the right hand side everything disappears all of the this terms will disappear because they are of the form ui transpose uj where i s not equal to j and the fir st term is student one one so what remains alpha one so you can directly find alpha one using a dot product of two vectors what is the co mplexity of this operation n th is just n products ok now how many such alphas do we need to find student refer ti me sixnine n of those so what is the complexity n square so that is now you see why an orthonormal basis is a very convenient basis you can get all these coefficients just by doing a dot product between two vectors and later on i will show you that you might not need all of these you might just need some subset k of these right so that means you just do k of these dot products and get these values so do you now understand the meaning of what is why why do i say it is orthonormal basis is the most c onvenient basi s that you can hope for right so the another way of looking it right at it is again just to make few more comfortable with vectors and projections and so on right so this was your original point z one z which is a comma b right and how do you actually draw that vector that this is a and this is b ok so how do you find the coordinates actually you projects on to your basis vectors which were these x and y vectors that is how you found the components along those the coefficient along tho se now instead of this x and y i have any other set of vectors which is u one and u two and i will do the same thing i will project this on to uone ok i will project this on to u two and that projection will give me alpha one and alpha two right so now what is a lpha one and that sense this is z this is alpha one and this is theta right so alpha one is equal to z into cos theta ok and what this cos theta so again you arrive at the same thing fine so essentially taking a projection of a vector on to your basis is this f ine to everyone t here is just to difference arriving at the same formula that alpha is are given by a dot product between the basis vector and your original vector so an orthogonal basis is the more convenient basis t hat you can hope for that is the point which i wanted to have you are convinced about that now but what does any of this have to do with eigenvectors  i started off with eigen vectors i proved one property there and then i c ame to this linear algebra basic definitions and what a basis is set of linear independent vectors and i eventually showed you that an orthonormal basis is the most convenient basis that you can hope so what does any of this have to do with eigen vector student refer time eightfifty always for us square symmetric matrix right why do you care about square symmetric matrix not sure yet so we get to that  so first of all it is turns out the eigen vectors can form a basis and this is for any matrix so  the eigenvectors of a matrix having distinct eigen va lues are linearly independent so does every matrix if i have an n cross n matrix will it have n eigen vectors no it can have less than or equal to eigen vector depending on the refer time ninefifteen so  what is this saying is that if these eigen vectors are having distinct eigen values ok then these eigen vectors would be linearly independent fine ok and turns out that for a square symmetric matrix that is the even more special the eigen vector of a squa re symmetric matrix are student orthogonal orthogonal right and we already know that orthogonal is good right so remember when we have orthogonal we do not really care about orthonormal  because that is it is a simple operation if you have a set of vectors u one u two u three which are orthogonal you can just divide them by the magnitudes and just get a set of orthonormal vectors right so orthogonal and orthonormal i will use it interchangeably ok and whatever i done thus they form a very convenient b asis so the eigen vectors of a square symmetric matrix form a very convenient basis so that is how i connect the parts which was about the eigen vectors to the second part which was about basis and why would we want to do th is and we already we had a coordinate axis that is the very good basis one zero zero zero zero one zero zero one and n dimension similarly so why should i want to use the different basis i have said that eigen vectors is a very convenient basis but why do i care about it i al ready have a very very convenient basis which is just these one or two vectors are along these directions right so why do i care about a different basis i understand that i that is there somewhere but something more than that that is one advantage which i will talk about what else more interesting ok in what sense i love the power which comes with my job right that you give a right answer and still i can embarrass you know so that is correct actually"}
{"audio_filepath": "Data/Preprocessed/Regularization in autoencoders (Motivation)_53.wav", "duration": 702.0, "text": "then we will go to the next module where we will talk about r egularization in auto encoders and we will talk about a m otivation for doing that so poor generalization so why do we need a regularization people have done the machine learning course or any equivalent course why do we n eed regularization to avoid student or enable or enable generalization right n ow in the case of an over complete auto encoder what is likely overfitting is likely why is it so w hat does what do you mean when you see generalization actually can yo u talk in terms of training time test time and so on so generalization is essentially that your are training so remember that at training time you are trying to solve an optimization problem where you are looking only at the training data so it is q uite likely that you will drive the error to zero for the training data that means you have learnt perfectly everything for the training data right but now it is also possible that when i give you a new test instance which you had not seen during training  that means you had not seen instance while doing the optimization that means this instance did not contribute to your loss function then it is very lightly that when i gave this instance then you would get a non zero loss or a loss much higher then wha t you get for your training data does that make sense t hat is what over fitting is and it leads to less generalization your model should have generalize to unseen data but it cannot do this o ne typical situation where over or where generalization happ ens is if you have a dash number of parameters now what did i ask actually student generalization no ok if a case where a over fitting would happen is when you have a dash number of parameters student large number of large number of parameters r ight  n ow do you see why i am saying this what is there on the slide an over complete auto encoder what would it have student a large number a large number of parameters so what could it do student overfitting over fitting what do we do to avoi d over fitting student regularization regularization so that is why we need regularization i have still no told you why do we need an over complete auto encoder ok still that is an random variable i still need to decide but can this happen in an un der complete auto encoder also it can right b ecause under complete auto encoder just says that your k is less than n it d oes not say how much less it is r ight so it is it is still have and depending on a data that you are trying to model it could stil l have a large number of parameters so for example let us take an example for the under complete case suppose you are doing image classification where you have a digit three at the center of the image ok and a lot of these are white spaces so what is the dimension and suppose this is a one hundred cross one hundred image what is the dimension of this image input how many if you cannot multiply one hundred into one hundred student ten ten k right of this a lot of data is not important so my n is ten k a nd at least by this thing that i have drawn it looks like probably only twenty percent of that is what actually captures the digit but now if i choose k to be equal to one thousand it might still be large for this application so i am using an under compete auto en coder but it could still be a situation that my under complete is still having a large number of parameters all of get this intuition it is a very weird example but still really do you get the intuition you could have a very high dimensional input and you might think you are shrinking it a lot but there is so much redundancy in your input that even that shrinking still leads to a large number of parameters and you could still over fit therefore even for an under complete auto encoder you could stil l need over a regularization so fine so that was the motivation since the over complete case of course the model can simply learn to copy we have seen that and that is why we need to introduce generalization fine now what is the simplest sorry w e need to introduce a regularization what is the simplest regularization technique that you know that is not the simplest l two regularization and you see why i say that is the simple st we can take the derivative for those of you do not get it do not wor ry we will get to it or if you do not get to it do not worry so ok the simplest solution is to add the l two regularization term to the objective function so this was my objective function i wanted to minimize the squared err or loss i have added a term to this what does this term do what does it doing first of all tell me wh at is this quantity theta is a student all all the parameters that you have right a nd i am assuming that they have just put it into large vector i am taking the l two norm of that vector so even you though you have those matrices and just flattening them all out and putting them into a large vector called theta right so what is happening here i am not allowing my weights to shrink or grow grow  because if my weights are very large what would happen student grow grow this quantity would grow so then i cannot really minimize this minimize this as effectively as i want right why this makes sense how many of you why this makes sense so i am now why am i not preventing the weights to go to zero ok so we will see this in more detail in the next lecture this is again a basic lecture on bias variance and regularization and so on so we will try to arrive at a more reasonable answer for this for now just see that i am putting some constraints and the weights so effectively and i am doing gradient descent i am not allowing the weights to take very large values i am trying to restrict them to a certain area so i am not allowing to it to explore the entire w comma b plane but trying to restrict it to smaller values of w comma v how many of you get this intuitive explanation so in other words what i am trying to do is that i am not giving it in a freedom so that it can completely drive the error on the training data to zero and my hope is that if i do not do this if i do not allow it to completely memorize a training data then it should generalize well on the te st data is that intuitive fine ok now i have changed the loss function aga in i have the square i have told you how to do it for squared error loss for the cross entropy loss and so on but now i have changed the a loss function again so again i need to teach you back propagation no what will change now again i need to der ive with respect to the last layer what is the minimalistic change that is going to happen now just tell me this theta is actually w one w two and so on right just assume all the parameters just flattened out into a vector fine and now tell me what is dou l theta by dou w one going to be or let us simplify things let us call this l theta a nd let us call this omega theta  let us call this l dash theta and then your l theta is the combination of these two terms so this derivative is going to be a sum of two derivatives out of that one you already know what is the second student two times lambda two times lambda w one so it is a very simply change to your gradient descent update rule how many of you see that whatever update you will had just add minus two lambda w one to that ok should have been two lambda w but of course you do a half here so it is fine is it a nother trick which is typically used at least in the context of auto encoders is to tie the weights of the encoder and the decoder how does that help what does tying the weights mean now i appreciate what you are trying to say so one we have doing this is just say w star is equal to w transpose you will enforce that you actually have o nly one matrix w and here you are using w transpose mathematically does that make sense all your operations go through because this is going to be n cross k and this is going to be k cross n so whatever effectively done i have reduce the number of parameters in my network right i am enforcing i am forcing this upon the network that i am not going to give you two sets of weights you just learn the w s in a way such that when you use w transpose you should be able to reconstruct this how many of yo u get this not many ok please ask me doubts if you do not there is nothing very student why is it w transpose why is it w transpose because otherwise student refer time eightfiftytwo claim that how can you claim that that would work because you have no linearitys in between right no w inverse would not work what is the simplest thing to do why would you want to compute an inverse that is an interesting question how would you implement this how would you if there are multiple paths from a weight to the output how do you compute the gradient sum it across all those paths what is happening here how many paths to there exist from the weight to the output one is this direct path and then the other is another this path also so you just sum it a cross these two paths do you get that how many of you do not get that how many of you do not get that ok so if this was w star you did no t have a problem y ou could just have computed dou l b y dou w star and dou l by dou w now think of it as this righ t that you have this this is one path w w to the output ok and now the gradient is just going to be sum across these two parts one path is the single path and the other path is the double path so it is just going to be a sum across these two paths oh no no so you just have one matrix w which are going to update you do not have two matrices you just have one matrix w at one place you are using w the other place you are using w transpose but just look at it element wise right do not try to look at i t in the terms of matrices so you have n cross k elements here w one one to w n k right you have to computing the partial derivative with respect to each of these and every time they are considering all possible paths to the output and that value is getti ng updated right and at one place you are using a particular arrangement of these ws at the other place you are using a different arrangement of those ws that s o it will just remain the same right is that ok student n o no  this is for regularizati on right so we are reducing the number of parameters by half student refer time tenfiftyone yes student refer time tenfiftytwo no that i mean that also has but that is not be the objective we are trying to do regularization how many of you have lost at this point please ask me if you have questions really i do not mind answering but if you just give me bl ank spaces i cannot read them  so this is used at quite a few places where you tie some weights right so that so effectively you are saying t hat learn it in such a way that it works at both the places and you are reducing the number of parameters so weight tying is something which is very commonly used for regularization in the context of neural networks so that is where we will end the mo tivation part and it is too very simple ways of doing regularization one is the standard known trick which is to use l two regularization and the other one was something special that we s aw which was tying the weights you all have a lot of doubts about tyi ng the weights"}
{"audio_filepath": "Data/Preprocessed/McCulloch Pitts Neuron, Thresholding Logic_11.wav", "duration": 774.0, "text": "let us start with module twowhich is about mcculloch pitts neuron so as we are done this during the history lecture way back in one thousand nine hundred and fortythree mcculloch and pitts they proposed highly simplified computational model of the brain so now let us see whats the motivation we know that our brain is capable of very complex processing its capable of taking a lot of inputs from various sources and then help us taking various decisions and actions now what if you want a computer to do thiswe want a module which is very similar to how the brain works or at least how we think the brain works which takes a lot of inputs and then does some processing and helps us take a decision so what they proposed is this model which will take a lot of inputs and these inputs are all binary all these inputs that you see here these inputs are fed to this mcc ulloch pitts neuronwhich is an artificial neuron and it is divided into two partssothe first part collects all the input so remember you had these dendrites which were taking all the information from everywheresothis just collects all the information and then the second part is aggregation i have collected a lot of information from all the sources n ow the second function will decide what this aggregation is and based on that it will take a decision whether to fire or not so the output is again boolean if its zero then neuron does not fire if its one the neuron fires so let us take a concrete example so suppose i am trying to make a decision whether i should watch a movie or not so xone could be is the genre of the movie thriller similarly there could be another variable say xn which says is the actor m att damon so these are all various such factors that i could take is the director christopher nolan the music given by someone and so on so all these are probably factors which help me decide whether i want to watch this movie or not and you want this neuron to help us make that decision so now what is happening here is these all inputs they can be either excitatory or inhibitory so let me tell you what inhibitory is first so you are taking input from a lot of sources no w see one of these sources or one of these inputs is a m i ill today  a m i down withfever  so if thatinputison irrespectiveof who theactor directoror whatever is i am not going to watch the movie right because i just cannot leave from my bed so these are known as inhibitory inputs irrespective of what else is on in your input features i f this input is on your output is always going to be zero that means the neuron is never going to fire so you could think of it as suppose my mood is not good today i do not feel like getting up or if i injured my leg or anything right if any of these conditions is on irrespective of what the other factors are i am not going to watch the movie so that is an inhibitory input and excitatory input are on the other hand is not something which will cause the neuron to fire on its own but it combine with all the other inputs that you have seen could cause the neuron to fire and how so this is how so these are all the inputs that your neuron is taking a ll i am going to do is i am going to take a sum of these i am going to take aggregation of all of these so what does this count actually give me the number of inputs which are on the number of inputs which are value one t hat is all this aggregate this is a sum of all the ones in my input now this is what g does this is a very simple function is taking a sum of my inputs n ow the function y takes this as the input that means it takes this sum as the input and if the sum is greater than a certain threshold then it fires i f the sum is less than the certain threshold then it does not fire so again see what is happening here is it is same as now if you depend on the actor director and genre and so on and you fine at least two of these three conditions are satisfied at least i am happy with the actor and the director even though the genre is not something that i care about i will watch the movie or you might be a very niche go movie watcher who only goes to a movie if the actor matches your requirement the director matches your requirement and the genre and the music and everything matches your requirement so you are threshold in that case it should be high so this is how it is going to help you make decisions now again a very simplified model and this is theta is called the thresholding parameter that is the value which decides whether the neuron is going to fire or not and this over all thing is known as the thresholding logic  so this is what a mcculloch pitts neuron looks like now let us implement some boolean functions using this mp neuron so from now on i will just called it mp neuron and we will try to implement some boolean functions using it  so now why are we interested in boolean functions it is because we have overly simplified the way we take decisions w e are saying that the way we take decisions is we take a lot of boolean inputs is actor matt damon and genre thriller and so on and based on that we produce a boolean output so an input is all booleans so we have xone to xn which are all booleans and your output is also boolean so that is a boolean function that you are trying to learn from x to y is that clear yo u have x just happens to contain n different variables here ok and lot of decision problems you could cast in this framework you can just imagine right whether to come for lecture today or not a gain is you could cast in it depending on various boolean inputs this is a very concise representation of the m cculloch pitts neuron w hat it says is it takes a few boolean inputs and it has certain threshold i f the sum of these inputs crosses this threshold the n the neuron will fire otherwise it will not fire  t hat is the simple representation of the m p neuron now suppose i am trying to learn the and function when would the and function fire all the inputs are on so what should be the value of the threshold in this case three e veryone agrees w hat about the or function one let u s see a few more this function so let me tell you what this function is so you see this circle here so that means that this input is an inhibitory input i f that is on then the neuron is not going to fire t hat is how i am representing it so now tell me what should the threshold for this be it is not so hard see if xtwo is on it is not going to fire so you have four rows zero zero zero one one zero one one so two of those are ruled out and it is not going to fire n ow out of the remaining two when do you wanted to fire so what should be the threshold one now what about this function zero or three three is not even a valid option zero e veryone agrees to that and what about this zero so you get this  so now if you have a certain number of input variables and the function that you are trying to model the decision that you are trying to make is a boolean function then you could represent using these mp neurons whether all boolean functions can be represented in this way or not that is still not clear i am just showed you some good examples we will come to the bad examples later on here is the question so can any boolean function be represented using a mcculloch pitts neuron so before answering this question we will see a bit of a geometric interpretation of what mp neuron is actually trying to do so let us take or function where you have two inputs xone and xtwo and this neuron is going to fire i f xone plus xtwo is greater than equal to one that is clear th at is how the definition is  no w if you look at this xone plus xtwo greater than equal to one  now let us ignore the greater than part first so we will just talk about xone plus xtwo equal to one w hat is this equation of a line everyone gets that ok n ow in this case since we are dealing with boolean inputs and we have two access xone and xtwo how many input points can we have  four right zero zero zero one one zero one one so you could have these four points so just note that this is an xone and xtwo axis but only four inputs are valid here this is not a real numbered access this is only boolean inputs possible here now what is the line xone plus xtwo equal to one tell you which line is that so one which passes through one zero here and zero one here this is that line now what do we want that for all those inputs for which the output is actually one they should lie on the line or on the positive side on the line and all those inputs for which the output is zero they should lie on the other side of the line i s that happening so what is actually mp in unit actually learning linear decision boundary it just what it is doing in effect is actually it is dividing the input points into two halves s uch that all the points lying on that line right are sorry all the points for which the input should be zero lie below this line and all the points for which the output should be one sorry in both cases it should have been output so let me just repeat it a ll the points for which the output is zero lie below this line and all the points for which the output is one either lie on this line or above the line is that fine and so let us convince ourselves about this e ven it is not already clear from the equation for how many of you it is already cleared from the equation that this is exactly what it does for a large number of periods but still we will just do a few examples and move ahead now for the and function what is the decision boundary it is xone plus xtwo n o that is the decision boundary equal to two so again i have these four points o nly these four points are possible n ow where is my decision line passing through that one one and intercepting this somewhere around two zero and this around zero two  so that is the line which i am interested in now again do you see that our condition is satisfied that all the inputs for which we want the output to be one are on or above the line and all the inputs for which we want the output to be zero or below the line now what about this function  w hat is the threshold zero so what would the line be xone plus xtwo equal to zero which passes through the origin right and again all the points are either on or above the line so this part we are going to call as a positive half space and this we are going to call as the negative half space now what if we have more than two inputs  i n a two dimensional case when we just had xone and xtwo we are trying to find a separating line in the three dimension case w hat will we do plane in the higher dimensions hyper plane so this is now your three dimensional case again there are three axis here but not all points are possible h ow many points are possible eight points and which is the function that we are trying to implement orso for these eight out of these eight points for how many is the output one seven and for one it is zero so what is the kind of plane that we are looking at w e are looking for a plane such that seven points lie on or above it and one point lies below it and which is that point zero so now what is the equation of that hyper plane x one plus x two plus x three is equal to one y ou see this so you see that all the seven points are visible but the points zero zero is not visible because it is on the other side of the plane so this is doable in three dimensions also and again in higher dimensions also right w e could find in hyper plane so the story so far is that single mcculloch pitts neuron can be used to represent boolean functions which are linearly separable so a linearly separable function is such that there exists a line such that for that function whichever points produce an output of one lie on one side of the line and whichever points produce an output zero lie on the other side of the line"}
{"audio_filepath": "Data/Preprocessed/Image Classification continued (GoogLeNet and ResNet)_92.wav", "duration": 1325.0, "text": "so we will g o to the nex t modu le where y ou wanted to look a t two more architectures for image classification these are googlenet and resnet so here is a question right so consider the output at a certain layer of a convolutional neural network so you have this layer after layer of c onvolutions and max pooling and so on and you are at somewhere in the middle and this is what your volume looks like this is what your output volume looks like now after that you could apply a max pooling layer you could apply a one   one convolution you could apply a three   three convolution or you could apply a five   five convolution right and so far we saw that all these architectures they do one of these things they either do a max pooling or they do a three   three convolution or they do a five   five convolution or a seven   seven eleven   eleven right any convolution but they are all uniform they are all either three   three all either five   five or either seven   seven right so why choose between these options why not do all of this at every layer do you get the question that i am trying to ask r ight so far what we were doing is that you have this volume this volume at a certain layer of the convolutional neural network and after that you are either using all three cross three filters so you are using two hundred and fiftysix three  three filters or five into three  three filters or using seven  seven or using five  five you are never using a mix of all these right so why not use a mix of all these why the why take a decision on that i only wan t three  three because it is possible that you want to capture interactions at different levels so you should have varied size filters at every layer so how many of you get the question and the intuition that i am trying to ask o kso the idea here in googlenet or in the inception net is that why not apply all of them at the same time and then concatenate the feature maps right so i will also do max pooling i will also do three  three feature maps i will a lso create five  five eleven  eleven and then just concatenate all of these together so let us see how to do that right now one problem with this naive idea is that it could result in a large number of computations so let us see what i mean by that so suppose the padding is zero the stride is one then if you have a w  h  d input as the volume and if you have an f  f  d filter then the output would be of this size we all agree with this this is the formula that we have been looking at so this is the size of the output volume now every entry in this output volume requires how m any computations to get a single entry in this output volume how many computations do i have to do student f  f  d f  f  d so how do i get a single value i apply a convolution at that value and then i do those many computations and here the number of computations is that i am going over this block of f  f  d doing a weighted multiplicati on and then adding them up right so you need that many computations everyone is clear with this okfine so each element of the output requires these many computations and we have so many elements in the output right so you are doing really those many number of computations right so can we do something to reduce the number of computations right so that is the key idea that we need to focus on so all of us buy the idea that doing this multi granular or multi sized filters is a good idea becaus e you are capturing interactions are different layer but i showed you that this is a problem with this you guys just apply multiple filters so let us see what we do so we what we do is one  one convo lutions what is the one  one convolution do what does it make sense student refer time fourtwo how does the one  one convolution make sense you have a pixel i fit a one  one convo lution on that what will i get i will get back the pixel student refer time fourtwelve along the depth right so remember it is not one  one it is one  one  depth student depth right so what does a one  one convolution do it actually aggregates along the depth so this is what my one  one convolution looks like it is one  one  d s o i just fit that block on that pixel and do everything along the depth and get a single value right so from a threed output using a one  one convolution i can go to a two  twod feature map everyone gets this ok now i could use several of these one cross one operations one cross one convolutions in fact i could use done of these such that done is less than d so what effectively happens the depth of the output reduces so i take a certain output volume whose origin al depth was d now i take done one  one convolutions right so i get an output whose depth is smaller than the depth of the original output is that fine everyone gets this ok  and you see how this will save computations right bec ause remember that this was f  f  d and now i have reduced d to done ok so it is going to reduce the number of computations so that is what the idea is you reduce it from f  f  d to f cross of cross done right so thats this particular network or this paper introduced the idea of this one cross one actually it did not introduce it used it but it made it popular probably n ow once you have done this so this is how i am going to proceed now i have a certain volume i have applied one  one convolutions to it using that i have reduced the number of dimensions now i am going to apply three  three convolutions as well as apply five  five convolutions on top of that right because that is the motivation that i had started with that i want to apply kernels of multiple granularity now can you think of some refinem ent to this you see this branching over here why use the same one  one convolutions before feeding to three  three as well as five  five i could use a different set of one  one convolutions and feed it to a five  five and use a different set of one cross one convolutions and feed it to three  three is that fine what is the problem with this student again increasing the number of computations again increasing the number of computations right but they found out that the tradeoff between this is fine even if you are doing more one cross one operations it still is ok the number of computations are still manageable ok and then you c ould also do a max pooling because we were choosing between these things right five  five three  three seven  seven and max pooling so we will do all of these in parallel and we also do some one  one convolutions so how many different types of operations we have done we have done one  one three  three five  five and max pooling followed by one  one convolutions now all of these outputs that we have got we have got a bunch of feature maps now this is one set of feature maps this is another feature maps this is another and this is another all of these four we are going to concat t ogether to get a single output volume do you see what is happening right it is not very mechanical there is nothing really profound about what is being done the only two profound ideas are one is apply multiple kernels of different sizes and the other is to use one  one convolutions to make the whole computation manageable that these are the only to main ideas the rest of it is not very different from what we have been doing how many if you get this operation completely so t his block is called the inception module ok this entire thing is called an inception module so in subsequent slides when i put an inception module then you know that these parallel operations are happening right so far whenever we had seen a convoluti onal neural network it was all serial right so you started with one operation then another operation then another operation and so on now you have an output or an input volume you apply multiple operations in parallel and get one single output right so it is a parallel serial combination ok so you will now see the full googlenet architecture so his question was basically three cross three would result in a different sized feature map right because of an five cross five would result in a different sized feature map so i will use appropriate padding so that all of this becomes equal ok so this is how googlenet looks like so you have the input again rgb and same two hundred and twentyseven  two hundred and twentyseven or two hundred and twentynine  two hundred and twentynine then you apply a convolution layer followed by a max pooling layer convolution max pooling then you have this inception module with a very specific configuration so they have ninetysix one  one convolutions before feeding to one hundred and twentyeight three  three convolutions sixteen one cross one convolutions before feeding to thirtytwo five  five convolutions and so on and i do not really see much point in going into the details of these numbers there is hardly any intuitio n behind them i again guess that its you try a bunch of things and this is the one which probably gave the best output so the key idea is that of course you have this inception module which is a parallel module which does a lot of operations in para llel this is again followed by another inception module which has a different configuration followed by max pooling then again a few inception modules in fact five of them again max pooling then inception and this is the other interesting idea that they c ame up with so at this point remember in vgg net at the final layer you had an output of size five hundred and twelve  seven  seven right and we said that this was a problem how many of you remember this why was this a probl em student refer time ninefiftyfour because i need to connect this to a student fully connected fully connected layer right and that fully connected layer was of size four thousand and ninetysix right so what they said is that what you could do is instead of taking five hundred and twelve cross seven cross seven for each of these five hundred and twelve feature maps that you have take the seven cross seven and just do an average pooling from there what does what do i mean by that student average take these seven cross seven values take an average of that so now instead of five hundred and twelve cros s seven cross seven how many values will you end up with student five hundred and twelve just five hundred and twelve and in their case instead of one thousand and twentyfour cross seven cross seven you will just end up with one thousand and twentyfour values right so instead of looking at these dense connections with every pixel in the output volume  you just take the average of those pixels and then do a dense connection from there so from this volume you just go to a vector of size one thousand and twentyfour which is exactly this vector shown here and from there on life becomes easier right because you have done a fifty percent sorry fifty times reduction in the volume so this was one thousand and twentyfour cross fortynine now we just have one thousand and twentyfour cross one so you have a fortynine times reduction in the size and that is a huge parameter reduction and that actually worked very well in practice they of course  add these dropouts and other things and then you have your fully connected layers and finally the soft max layer at the output to predict one of the thousand classes right so this is the full structure of googlenet or inception net or with multiple i nception modules right so just remember that key takeaways are three one is half filters at multiple granularity applied in parallel the other is use one cross one convolutions to reduce the number of computations and the final one is to use this average pooling to make sure that you do not have this blow up of parameters at the output ok so these are the three main ideas that you need to do right ok so this is exactly what i explained so instead of having this nasty looki ng connection which would have been fifty thousand one hundred and seventysix cross one thousand you just take the average from this grid and just get a one thousand and twentyfour dimensional vector which results in a much smaller weight matrix at the output everyone gets this so ok yeah so this is fine refer sl ide time twelveeight so this has twelve times less parameters than alexnet it has two times more computations right so that is what i meant by the tradeoff so the number of parameters has reduced significantly of course a large amount of this savings happen in the fully connected layer its not the ingenuity in the inception module which led to the fewer number of parameters that actually leads to more number of parameters right but the reason they could afford more number of parameters in the convolution layers is because they reduced a lot of parameters in the fully connected layer do you get that so they did this tradeoff and it has two times more computations then alexnet but it is still acceptable because you see that there are many many layers as c ompared to alexnet right so let us actually count the number of layers that we had here so one two three four five six seven eight nine ten eleven twelve thirteen fourteen right so it has fourteen layers and each of these inception modules is again like split layer right it has this parallel components t here so having two times more computations was still an acceptable tradeoff and it of course led to much better accuracy as compared to alexnet or zf net or vgg net right that we had seen in the original trend graph ok so now we will go on to the las t architecture that we will discuss for image classification which is resnet so here is the idea behind resnet or here is the motivation right now suppose we have been able to train a shallow neural network well now again m y definitions of shallow are relative this is by no means shallow there are many layers here right so you have some eight layers here now if i have been able to do this properly that means what i mean by that is that using this network at least i was a ble to reduce the training error to zero or close to zero some acceptable value and i was able to get some reasonable generalization performance that means on the test that i was able to get some reasonable accuracy that is what i mean by i was able to make this network work well now suppose i add a few layers to this network and i have carefully added some layers in between here and in between here or over there right now intuitively i could argue that if the shallow network was working well right then for the deep network this is exist at least one solution which can directly come from the shallow network can you tell me what that solution is student refer time fourteentwentynine i want all of you to kind of digest that idea what the deep network could have done is i know that this shallow network works why not i just behave like that and i learn these parameters in such a way that i just end up copying from here to here how many if you get this right so there is a case for the deep network to do at leas t as well as the shallow network and it could do the same thing at this point all of you get this idea right so in other words the solution space of the deep network or rather the solution space of the shallow network is actually a subset of the solutio n space of the deep network there was one solution for the shallow network which could have been used as it is for the deep network of course for the deep network there are several other solutions because instead of the identity here you could learn di fferent things there but at least that one solution exists so i should at least if i do use this in practice ex i should expect that this would work as well as the shallow network right is that argument fine with everyone student refer time fifteentwentynine or which has only one yeah yeah of course student yes i mean so it those arbitrary things would not work but here what there it is the for the explanation intuition right you are using some reasonable things and you are just trying to make it comp atible with whatever you have so far so the argument is valid right so i cannot expect that i had a volume whose depth was one hundred and twentyeight and then i suddenly decide to use only one filter in the next layer right that means i have compressed everything and now i e xpect it to be able to recover from there that is not going to happen right so that is a fair argument but the argument which i was trying to make or at least for the illustration purpose is that if you do reasonable things and that is what people were trying out right these this is the exact network that someone was trying out and this did not work with well i will tell you what it is so do you get his doubt and my clarification on that is that fine ok so this is wh at was happening in practice right so you have a twenty layer network or thirtytwo layer network or fortyfour layer network and a fiftysix layer network and you see that the training error of deeper networks is much higher than the training error of the shallow networks that means this argument which i was trying to make that the deep network should at least do as well as the shallow network was not working well in practice right and it is if you think about it is not very surprising because this argument hinged on the fa ct that it should be able to learn this identity mapping but this identity mapping is one of many solutions right so for it to be able to narrow down on that solution it is easy for you and me to think about it but for the network it does not have t his intuition right that i can just copy it from there to here do you get that the solution space is really really large and like finding that needle in a haystack right you have these many solutions possible and i am trying you to arrive at a soluti on where you end up with the identity solution is that clear and it is not so easy for the network to do that everyone gets this how many of you get this idea so why not explicitly try to do something of this sort where the network can actually learn some kind of an identity function so now consider any two layers you know by stack layers i mean this is a convolution layer and followed by a convolution layer right so these are two convolution layers back to back so from i what do i actually end up doing here i had a certain input x and i am learning some transformation of x through these convolution layers right i am trying to learn x and then i sorry i was given x i am passing it to convolution layer so i will run s ome transformation of x and my argument was that if it could learn to directly copy x here the deep network should at least work as well as the shallow network so why not i explicitly ensure this so why not i do this that in addition to these conn ections i also explicitly connect x to the output do you get this so now what i am trying to do this is hx is equal to f of x which is the transformation that i learned for x and in addition i also add x so what am i doing i am explicitly feeding i t the identity function right how many if you get the intuition for this so what i am trying to do is i have a sense that if i could have transferred this x as it is across layers then there is a chance that i should be able to do as well as the shallo w network right so now i am going to explicitly ensure that that you learn these transformations but i will also feed you x at every stage at a reasonable time right so these are known as skip connections so after every two layers i will feed back th e x or you could try after every three layers i will feed back the x so i am trying to maintain the original copy of x after every interval ok fine so why would this help so this follows back from our argument and it is the same thing which i said befor e so using this idea of using these skip connections these authors were able to train a really deep network of one hundred and fiftytwo layers right and this gave on multiple vision challenges right one being imagenet it gave sixteen percent better results and the best network and this is the one which reads that near human performance then imagenet localization is another challenge where you need to localize the object so there they get twentyseven percent better than the best results and there are th ese bunch of other vision tasks detection segmentation and all of them and in all of them this significantly outperformed the best system using a very very deep network of course the downside is that you have a very very deep network it will take its own time to train and so on but of course if you have a microsoft or google you can afford to do that so that is the current theme right i mean the one with the largest computational resources wins everything right so and they also are there is some other bag of tricks which is not i mean it is not very difficult to understand so they used batch normalization every after every conversation layer have you heard of batch normalization ever in your life ok good they used xavier by two initialization ever heard of that xavier by two was the same as student he initialization he initialization right then they use sgd not any of the fancy adam or adagrad or anything with a momentum of zeronine learning rate was set to zeroone and divided by ten whenever the validation error plateaus the mini batch size was two hundred and fiftysix they use a weight decay of one ht what is weight decay weight decay is in the context of which regularization student ltwo ltwo and what does this mean weight decay of one e raised to minus five student lambda refer time twentyonefive the lambda was set to one e raised to minus five all of you remember these things right we did it in some previous course in some previous life and no drop out was used right so since i have this here i will just say something more on this so in your reading papers on deep learning right focus on the experimental section where all these hyper parameters are described so these are known as the hyper parameters these are not related to the parameters of the model these are related to hyper parameters which is what the batch size is whether you used l two regularization what was the learning rate what was the optimization and so on so turns out in many cases if you do not stick to this you will not be able to reproduce the results of the paper right so you might be wondering that this network i understand this is just one hundred and fiftytwo layers and i can just keep adding skip connections i can easily code this up but i am not getting the same results as the or iginal authors of the paper so this is where you need to dig up them right you need to look at the experimental section where most good authors provide these details of how they have tr ained the network how many epoch that they use what was the patie nts set and all that if you follow those the chances of reproducing are much higher still not guaranteed but definitely much higher ok so that is where we end the lecture on convolutional neural networks and imagenet classification"}
{"audio_filepath": "Data/Preprocessed/Singular Value Decomposition_50.wav", "duration": 1515.0, "text": "so with this we will move on to the last topic in the yeah so  that is something that you will have to so the way i would do it right is that you keep aside some one hundred images from your data as validation data now once you have learned these eigenvectors try to compute the reconstruction error for t hese one hundred images and just vary it do one hundred one zero tenzero written as many dimensions as you can and see at what point is a reconstruction error ok for you right and this is assuming that you have some notion of what is a reasonable reconstruction error s o we all know that the minimum is zero but if you have zerofive then maybe for face database it might be b ut if it is a database where you are trying to look at mechanical parts so suppose you are looking at motors and rotors from a machine assembly now the re you want to be able to distinguish minor detects defects on this and a detect could a defect could actually just be one single or two pixels getting different from the original image right so there the reconstruction loss would be much needs to be much more robust you get the point so it depends on your application so you will have to take some validation data either have a domain expert to tell you what is reasonable or go by the number that you get right and this is the validation error that i get so everyone understands the question and perhaps the answers ok so we now go to the last module student refer time oneforty yeah if you can student refer time onefortyone yes you can now project any face into this database a so that is the eigen basis that you have got you have got the basis vectors now any data you can project onto this basis student refer time onefiftyfour n ow so if you are trying to learn these eigenvectors by say using one hundred images all of which belonging to a particular demographic say all caucasian images right and now at the runtime you have an asian image then you will have obviously have some error right but you have large even of data say if you have if you are constructing this from million images then it should generalize that is i mean just as for any ma chine learning algorithm the training it from small data and you bring out some outlier at test time it is not going to work right b ut if you have reasonable data it should generalize any other questio ns to calculate the eigenvectors x is m cross ten m cross ten k yes n ow we move on to the last topic for the basic portion and the next class we will do auto encoders will be back to deep neural networks so sing ular value decomposition right refer sl ide time twofifty so this is actually the stuff that i need an important theorem from here at multiple two places in the course so now before doing the right  l et us get some more perspective on what eigen vectors do and why are they actually important so let v one to v n be the eigen vectors of a and let l ambda one be the corresponding e igen values so we know this a v one equal to lambda v one and so on ok now suppose all the vectors in r should be r raised to n ok so if a v ector x belonging to rn can be represented using this basis ok now what if i am interested in the operation a into x what is the advantage of representing it using this basis so this is what you are saying the other day student refer time threefiftyfour what is ax it is a matrix vector multiplication right and it is going to be a heavy computation now if all my vectors in rn are represented using the eigenvector as the basis what happens to this matrix operation student refer time foursixteen it redu ces to student refer time fourtwentyone let us see so i was interested in ax but i know x is this so you get this step and what happens finally do you have the matrix anywhere here so what happens to the matrix operation student refer time fourfortyone it reduces to a sum of scalar operations right if your vectors were representing using the eigenvector as a basis so this is one reason why this is important right so you can now get away of the get rid of the matrix operations and just do scalar operations right so now there is a catch here which i am going to ignore just to try it if i bring in the catch you guys will get confused so i will ignore if anyone has a doubt maybe talk to me after the class but for now let us go with the fact th at the matrix operation re duces to a scalar operation now so far what we have done is discussed square matrices i have said that they are the villains of linear algebra but who are the super villains of linear algebrarect angular matrices everyone says that but why imagine what they do to a vector yeah so can rectangular matrices have an eigenvector student refer time fivefortyfive yes obviously yes that i mean any matrix can have an eigenvector student no no why ca n you write so mething of this form you can t right because when the matrix operates on an n dimensional vector what does it give you student refer time sixone a n m dimensional vector right hence they are super villain right because they take th e vector from one space and transform it to a completely different space that completely lots lost it is identity right so that is why rectangular matrices are even harder so now we just saw that for square matrices this eigenvectors form a very conve nient basis where these operations reduce to a scalar operation but now rectangular matrices do not even have eigenvectors so then cannot we have the same advantage there can we have the same advantage there you can t right because you do not have an eigenvector but i would teach you about singular value decomposition so i better have something so get the connection ok there is a problem with square matrices with the rectangular matrices so now let us see so we will try the aim is to see if we have something equivalent to this scalar transformation that we had for square matrices how many of you have seen this in linear algebra before so you know whatever i am going to talk about fine so the result of ax is a vector belonging to r m an d the original x belongs to r m so we do miss it miss out on this advantage that you could have reduced the matrix operation to a scalar operation and now we will try to see if we can still get back that advantage so notice this is matrix you can think of it as a function which provides a transformation from rn to rm so what is the set of inputs to the matrix it is vectors belongi ng to rn t hat is the set of input now suppose we had a pair of vectors v one u one v two u two vk u k each belonging to these two different universes one is rn the other is rm and there was a specular relation between them that a into vi is equal to sigma into ui suppose i am just being ambitious let us see whether we can actually have this pair but suppose we had this pair then can you connect this back to the discussion on scalar operations so let us just see that in detail and we will of course assume that these are orthogonal and form a basis so the vis form a basis in rn and the uis form a basis in rm is that clear that is all straightforward we have these vectors now every a vector belonging to rn which was the input space can be represented using a linear combination of v straightforward and any vector belonging to the output s pace can be represented of student refer time eightfiftyfive of u right so that means any x in the input space i can write it as this linear combination and now if i do the matrix operation what happens student refer time ninenine you get this a into vi what is a into vi sigma ui i have still not shown you how to find these sigma is ui by the way right ok once again the matrix multiplication reduces to a scalar multiplication so now let us try to look at a geometric interpretation of this so what you have is this original space which is rn you are using a as a matrix operation right as a function and you are transforming vectors from n to rm right so this is the space transfer that i w as saying it vectors are being picked up from rn and being put into r m ok and rn is a space of all vectors which can act as inputs to this function and rm is a space of all vectors which are the outputs of this function ax now we are interested in fi nding a basis u v such that v is the basis for the inputs when i say basis all of you should immediately start thinking of dash vectors student orthonormal vectors orthonormal vectors orthogonal or orthonormal right once we have orthogonal we do no t care about the rest u is the basis for the outputs such that if the inputs are and outputs are represented using this basis then all our matrix operations reduce to scalar operations so we are just trying to find the rectangular analogy for the squa re a phenomenon that we observed ok that is what we a re trying to do now can you tell me i have told you that if such a v and u exists then you could do this can you give me such a u and v so what do we mean by so here i said actually i said this right that the dimension of the row space is actually k and the dimension of the column space is also k what do you mean by the dimension is i mean right here i am telling this is rn and this is rm and now i am telling you the dimension is k what do i mean by that student refer time elevenfive the only k linearly independent vectors and this is again something from linear algebra which i expect you to know is that all possible vectors in rn only a subspace belonging to rk can actually act as input to a x to produce a non zero output so i am talking about a null space column space and things like that right so this sho uld be clear if it is not it is  it is not very impo rtant at for us right now and hence we have only k dimensions s o let us look at a different way of writing this so you have this a v one is equal to sigma one u one av two is equal to sigma two u two so i can again do the same trick that i put all the v s into one matrix where vi s are the columns of this matrix and i will put all the us into another matrix where ui s are the column of this matrix is that fine everyone ok so far and then i can write it as this matrix operation same thing that we did when we are doing eigenvalue decomposition right so we had written it as a into u is equal to u into sigma right because there we had the condition that ax is equal to lambda x now we have a u is equal to sigma v or rather the other way around so av is equal to did i missed up  did i no right student refer time twelvetwentynine sorry student refer time twelvethirtytwo fine yeah so av is equal to sigma into u so is this fine no but when you do the diagonal operation you will get it as u into sigma y the same way as a x equal to la mbda x but when you write it is a into u is equal to lambda comes later on and if we have k orthogonal vectors so  remember i said that this basis consists only of k dimensions right because that is r the set of vectors whi ch can act as input to a so what i but i want a basis for the full rn so what do i do for the remaining n minus k have you heard this gram schmidt o thogonalization right so if i give you if there if you are trying to construct a basis for n ok f or rn rather and if i give you k orthogonal vectors they can do k you can construct the remaining n minus k using gram schmidt orthogonalization right so you can get the full basis ok fine so let me just see and this is orthogonal ok so you can writ e so you see these two forms can you relate it to something that we have seen before in the course this is singular value decomposition what else did we see before student refer time thirteenfiftythree eigenvalue so this exactly the same forms right and i h ave used the same set of tricks to arrive at it right so i first put the vectors into a column as columns into a matrix then wrote this in the matrix format and then pre multiplied post multiplied by certain things and i got these two formats and remembe r that v and u both are dash matrices student refer time fourteenthirteen orthogonal matrices right so that inverse is just their transpose ok ok  s o far everything is fine now i still do not know what u and v are all this analysis is assuming that i kno w what u and v are so now can you tell me how to get these u s and v s suppose v u and sigma exist  then we can write this right so a is u sigma v so a transpose would be the transpose of that now can you work with me wha t is the next step student refer time fourteenfortyseven ok next student refer time fourteenfiftyone t his is u sigma v transpose so then this would be i think the next step is no the next step is also wrong that fine ok fine i just had some error with the transpo se ok what will happen now what will disappear from here student refer time fifteentwentysix u transpose u that is i right u transpose the inverse of u so you get this what does this look like this looks like the eigenvalue decomposition of student a transpose a a transpose a that means v consists of the student eigenvectors eigenvectors of the student a transpose a a transpose a so now can you tell me what u would student refer time fifteenfiftyfour ok fine so thi s looks like the eigenvalue of eigenvalue decomposition of a transpose a similarly we can show that a a transpose is equal to u transpose sigma square u ok so then u is the set of eigen vectors of a a transpose right and now here what was with will the eigenvalue decomposition always exist for a matrix student no no under what conditions would it exist first of all it has to be a square matrix student refer time sixteenthirtytwo ok right but now for a rectangular matrix would be singular value decom position always exist student yes yes right because it depends on the eigenvalue decomposition of square symmetric matrices ok is that fine ok so for any matrix shall always have the eigen value oh sorry the singular value decomposition refer slide time sixteenfiftyeight so this is perhaps yeah  ok now just one last bit and let us see if all of you can understand this so now i can write a in this form this is nothing but what i already said right this is u this is sigma this is v transpose ok n ow from here from this step do you see how i got to this step this is something that we were struggling with yesterday also when we were trying to find out summation x i x i transpose something similar here you know the four ways of multiplying matrices r ight so this is which way one of the ways y eah so does everyone get this right so a simple thing would be first to just take these sigmas inside right because this is a diagonal matrix right this is all zeros so these are actually you can write i t as sigma one u one sigma two u two and sigma k u k right now this ends up being the product of two matrices right and you can write it as a sum of columns into rows right so what i am writing it as a sum of sigma one u one multiplied by v one so sigma one u one int o v one transpose is a scalar matrix vector matrix right so  each of these terms here is a student matrix matrix and you are adding k such matrices ok now try to relate it to reconstruction error you are taking a matrix trying to write it as sum of m any matrices if i trim some terms from this some terms from this sum what would happen if i have all the terms then what would happen i will get a back exactly if i drop some terms what would happen student refer time eighteenfiftyseven i get an approximatio n of a how good would that approximation be student refer time nineteenthree first is depending on the number of dimensions but is there a natural ordering in these dimensions if i want to throw away some dimensions which one would i throw away s tudent refer time nineteeneleven smallest student singular values singular values sigmas are the singular values so you see that this is getting multiplied here every matrix is getting multiplied by the singular value corresponding singular value so if i drop out the terms which have the smallest singular values then those matrices the elements would have been very small so i will not lose much in the approximation s o  again the same idea that i am trying to approximate the original matrix by a smaller ran k by of so now the original matrix had m cross n entries ok how much if i use only k eigenvectors or the sorry k singular vectors or k dimensions to approximate it how much storage would i need how many values do i need so the original matrix was m cross n how many entries are there here student refer time twentysix each of this is how much student refer time twentyten m for ui plus n for vi ok and plus one for the sigma and how many of these are there student k k so if k is very less tha n your m and n right then again you will have some compression you get this ok so all of these ideas are related and i want you to be able to connect them right that all of this is towards doing some approximations reconstructing some reconstructing a matrix from it is components and doing this reconstruction in a manner that you end up making minimum error in the reconstruction is this idea clear even if some part of the math is not clear is this idea clear how many forget this ok so some of y ou do not you do not student refer time twentyfiftynine yeah so what is the original dimension of a m cross n right now i am trying to reconstruct it using a sum of sum k terms ok so hence this k comes now each of these terms how many elements do i hav e i have ui which is of dimension m i have v i which is of dimensi on n and then i have the sigma i which is of dimension one right and i have k of these so this is the total amount of storage that i need i am saying that as k is much less than m and n which would typically be the case then you are getting a much lower space reconstruction of the original data right and you are doing this reconstruction smartly because you are not taking any k dimensions you are taking the k most important dimen sions and this most important is defined by the singular values this is designed by the sigma is that fine ok and actually there is a formal theorem which says that sigma one u one v one transpose is the best ranked one approximati on of the matrix is this a rank one matrix is sigma one u n i hope you guys have done the assignment right sigma one u one v one transpose is the rank one matrix and if i take this idea further this summation is the best ranked two approximation and if i keep going this summation is the best rank k approximation so what it says is that if you are trying to reconstruct the original matrix right from these components and if you go by the eigen or the singular values and you pick the ones corresponding the t op k singular values then the best that is the best possible reconstruction that you could have done now how do you formally define reconstruction how would you make it as an optimization problem what are you trying to minimize student refer time two twofiftynine the actual matrix has some values which is the matrix a ok b is the reconstructed matrix using only k dimensions how many of you understand what is this product saying what is this student refer time twentythreetwenty first k columns of u these are t he k singular values and these are the first k rows of so ok i was just talking about this is the first k columns of u these are the k singular values are put across the diagonal and this is the first k rows of v transpose and this is exactly the pro duct which i showed you here is that fine ok so there is a theorem this is called the svd theorem it says that if you want to reconstruct a then this is the best rank k approximation that you can get now if i want to p ose i t as an optimization problem what will i say what would i have minimized actually this is the reconstruction right so let us call it a hat actually and what does this mean this is the dash norm student refer time twentyfourfifteen frobenium norm what does the frobenium norm give you squared difference between the elements right  r oughly speaking ok so  it will tell you what is the square difference between the ij th element of a and the ij th element of b so whenever we have this situation if you are t rying to if this is our objective function that we have trying to reconstruct a or try to transform something and get a predicted a or a reconstructed a then the best possible reconstruction would be given by this solution so this optimization problem h as a solution that you just use the eigenvect ors of xx transpose and sorry a a transpose a nd a transpose a right is that clear ok so this is the theorem that we will be using when we are talking about autoencoders and we will try to connect auto encode rs to pca ok so just revise this is the prerequisite for next class whatever we have done in the last three sort of extra lectures you have to revise it before you come for class tomorrow right ok and yeah this is sigma is just some terminology sigma is actually the square root of lambda a that was obvious and u is called the left singular matrix of a and v is called the right singular matrix of a"}
{"audio_filepath": "Data/Preprocessed/Some Gory Details_107.wav", "duration": 373.0, "text": "before that i will just go into some more gory details about the math this is not to scare but just to make you more comfortable that we can actually deal with something which is not very straightforward or very neat as compared to what you are seen so far so i just go back to the formula which i had for computing the gradient of the loss function with respect to w and i cannot repeat enough times that all these notations are actually a bit of abuse of notation because these are gradients and not partial derivatives so i should actually be using this notation but for ease of explanation i use i stick to my original notations ok so now let us look at each of these quantities here and tell me the dimensions of these quantities let us start with the left hand side what is the dimension of this w was what ma trix what is i am talking about the circle entity what is the magnitude what is the dimension of that k  d student refer time onenine d  d student refer time oneeleven d  d someone n  d student refer time onethirteen n  d and n  k are the two options which are left w is the recurrent weight so w is what dimension student refer tim e onetwenty d  d so what is this gradient d  d ok what about this f ast s t what the hidden representation so that was d dimensional so what is this d  one ok what about t his why do you guys still struggle with this student refer time onefortytwo d  d and this d cross student refer time onefortyfour d cross it is very straightforward right what is the dimension of numerator what is the dimensio n of denominator thats all right so you see the kind of multiplication that you ar e doing here say of d cross d one  d d  d and then d  d  d ok let us look at each of these quantities and see if you are actually comfortable in implementing these are you comfortable with this the loss function with respect to the hidden representation we have done this enough times in ba ck propagation what about this we just saw a formula for this right so we know how to compute this quantity we have seen this in back propagation this is the derivative of a scalar with respect to a vector and we are very comfortable in computing thi s this was slightly tricky but we just derive this formula on the previous slides everyone okay with that what about this this is a tensor how do we compute this tensor what is our standard recipe focus on student refer time twothirtyeight the little g uy one element of this tensor and then you can generalize somewhere right so this is the tensor and we will just see that this just to make you all comfortable is this not like just to intimated you with all these large sized tensors but i am just tryin g to show that this is all easy this is not hard ok so how do we compute this all the other terms are covered this is the only one that we do not know so we will just look at one element of this t ensor and it is going to be s k p w q r so let us just see that you have s k as this vector and you have w as this matrix so i am considering one such weight which is w p comma q and one such element from here which is s k sorry so q r and i am considering one element from this which is s k p so i am trying to compute the derivative of one element of the vector with respect to one element of the matrix so this is going to give me one entry in my tensor and that entry is going to be what p q r how many of you are fine with this ok fine so now recall that a k was equal to w into s k minus one plus b and s k was sigmoid of a k i think again i have miss that u into x k but that will not matter because that is not there in the derivative ok  you are fine with so far so now let us look at this because the other two terms do not matter so i just look at a k is equal to w into s k minus one so this is the matrix way of writing it ok now i am looking at one of th ese elements which actually comes from the multiplication of a row and a column the highlighted row and the column everyone gets this ok now so i can write it as a k p is actually equal to this summation which is nothing but the dot product of this row with this column ok now s k p is just the sigmoid of that so now if i want to compute s k p with respect to w q r i can just write the chain rule that s k p with respect to a k p which is straightforward and then a k p with respect to w q r and i already have a formula for a k p how many of you are fine so far please raise your hands high up if you are fine ok so what is the first term going to be sigma student refer time fiveten sigma prime of a k p and what is the second term going to b e this is what the second term is now what this is lot of terms here which of these terms would actually remain only the once where only the terms where i is equal to student refer time fivetwentyeight r and student refer time fivethirty p is equal to q so only that term will remain in that case it would be this right and in the other cases going to be zero right so now you have one element of this tensor and you have it as a very generic formula you can just fill in all the elements of the tensor right so what does this tensor look like it is a very student refer time fivefiftytwo sparse tensor right that is all i wanted to convey ok so this is again the same thing right is that fine so even though it is a nasty looking tensor if we just br eak it down to one element it is going to be very easy and now from this element you can just reconstruct the entire tensor do not worry i am not going to ask you to implement this but if someone were to maybe at some point then you should be able t o do it right that is where we will end today so we have finished recurrent neural networks and the next thing that we are going to look at is lstms and gated recurrent refer time sixtwentyfive ok thank you"}
{"audio_filepath": "Data/Preprocessed/GloVe representations_83.wav", "duration": 434.0, "text": "so now from here we will move on to yet another way of learning wo rd representations which is known as the glove representations so the count based methods rely on global co occurrence counts from the corpus for computing word representations that is what we saw in svd t hey look at these co occurrence counts and from there they build the word representations t he predict based models set up a learning problem where you have this feed forward net network and it tries to predict certain things from the given words and then you learn the parameters of that netw ork and you set up the task in such a way that the parameters actually correspond to word representations  so this was the difference between count and predict based methods  n ow what is the obvious next thing to do like hear the answer from a few of yo u  but i want to hear it from everyone w hat is the obvious next thing to do you have count based methods you have predict based methods combine the two right so come up with some kind of a hybrid so that is exactly what glove does which is known as global vectors refer slide time oneten so i will go back to the co occurrence matrix  so remember x ij encodes the important global information about the word i and j and whether you replace it by pmi or ppmi or just keep the counts it just gives you some information about how many times these two words actually appeared together so x ij encodes this global information and i call it global because it is computed from the entire corpus fine  w hy not learn word vectors which are faithful to this i nformation so what do i mean by that  s uppose v i is the representation of the i th word and v j as a representative the j th word which i want to learn i do not have these representations i wanted to learn  n ow this gives me the dot product between th em which gives me the similarity between them w hy not i set up my task in such a way that this similarity is actually proportional to this probability so what does a similarly tell us how well these two go together what does p of j given i tell us h ow likely j is given i right  so does that make sense to have this analogy that the dot product tells me the similarity the other notion of similarity is that how likely j is to appear in context of i which is given by p of j given i  so why not set up my task such that whatever vector as i learn are actually faithful to this global similarity that i have computed from the entire corpus how many if you get this intuition h ow many if you see the difference between this and the predict based models i n th e prediction based models you are operating at one word pair at a time here you are looking at these global counts ok w e are trying to directly learn vectors which are faithful to your global similarity as given by your co occurrence counts you get the merger between the two methods you should not get it yet because we still have to do something or at least you get the intuition n ow what is p of j given i i t is actually this ok so i can write it as this ok and similarly i can write the other guy v j transpose v i and that is going to be different because that is going to have p i given j instead of p j given i so i will have log x ij is fine  but instead of x i i will have x j here refer slide time threethirtyfour now if i add these two equations  s o i am going to add this equation and this equation so the left hand side i just get two times v i transpose v j because v i transpose v j is the same as v j transpose v i and on the right hand side i get certain quantities  so this is what i would actu ally want my word vectors to look at look like i would want my word vectors to be such that when i take their dot product they give me the quantity on the right hand side and this quantity has come based on counts learned from the corpus so i have coun ts on the right hand side and i have learnable parameters on the left hand side  so you see how we are merging these two  but how do you learn this problem n ow it is ok to say what i like what i have said now is that this is what i desire i desire tha t my word vectors should be learned in such a way that they are faithful to the global counts through the following equation  t his is what i desire  d esiring something is one thing  but now how do i set this up as a learning problem so when i ask you wha t is the learning problem what do you need to think about o bjective function good that is a good start  so what is the objective function for this what are the parameters of the optimization you are optimizing with respect to what  t he v i is and the v j s right all the word representations how many of those do you have  v each of size k  so those are your parameters for optimization now what is the loss function  i f i give you the loss function it will look very very obvious  but i do not want to do t hat so just continue thinking about that while i will make some more simplifications to what we have here now what is this count  t his is the co occurrence count how many times these two occur together what is this count  t he number of times the word i appear  so this depends only on i what is this count t he number of times the word g appears so to make the model more flexible  that means give it some more freedom what i am going to do is instead of log x i and log x j i am going to introduce para meters b i and b j ok refer slide time fivethirtytwo i am saying that these parameters can also be learned  so effectively using all these three i should be able to get this this is what i desire now set up the loss function  u sing these two things come on that should not be s o hard w hat is this t his is what you are trying to predict what is this  t his is what you know is true because you have computed from the corpus now can you come say the loss function the difference between these two right  so you could have this as the loss function this is the predicted value using models parameters this is the actual value computed from the corpus so think of this that you are trying to learn the parameters in such a way that you end up predicting this and if you predicted this you know you have done the right thing ok and this you know already because you have computed it from the corpus  so this is the true value and this is the predicted value  so as in any loss function predicted minus true the whole square does that make sense how many if you are fine with this refer slide time sixfortyfour so now how will you train in this network  g radient descent  so i will use gradient descent and you will get these parameters so there is a bit more on this wh ich i will not cover actually  so i will just skip this slide you can go back and take a look at it  i t is a some slight modifications to this yes so again the same idea that cat will go close to all the  so here again you will have the v i and the uc s right  so you will have cat will come close to all the words that it co occurs with feline will also come close to the same words  so maybe i have not used to right notation here if you need to change it again  so we should have v i s and u j s righ t so again you have one word matrix word representations and the other is the context representation then it is fine right that is the problem here how many if you get that right a gain we have to have these two things let us change that everywhere"}
{"audio_filepath": "Data/Preprocessed/Ensemble Methods_67.wav", "duration": 429.0, "text": "so next we look a t on ensemble methods and thi s is just to buil d the intuit ions for something known a s dropout whic h is very popular tec hnique in dee p neura l networks and convolution neural networks and even recurrent neural networks so how many you have seen ensembles before see n it in machine learning ensemble was not done in machinery done with ok ravi did it so as a combine so the ensemble is essentially just the combining the output of different models to reduce the generalization error right why does that make sense ha ve these different models all of these would have different biases and variances right so now you are combining them so i will end up with a bette r thing on the test error right so that is the idea behind ensemble now the models could correspond to d ifferent classifiers right for example here i have a logistic regression and svm and a naive bayes i have trained them independently using the same data or different subsets of the data and a test time i am taking a prediction from all of them and then taking an ensemble of those predictions that is the basic idea now it could be different instances of the same classifier trained with different hyper parameters i could have the same neural network a three layer neural network but trained with different hyper parameters so the hyper parameters could be learning rate it could be batch size it could be the number of neurons in each layer and so on right so it could be same classifier but different hyper parameters different features right so inste ad of looking at all the one hundred features that i have given i could train these classifiers with different subsets of the features ok or different samples of the training data so bagging is one such ensemble method where you have different instances of the same classifier which are trained on different samples of the training data ok so i have one classifiers trained on a subset t one of the training data another classifier trained on a subset t two of the training data and so on right and so each of these model is trained with a different sample of the data now when would bagging actually work what would you want these classifiers to be so each classifier is going to make certain errors what do you want these errors across classifiers to be dependent independent student independent independent right  so  if one classifier makes the errors on certain test instances other classifier makes errors on a different set of test instances and the th ird classifier makes errors on a very different set of instances that is the condition that you are looking for right there is errors if all of them make error on the same instance then all of them are collectively going to make an error on the final pre diction also right because it is like i asked three guys all of them gave me the wrong answer so my final answer is going to be wrong but at least two of these three guys gave me the correct answer then my final answer is going to be correct right so  that mean s the errors that these models make i want these errors to be independent if i treat error as a random variable i want these errors to be independent so so consider a set of k such logistic regression models suppose that each model makes an error epsi lon i on the test example now let epsilon i be drawn from a zero mean multivariate normal distribution so the variance is equal to v and how many such epsilons do i have how many such distributions i am considering student k k right because for each c lassifier there is a distribution so then i can compute the covariance between these random variables ok i will add that let that covariance be c is that fine ok now the true the error made by the average prediction of all the models is going to be giv en by this model one made an error of epsilon one model two made an error of epsilon two so the average error is going to be given by this  now what is this expected squared error this is the error this is the expectation this is the square that is the exp ected squared error is that fine again this is a square of a sum  so  it will lead to a lot of terms of the form epsilon i squares and what will happen now which terms will go to zero the terms having epsilon i epsilon j again th e same thing they are independent so i can write the expectation of a product as the product of expectations and those expectations are zero so this is what it is going to look like what is this oh sorry actually we had not assumed that the covalence w hat is this right and what is this covariance i am sorry i have not we had assumed that there is some covariance said wed not assume they are independent right we would want it to be independent but in the general case we will assume some covariance an d then i will show you the special case where they are independent so then how many vs do i have here k right and how many cs do i have here this summation is k into k minus one right or i equal to one to k and j equal to i plu s one to k fine and so this is what it looks like now can you make some inferences from this equation this is what the expected mean square error is going to be now think in terms of variance covariance and tell me when would this be beneficial i have already told you the answer if the errors are independent what would covariance be zero right so then what is the mean square error one by k one by k into v right so that means bagging would work when your classifiers the k classifiers that you are combinin g if the errors are independent then the mean square error should actually have been v right for a single classifier it was v right because mean square error is nothing but the expectation of the error expectation of epsilon i square which is nothing but v but if you are if you are combining k classifiers and if these classifiers are independent in terms of their errors then your mean square error is going to be one by k into v because this term is going to disappear ok now if your classifiers are perfectly correlated then what would happen and basically c is equal to v right is that fine so now what would happen what is the net result if i substitute this as v going to be v right so if you are all your classifiers ar e perfectly correlated that is the other case we had tried taken and all of them are making errors on the same test instances and the same errors right then you will not get any benefit of doing bagging but if you look at the other extreme where all your errors are independent or all your classifiers are making independent errors then you will get a benefit your expected mean square error would go down from v to one by k into v everyone gets that so this was just to devel op an intuition that taking an ensemble helps right and using this intuition now we are going to see at how to do this ensemble in the case of deep neural networks"}
{"audio_filepath": "Data/Preprocessed/Adding Noise to the inputs_64.wav", "duration": 438.0, "text": "we go down the next module which is adding noise to the inputs right refer slide time zeroseventeen so we have some kind of a noise process and now can you relate that how that was related to regularization that was exactly the motivation in that case that we could have an over complete auto encoder which is a very complex model because it has a large number of parameters and to avoid that we were adding this noise to the inputs so that even if it tries to minimize the training error it is not actually minimizing the true training error right b ecause you have fed some noise to it everyone gets this  r ight o k now actually we can show that for a simple input output neural network right that means you do not have any hidden layer you just have a set of inputs and you have the output layer then adding noise to the input or rat her adding gaussian noise to the input it is equivalent to weight decay so this can also be viewed  so  we will do this part right so we will just quickly do a small derivation where we show that adding gaussian noise to the inputs is the same as doin g a l two regularization that is a very neat idea so this can also be viewed as data augmentation right exactly what i shown on the previous slide you added two you just corrupted some inputs of it that is the same as adding noise to the data so the essen tially augmenting the data right you have some training data and just augmenting it so to get more training data is that fine now about this smallest derivation this is again just a set of steps i will go over it reasonabl y fast i will give you the set up and then it is quickly work through the derivation right so what i was trying to say is that if you have a simple input output neural network that means you just have inputs and the output you do not have a hidden lay er right then adding a gaussian noise to the input units where the noise comes from this distribution it is a gaussian distribution zero mean i want to show that doing this is effectiveness the same as doing l two regularization now again see this is the s ame thing squared eggs in vacuum because this is not the kind of networks that we deal with but it is good to see what happens at least in these neat conditions because we will never have a simple input output network at least not in this course we wil l have a deep neural network always so but at least see what happens in the simple case right so what we are doing is from the x is we are creating a noisy x i by just adding some epsilon noise to that and what is our model going to be it is just an aggregation of all the inputs ok so this is what our original model would have been without the noise fine i would have just aggregated all the inputs i am assuming there is no non linearity at the output and i am just taking y i is equal to summation of all my inputs everyone fine with this side or this is too simple for you guys to understand because we have been doing a lot of deep neural networks so suddenly one layer network i do not know what it is everyone gets it right and instead of y hat  now i have y tilde because instead of x i i have x i tilde ok but what is x i tilde x i plus epsilon i right so i can write it as this just fine so actually y tilde is nothing but y hat plus some quantity ok what are we interested in always this quantity the expected mean square error ok i mean expected squared error and why not y hat so we have added noise to the input so now y tilde are the outputs that we are going to tilde so let us see what that quantity is and again just going to be some simple stuff so i replaced y tilde by this that we just derived on the right hand side on the left hand side ok so i am going to take these two terms together so  i can write it as this plus this the whole square fine and i am going to keep this as it is what is this quantity the original squared error expected squared error right when i was not adding noise to the inputs ok and you see how we got these two quantities this is just a plus b the whole square is equal to whatever it is equal to ri ght now let us look at the last term this is a square of a sum right so what kind of terms would you have inside you will have some terms which are epsilon i squares and you would have some terms which were epsilon i epsilon j right ok so we will hav e some expectations which are going to be something into epsilon i square and some expectations which are going to be epsilon i epsilon j everyone gets this some terms there now which of these terms would disappear student refer time fivenine the se terms right why because the noises are independent ok i am not if i have drawn a noise f or one instance it does not have any influence on the noise that i am going to add to the next instance if i have taken one x i corrupted it with some noise th ere is no bearing on the noise that i am going to use for the next epsilon i right all these features are the noise added to the features are independent so now from these terms only the square terms are going to remain is th at fine and similarly this quantity what can you say about this we just did something similar why i am a saying that this is going to zero again i can show that this is the covariance between this random variable and this random variable ok and now are these two random variables dependent  what is epsilon i the noise that i am adding to the input does it have any effect on y hat no right because y hat does not depend on the noise what is y true output does it have anything to do with the noise no ri ght so that is why these two random variables are independent so i can again write there the expectation of their product as a product of expectations and then the expectation of this is going to be zero because epsilon i was drawn from a zero mean distribut ion is that fine everyone gets that the same trickery that we did earlier so this is the quantity that we are left with you see how i got from here to here this is an expectation of a sum which is equal to a sum of expectations w i has nothing to do with it is not a random variable so it is just the expectation of sigma i square which is nothing but the variance right so i get this what does this look like i already told you the answer before s tarting right this looks like l two regularization thi s is the true error i mean this is the empirical estimate from the training error and this is the weight decay term everyone get this how you see that this is an equivalent thing so at least in this neat set up you get the intuition that adding noise to the inputs is a same as adding a l two regularization term"}
{"audio_filepath": "Data/Preprocessed/Skip-gram model (Contd.)_80.wav", "duration": 470.0, "text": "so what i will do is i will quickly  go over what we were doing yesterday  and then by the time people come in we can start with the new stuff right so we were looking at so that is so this needs to be corrected someone who pointed out yesterday  same as bag of words it should be same problems as the bag of words model so we are trying to fix this problem where we have this large softmax computation which is very inefficient and you wanted ways of getting rid of that so the first thing that we were looking at is using negative sampling and here the key idea was to con construct this d and d prime where d prime was the random corpus and d was a true corpus and how do you create this random corpus is something that was left at the end and which i need to go over today  so i will go over that and then we realize that this actually could be modeled using such a network where you take the dot product between the word representations and try to maximize this to dot product for all the correct pairs by setting up your loss function accordingly  and try to maximize or rather minimize this dot product minimize this dot product for all the incorrect pairs by again setting the objective function appropriately  so we had this objective function where we want to maximize the probability that the pair is correct for the correct pairs and maximize the probability  that the pair is incorrect for the incorrect pairs and both these probabilities we had modeled using a sigmoid function and inside the sigmoid function we had the dot product between the corresponding representations so the net effect is you either maximize the dot product of the correct pairs or minimize the dot product of the or rather and in minimize the dot product of the incorrect pairs fine and then so now today the part which was remaining about the comparison between d and d prime so what i was saying last time is that d prime is actually k times d that means in sample more negative examples than positive examples so if you think about it actually the number of negative examples in the language is much much more than a number of positive examples let us say if you have fifty k words in your vocabulary most of them do not appear together  right so that number is actually very very large as compared to the number of words which can occur together  so how do you account for this natural imbalance so they said that if you keep it same then we are saying that the size of d prime and d is going to be same that means the words which appear together and not to appear together we are keeping those two corpora as the same so that does not sound reasonable so they decided that we will keep it k times ok now this k was a hyper parameter which was tuned based on the data that they had and can you guess how they would have tuned it no what do you tune your parameters on what did how did you tune your parameters for the back propagation of the word no using what student validation set a validation set is it too early in the morning it fine validation set so they might have had some validation set and if you look at the original word to word code which someone had posted yesterday  which allows you to compute the distance matrix right so you could what you could do is you could learn these representations take a few pairs of words and take a few pairs of good words right say cat and dog or cat and feline and so on and also bad words like cat and truck bad combinations rather  and see if the distance between cat and truck is much higher than the distance between cat and feline or cat and dog so you select that k which gives you the best performance on your validation set and the validation set here would essentially be to find if you get good representations for word pairs that you care about and for word pairs that you do not care about ok now the other thing was how do you create this r so you have v words in the vocabulary you are looking at one of those w  you know that some of those have appeared with w in some context but there is this large set which has not appeared with w in any context right so you are going to draw r from this set and the simplest thing to do would be to just draw the uniform distribution that means all words and let us call this suppose there are capital r words here all of these words could be drawn from using the probability one by r where r is less than v  is that fine that is one way of doing it just randomly pick any word from the remaining words and put it a pair it with w but you would also want to account for the individual frequencies of those words right if the word is actually very frequent pair it up more with w if it is not frequent do not pair it up enough does that make sense so i could actually use the frequencies of each of these words and sample according to that frequency right instead of using a unigram distribution so they did something similar  but they had this hyper parameter again so basically i was sampling using the probability of r which is equal to count of r divided by the number of times number of all the words in the corpus that is actually the frequency of r divided by the total number of words in the corpus so instead of just taking that they had this wearied factor of three by four do you we realize that if you take this three by four you get the best performance so let me just make a few comments on that so the original code of or rather the original skip gram or the bag of words model actually worked very well and it kind of hard a lot of seminal effect or a lot of revolutionary effect on the field of nlp right so now everyone started talking about word vectors and how you can use this meaningful representations of words as features for various down steep nlpthus right so at the end in nlp what you are doing is you are collecting of a bunch of words a document or a sentence or something and trying to do some processing on that now earlier used to construct features out of these sentences using some handcrafted features but now someone said that there is this automatic way of constructing word features right which is using this method so people really bought onto that idea and a lot of work started happening and then later on at the end of the course we will see something that what it eventually led to but later on when people started analyzing this more carefully right they realized that the original wordtwov ec implementation had a lot of these heuristics or lot of these parameters which need to be really tuned to the core for it to be able to compete with svd right so that is what we look at the end so svd was already one way of computing word representations ah which while popular was not so popular it was used for various reasons but it was not like every npl application is using svd representations right but now it is almost like every npl application is using word representations so later on we will see that some of these things like three by four or k the value of k the value of learning rate and some other hyper parameters if you really tuned them very very well it is only then that as this wordtwov ec algorithm can beat the world representations learned by svd or rather the other thing that if you introduce some parameters in svd and tune them because remember for svd there was no tuning right we just got a solution we just had the closed form solution which is the eigen vectors but you could do some things for creating the cooccurrence matrix if you introduce some factor there which is also looks like this three by four or something like that or if you also introduced something which looks like a k then you will be able to get the same kind of representations or equally powerful representations from svd as what you get from wordtwov ec so that is why i am stressing on these hyper parameters there is some significance of those"}
{"audio_filepath": "Data/Preprocessed/Optimization over images_98.wav", "duration": 583.0, "text": "ok the next thing that we are going to do is optimizat ion ove r image s so this is again interesting a nd it eventually led to thi s whole fie ld of adversaria l dee p learning or adversarial machine learning in general right so we will see what this is suppose i have a trained convolutional neural network ok and now i wa nt to figure out what kind of image should i pass through this so that it gets recognized as a dumbbell why we want to do i would not want to have such a weird objective can you think of a reason why would want such a weird objective i know there is a convolutional neural network which can distinguish k classes these classes could be anything now i want to deliberately create images which get passed as the dumbbell class why would i want to do this ok you are going into your details so i will give you a application right suppose this network is supposed to do face detection and the k classes which are there are k people right now you want to see what kind of image should i feed to this so that i get recognized as amitabh bacchan right so now that could have certain benefits and various high places and so on its i would want to do that right so thats the whole idea behind adversarial learning so now i am asking this question that i want and here its in of course a toy setup there is no reason i why i would want to generate dumbbells but say if i am going to if its an automatic verification whether my product looks like a dumbbell or not i might want to do this right so you could think of all sorts of reasons why you want to d o this so what we will do is the question that we are interested in is that i have a blank slate with me it just contains some pixels i want to be able to modify this pixel so that my class dumbbell class gets fired now we have done enough gradie nts enough back propagation everything in this class so i will ask you to give me a solution for this and the hint is treat the image itself as a parameter matrix the second hint is assume that all of this is going to remain constant you are not goi ng to change any of this and you have initialized your parameters which is the image pixels to zeros that means you are started with a gray image now i will change the question a bit only a bit and all of you will be able to answer this ok suppose my network is strained and now i want to change the weights in this layer so that my accuracy improves so that when its a dumbbell class it predicts dumbbell how will you do that it will pass the same image what will you do how will you change the we ights in this layer back propagation what is the update rule say the gradient descent update rule say that the gradient descent update rule student refer time threeone w is equal to ok you guys actually unanimously said gradient is an update rule ok  so w is equal to w minus oops oops oops ok minus eta into ok student refer time threesixteen thats what you will do now if i ask you the question for this you can answer it but if i ask you the same question here why cannot you answer it student refer time threetwentyseven so here what were you doing computing the gradients of the loss with respect to the weights what will you do here student refer time threethirtythree it put respect to each of these pixels and then update this pixel by using what formul a student refer time threethirtynine ione thats the first pixel is equal to ione minus eta gradient ione where what is gradient ione actually everyone gets the intuition right you can do it now so we could pose this as an optimization problem where what we want to do is given an image we want to maximize the score of the output class and i also want some regularization because whatever i get i want it to look like an image right so we will see different types of regularization fo r doing this some very simple regularizations but this is the overall idea right so any generic loss function is always the training loss plus the regularization so i have just kept both the training loss as well as the regularization whats my traini ng loss the score for the class that i am interested in and what are the parameters of this object of this optimization problem the input pixels right so far we had already be always been doing w b but instead of w b you now have i as the parameter s of your optimization problem is that fine and now we can just think of the entire image as a collection of parameters and we can now update the weights of this matrix which is the image matrix ok refer slide time fourfiftyeight so let us see how we will do it so we start with a zero image as i said set the score vector to all zeros and one for the class that i am interested in ok now compute the gradient of this score vector with respect to i k  its i want this quantity to b e maximized everything else to be zero so thats what my loss function is so i am going to compute the gradient of each of the pixels with this now i am going to update the pixel using my gradient descent rule which i just explained brief previously now i again do a forward so now instead of this zero image i have a modified image slightly modified image because the pixels i have moved away from zero update based on the gradients now this image i will pass back through the network and what will i do now a gain change so this is the same as the weight matrix right so you should be able to visualize it exactly the same way as you would have visualized this you had certain weights here you change them a bit again did the forward pass again did the backwa rd pass change them a bit and keep doing this till student refer time fivefiftynine till convergence right whatever is your definition for convergence till you are satisfied and instead of score of one you are at least getting a score of zeronine or zeroninetyfive or somet hing like that right so we will keep doing this right till convergence at the end you will you all of you can imagine that this image will keep getting modified ok so now let us see if we learn run this score or the run t his code for certain classes so i mean interested in the dumbbell class and i have ran that algorithm starting with the zero image and this is the kind of image that i end up with you see a dumbbell here without me drawing it right if you go back and loo k at it you will see that there are a lot of these dumbbell like shapes which have actually appeared here the colour is of course very much different i dont think dumbbells are of these colours ever but you can see that its actually trying to produce t hat shapes which will cause the dumbbell output to fire now what is interesting is that its being very redundant so its not trying to generate a single dumbbell a generating a lot of dumbbells of different orientations so i just keeping its basis covered so that some of this should actually fire and cause a dumbbell output to be maximized ok now let us see if we take a cup and this is like the trophy cup i believe so this is what is appearing here there is one mo re cup here and there is one more cup here its a generating these cups so that you cant be you would not be able to see it its different oh it really looks like i am manipulating it but i am not you can go back in check it those cups are there ok and then for dalmatian actually this at least you can see some white and black spots right at least thats fine so dalmatians are these dog which have these white and black spots so and you can also see some kind of a shape here right which with my drawing so it is actually producing that doglike shape and its producing multiple of those so its being redundant i am trying to compute that right and now you see right with these very arbitrary images which to you and me do not know nowhere close to we will fire will classify this as dalmatian but for the machine and is classifying this as a dalmatian and this is bad right this is not good there is nothing to be impressed about this is actual ly bad because i can give it these horrible images and still get away by something called as a dalmatian so if i want to sell some a dalmatian on olx this is what i can do right i can upload this image and a machine would trigger it and some one would buy it ok so and this is a bell paper so you can go back and see you see a lot of bell papers here and similar for lemon and so on right so various classes you can see that its actually trying to produce those shapes but its nowhere actually producing a clear image which is undoubtedly of that object right is generating something which can later on be used to fool the network right which is not a good thing ok and we can actually do this fo r any arbitrary neuron so i was trying to actually fire this neuron which was the output layer but maybe i want something else to fire here so i want to actually see what is it that causes this neuron to fire so i could repeat the same algorithm by setting something here as high and then again back propagating the gradients only from here and reconstructing the image every time so that this neuron then five is right so these are what the updated images look like which e xcite certain neurons and some layers so what does this look like its actually like a pirates ship if its not very clear you have these multiple layers of things and something like this ok so its some neurons are actually firing for this kind of a pa ttern there are some other neurons which are firing for different kinds of patterns and so on right so you can just create images which cause certain neurons to fire and all these are lot of fun to do so you should i would encourage you to do this i will get more insights into what your network is"}
{"audio_filepath": "Data/Preprocessed/ The Madness (2013-)_8.wav", "duration": 250.0, "text": "so this was all happening where deep learning now started showing a lot of promise in a lot of fields nlp vision speech and again this deep reinforcement learning and so on which led to this complete madness starting from two thousand and thirteen well almost for every application the traditional methods were then overwritten or kind of beaten by deep neural network based system so something like language modelling which has been around since probably one thousand nine hundred and fiftys or so nowthe reining algorithmor the better algorithmfor languagemodellingis now something which is based on deep neural networks then similarly for speech recognition lot of work a lot of probabilistic lot of work based on probabilistic models was done in this or in the speech area or the speech literature for the past thirty forty years a nd now all of that has been overcome by deep neural network based solutions same for machine translation a lot of interest in this field a lot of companies now have their machine translation systemsbased on deep neural networks as opposed to the earlier phrase based statistical machine translations or the probabilistic models which were used earlier similarly for conversation modelling dialogue a lot of new work started in dialogue post a deep learning era where people now realize that if you have a lot of sequences of conversations you could actually try to train a deep neural network to learn from this sequence and have conversations with humans of course you are nowhere close to human level conversations we are very very far off from them but in limited domains these bots are showing some success now same for question answering where you are given a question and you want to answer it either from a knowledge graph or from a document or from a image and so on and in the field of computer vision things like object detection most of the reigning systemsorthebestperformingsystemsnowadaysaredeepneuralnetworkbased systems a lot of advances are being made on these systems over in the last few years same for visual tracking where you want to track the same person in a video or image captioning where you want to generate captions for images for example people upload a lot of images on facebook and if you want to automatically caption them or imagine you are on a reselling site right something like olx where you upload your furniture and you do not provide a description from that but can the machine already automatically generate a description for it so it is easier for the human to read what that product is and so on so similarly video captioning i given a video anyone to caption the main activity which is happening in that video all of these problems are being solved using deep learning based solutions using a combinationof something known asfeed forward neural networks or convolutional neural networks or recurrent neural networks and so on visual question answering you are given an image and a question and you want to answer that question video question answering answering questions from videos video summarizations if you are given a large video and you want to generate a trailer a sort of a trailer for that video contains which kind is the most important frame for that video even these systems are based on deep learning then this was all about classification recognition and so on but now people started getting more ambitious that can we humans are very good at creativity so can we use machinesto be creative right to generate images so now if i have seen a lot of celebrity faces can i generate new celebrity faces or if i have seen a lot of bedroom images and i am if a fireman architect now can i generate new bedroom images can i can we train a machine to generate new bed bedroom images so a lot of phenomenal progress or work has happened in this field in the last four five years starting with things like generative adversarial networks variational autoencoders and so on andpeoplearenowstartingtoseriouslyinvestintocreativitythathowtomake machines creative again we are far off from where the desired output but there is still significant progress happening in this field generating audio so that was about generating images you can generate music also and this is again about generating images and so on"}
{"audio_filepath": "Data/Preprocessed/ Bias and Variance_57.wav", "duration": 608.0, "text": "so in thislecturewe are goingto talk abouta bunchof regularizationtechniquesfor deep neural networks you might find some very familiar terms here for example ltwo regularizationperha ps something e lse alsobut i promise y ou that we will see a very different interpretation of this from wha t you have done in your earlier courses right so againas is thetrendin thiscourse i will startwith some basicconceptsi will take todays lectureto finish offthe basicpartwhichis the bias variancetradeoffand i will try to make it more informative the n what you have done in your earlier courses and in the rest of the lecturewhich willhappenon fridaywe will buildupon thesebasicsand then try to look at these as the regularization forms so let us start  so these are the sources which i have looked at so one of them is the c hapter seven from deep learning book o ther is this very good lecture by a li g hod sis on r egularization and of course this p aper on drop out  so let u s start with b ias and va riance  a gain some five ten minutes would be similar to what you have seen in the middle class  but then i will go on to something different refer slide time onetwentytwo so we will begin with a quick overview of b ias v ariance and the trade off between them refer slide time onetwentysix so let us consider the problem of fitting a curve through a given set of points ok  n ow remember i have alw ays been telling you that t here i s always this true relation between x and y which is f of x right and which we never know so w e do no t know what this is  i n the movie example w e do no t know what this is in the credit card fraud detection or in the oil mining  e xample  i n this p articular example i know it r ight so what i have done is  i know that the true relation between x and y is the sinusoidal c urve  i know this  but instead of giving you every point on this sinusoidal curve what i have done is  i have such sampled some poi nts from it  i hav e taken some points and given to you refer slide time twoeleven so from now on we will behave as if w e do no t know that this is how it came  it i s a big secret and we now want to fit a curve to this  t hat means i want to learn the fu nction f hat of x ok  w hich of course will have some parameters and what will be my goal is that now let u s look at this a gain my goal would be if i feed at this point after the model is train the output should be as close to th is point as possible  t hat is our training c riteria e veryone gets this refer slide time twofortyeight so we consider two models  t he first model is a simple model  h ow many parameters does i t have s tudent two t wo parameters right t he other model and this is what happens when i tr ain the simple model  o f course i will get a line  but do you see something special about this line  w hy did i get this as a line or this as a line so on average it i s trying to minimize the distance from all the points  if i have this as the line then i will have a very high error for these points right so just something which goes along the average and hence the sum of the squared errors would be minimized right so it i s important that when you see these figures you should make these connection s to the math behind it  so this is the geometry you have to make connections to the math behind it right and i hope all of you make that connection  n ow  i take a complex model which is a degree twentyfive polynomial ok so this is w one x w two x square w three x cu be and so on it i s a degree twentyfive polynomial that i have used and i again learn the parameters of this using how will you learn the parameters  y ou have a quiz two days from now on g radient d escent w hat else do you know if you know any other algorithm  o f co urse you know  but getting this end right what else will you use y ou can use gradient descent for learning t hese parameters the same idea right y ou will define a loss you will compute the gradients with respect to all these parameters  h ow many of th em are there h ere twentysix and just update those parameters till a fixed number of iterations or any convergence c riteria ok and this is the curve which i get for the complex model  n ote this in both these cases we are making an assumpti on about how y is rela ted to x right i n this case  i made a simple assumption  i n this case  i made a slightly complex assumption  but in both the cases w e do no t know what is true r elation i s t he true r elation is actually the sine curve  but w e do no t know that w e a re just making an assumption  so you remember the five things in machine learning you have a data you make an assumption about how the input is related to the output  so these are my two assumptions  t hen  i have some parameters  y ou know the number of parame ters in these cases i use a learning algorithm which happens to be gradient descent and then i minimize an objective function which would be squared error loss in this c ase fine now the training data actually c onsists of one hundred points ok  b ut you d o no t see one hundred points here refer slide time fourfiftyeight so what i have done is  i have sampled some twentyfive points from here and use that as the training data  so i ha ve learned my parameters w one and w naught or w twentyfive up to w naught  u sing these twentyfive points no w i will r epeat this experiment k times  w hat i do is every time i will get a different sample of twentyfive points and i will try to learn the parameters of the model w ill i get the same curve every time w ill i get the same function every time n o my parameters would change slightly right b ecause my training data is different so i am trying to learn it differently to adjust to that training data  so my function is going to be different  it is the same form  it is either the linear function or the polynomial functio n  but the parameters the coefficients are going to be different refer slide time fivefortythree i will actually draw these different functions and w e wi ll make some observations from that  so this is the black curve that you see is the true sinusoidal curv e from which the data has come  t he blue line is one of these functions which i have trained from one random sample of the data right n ow  i train different functions from different random samples of the data and see what happens  i get different lines t his obvious can you relate to this every time  i am basically learning a different value of w one and w naught  i s that ok a nd i have done this twentyfive times and plotted these lines w hat do you observe with respect to each of these if you compare any line to a ny other line so if you compare one of these lines to the remaining twentyfour lines what do you observe t hey a re very c lose to each other  th ey a re not very different from each other however there is a p roblem  t hey are very far from dash the actual function  t hat means we are under fitting  w e have very few parameters i n fact only two that i s why we are under fitting  let u s look at the other case fine  t his is the function the polynomial the blue curve that you see is the polynomial that i learned fr om on e random sample of the data now i a m going to learn this from a different sample of the data  y ou see what happens y ou see that the green curve is actually very different from the blue curve  y ou see that here actually this was peaking whereas this is going down  s imilarly this was peaking but this is going down and so on  so you see that there are clear differences between the two curves and if i draw the next curve you see it i s even more different  t he same function learnt from different data poin t is turning out to be very different  w hy  b ecause it i s over fitting on those twentyfive points that i have given  t he simple model did not even have the capacity to do or fit because it is just two parameters h ow much can i over f it  i will just end up drawing the average line right  but here it is really able to overt fit and you see that these twentyfive curves or i do no t know how many curves that i will draw all of these are going to be very different f rom each other  y ou see that and everyone agrees that this wo uld happen if you ac tually try to do this ok so complex models train on different samples of the data are very different from each other w hat is happening there is over fitting ok refer slide time sevenfiftyfour now let me define two concepts f rom statis tics one is bias  b ias is very simple  it tells us that this is the true function  i f you a re trying to learn the approximate function and you do it many times then y ou wi ll get an expected value of the function  so it tells you how much does this expected value di ffer from the true function ok  y ou get the definition t he definition i s straight forward ok n ow for the simple line or the simple model the green line that you see is actually the average of all those twentyfive lines that you had see n ok w hat c a n you say about the bias  v ery high right because this difference is very high t his green line is very different from the red curve which is my true function right pr edicted and true function  n ow what about complex model th e blue curve that you see is actually the average of all those twentyfive different curves that i had drawn  so w hat i s the bias  it is v e ry low d oes that make sense  t his means that the simple model has a high bias and the complex model has a low bia s is i t c lear to everyone refer slid e time ninesix now let u s define another quantity which is variance  ev eryone knows what variance is so this is one of the functions that i have learned  t his is the average of that function and the vari ance tells me the spread  n ow based on the figu res that you have seen can you tell me what would happen for the simple model  l ow variance or high variance s tudent lo w variance l ow variance because all these models were very close to each other  t here was not much spread in the models  w hat about the complex model h igh variance  a ll these models were very far from each other  t he spread w as very high ok  so roughly speaking it tells us how much the different f f act f x i s that you a re learning how different are they from the average f of x r efer slide time ninefifty so informally i can say the following simple model has a high bias low variance complex model has a low bias high variance an d as always going to be a trade off between the bias and variance so why i s there always a trade off be tween the bias and variance p eople have done ml course  w hy i s there a trade off h ow many of you know the mathematical answer to that y ou have not done this in the ml course no  so it turns out that both bias and variance contribute to the mean square error and let us see how"}
{"audio_filepath": "Data/Preprocessed/Bias Correction in Adam_41.wav", "duration": 567.0, "text": "refer slide time zerotwelve so in this video we will try to look at an explanation for why we need b ias c orrection in a dam o r in other words i want to explain why do i do this particular step why did i take m t and v t as it is but why did i do this particular step which i calle d as the bias correction step refer s lide time zerotwentyone so note that in the case of a dam if you look at this equation for m t we are actually taking a running average of the gradients and storing it as m t right s o this is the gradient and we are taking a running average or exponential runn ing average of these gradients exponentially decaying running average s o the reason we are doing that is that we do not want to rely on a single estimate so we do not want to rely only on gradient of w t we want to look at the overall behaviour of the gradients over multiple time steps and then take a decision s o that means in one particular gradient at time t is actually pushing us in some direction we do not want to be very hasty and start moving there we want to accumulate the history and appropria tely weigh everything in the history that is the idea behind taking this running average of radiance a nd the other way of looking at is that we are interested in the expected value of the gradients and not the point estimate at time w t right a t time t rather so gradient of wt which is this quantity which is the point estimate at time t we are not interested in that were interested in the expected value and our behaviour should be according to the expected value that is what we desire s o however inste ad of computing the expected value of this quantity which should have been ideal we are computing mt as the exponentially moving average s o in the ideal case we would want that these two quantities are the same that the expected value of mt the way i am computing it and the expected value of the gradient of w t should be the same  i f that is the same then i am fine because then  that means i am just taking the expected value or the of the gradient instead of relying on the point estimate ok so let u s see if that is indeed the case refer slide time twofourteen so for convenience we are going to just denote this gradient w t as g t because it i s cumbersome to write this grad symbol and we will just not make it so readable the derivation that we are goin g to do s o i am just going to replace that as g t so what i have written is g t here instead of grad w t right s o from now on i will just use g t for grad w t is that fine ok so we have this expression for m t so now let u s just try to expand it and s ee what happens right so m zero it is going to be zero because that is my starting points i have no history nothings so i will just going to keep it as zero m one is my first time step at which it is going to be beta into m zero so i am just substituted t minus one and t here a nd in the original expression i have just substituted appropriate quantities for m of t minus one and g of t so m of t minus one is zero m zero and g of t is g one and of course b zero m zero itself was zero so what will be left it is one minus beta g one now let u s look at what happens is m two m two is going to be beta m one plus one minus beta g two  but i already have an expression for m one so i am just going to substitute that here and this is what i get n ow let u s look at m three m three is again going to be beta times m two plu s one minus beta times g three and i have an expression for m two so i am going to substitute that here and see if that leads to something interesting so i have just substituted the value of m two here right and i already had the m three part here the this term here as it is ok a nd now let u s see so this already starts looking something interesting you see some pattern here in particular we could take these one minus beta terms outside they can be taken common and then you will be left with beta square g one plus beta s quare g one plus beta g two plus g three so let u s try to write this more compactly right so i have taken one minus beta common and then i have written the remaining terms as this particular summation and you can verify so when i is equal to one this is going to be beta three minus one which is beta square into g one w hen i is equal to two this is going to be beta three minus two which is going to be beta into g two and when i is going to be three this is going to be beta raise to three minus three which is beta raise to zero which is just one in to g three right s o we get back the same expression that we had here of course there is a one minus beta outside so this is a more compact way of writing it and this was for the three th entry right this was for m three the third entry now what if we want to write it for the t th entry in general what if we want to write the expression for m t refer slide time fivefive so in general m t we can write it as one minus beta as i equal to one to t b beta t beta raised to t minus i into g i right s o this three is here i hav e just replaced them by t s right you can just verify that this is from you can just generalize from the third entry to the t th entry refer slide time fivetwentyseven so now let u s see we have the following expression we have simplified the expression for m t and written it more compactly  but what we were eventually interested in the expected value of m t right we wanted to show that certain things holds for the expected value of m t refer slide time fiveforty so you just take expectation on both sides s o this is what we will get ok n ow one minus beta is of course a constant so i can move it outside the expectation so then i get an expectation of a sum now the expectation of a sum is the same as the sum of expectations so i can write it as a sum of ex pectations ok n ow again beta is a constant so i can take it outside the expect expectation so what i will be left with is beta raise to t minus i outside and expectation of g i right s o this is actually expectation of g one when i equal to one then expecta tion of g two expectation of g three and so on now we will make an assumption that all these g i s  that means the gradient at time step one the gradient at time step two the gradient as time step three and so on they all come from the same distribution ok w e are going to make that assumption so let u s try to understand the implication of that right s o let u s say this was a distribution from which g one came right suppose i am dealing with a scalar quantity and maybe this was the distribution from which g one came n o w g two could have come from a different distribution g three could have come from a different distribution and if that was the case then expectation of g one would be different from the expectation of g two and so on so what we have assumed to it will make thing s simple for us is that g one g two g three any g i comes from the same distribution and hence you can say that the expectation of all these gi s is going to be just the expectation of g that is this one single distribution from these which these entries come t his of course a very strong assumption  but we are going to live with this assumption refer slide time sevensixteen so then this expectation of g i just becomes expectation of g so i have gotten rid of the index i  that means i can move it outside the su mmation right so this is what i will get now t hese two have come out of the summation and inside i have this quantity now let me just expand this quantity this is nothing but beta raise to t minus one plus beta raise to t minus two plus so on at last you wil l reach t minus t which is just going to be beta raise to zero so this is nothing but a sum of a g p with common ratio beta and i can replace that sum by this formula you know this is the formula for the sum of a g p with common ratio beta s o i have just replaced that and now what happens is this one minus beta and one minus beta cancel out so i get this particular expression that the expected value of m t is equal to the expected value of g into one minus beta t so i will just take one minus beta t on the othe r side and i can move it inside the expectation because it i s a constant it does not matter s o i will get as oh actually yeah i can just move it inside so i will get it as expectation of m t over one minus beta is equal to expectation of g t right and this quantity the one which i have circled is nothing but m hat t right this was exactly the bias correction that i was applying i f i go back to the previous slide or the slide before that so this was exactly the bias correction that i was applying so what i have inside is this so what i have shown is that if i apply the bias correction then the expected value of the bias corrected m t is equal to the expected value of the gradient and that is actually what i wanted i wanted that whatever m t i am computi ng if i look at its expected value it should be the same as the expected value of my gradients and that is what i have arrived it h ence this bias correction makes sense and hence we apply this bias correction for a dam s o this we have shown for m t we ha d a similar expression for v t right so for m t we had this bias correction as m hat t and similarly for v t also we had this bias correction as v hat t so you can derive the same kind of derivation for v t also and show that that bias correction makes se nse right s o this is an explanation for why you do bias correction in the case of a dam t hank you"}
{"audio_filepath": "Data/Preprocessed/Information content, Entropy & cross entropy_32.wav", "duration": 2623.0, "text": "so for the next module we need something known as cross entropy so we will just try to make some develop some intuitions for cross entropy and get to the formula for that and then i will tell you how it relates to the problems that we deal with ok so first let us start with something simple that what is it that we are trying to do ok so with that i will give you an example and i will ask you a few questions and then from there we will slightl y try to go toward s cross entropyso nowsuppos e you have an urn which has thousands of balls and these balls are of three different colors which are red black and white so you have an urn which has three different types of and there are many such balls which you have put in it and since you have put in it you know how many red balls are there how many blue balls are there and how many white balls are there so for you it is very easy to compute the probability of each of these things so that is say our probability is zerotwentyfive zerothirtyfive and zerofour now talking more formally what is happening here is that you have a random variable x which can take on the values red blue or white right and this is the probability of each of those or the random variable x taking any of these values ok so this is the setup now i am your friend and you tell me that you can peep into the zone you cannot actually count take out all the balls and count them and estimate the probability we can just take a look into this and try to give me an estimate of what these actually what are these probability values that means what is the value so what is the probability that x is equal to red or x equal to white or x equal to blue so i just take a look at it turn it around a bit and try to get some feel ok i see a lot of red balls but a fewer blue balls or white balls and so on and based on that i make my best estimate right so i will just say that maybe these probabilities are zerothirtyfive zerofortyfive and zerotwo right so this is actually the true distribution i will call it as p right because this is the correct one and what i have estimated i will call it as q and remember now p has you can think of p as a vector which has these three values p one p two p three because there are three possible events here and similarly q has three values q one q two and q three so in this case i clearly know that i am wrong or when i give you these values you know that you are wrong so you tell me that whatever you have estimated is wrong then i obviously ask you tell me how wrong was i so how would you give me that number that is the thing that we are interested in so the general problem that we are interested in is that there is a true probability distribution and there is an estimated probability distribution and we want to find out how bad was the estimation now can you tell me a simple way of computing this it may not be correct but still it makes sense student squared error you can just take the squared error so what you are essentially telling me is that you could just treat these two as any other vector right and you could take the squared error difference between these quantities so what you are telling me is this where i goes from one to three ok so this is one valid way of doing this but then we are ignoring the fact that this is a distribution and hence it has certain properties that the sum of the elements is one and so on all of them are positive and things like that so we are ignoring those kind of things we are completely ignoring the fact that we are not dealing with a normal vector but a spatial vector which happens to be a distribution so now we want to find out a more principled way of computing the difference between two distributions and in practice why are we interested in this because we will always have a true distribution and a predicted distribution so that is what we want to do we have some way of computing it but you want a better way of computing it now let me make a case for why do we care about such differences right so let me take a simple case of a classificationproblemand to motivatethati will start from a different example and then i will come to the classification problem suppose there is a tournament going on and there were four teams which leads the semifinals let us call them a b c d ok now you were following the tournament up to the semifinals and after that you didnt watch the tournament and you do not know who eventually won well the tournament is over and someone has won it i actually watched the tournament and i know that b has won it now can i express this in terms of a probability distribution right so first let us look at what is the random variable here what is a random variable here the team which won right so that is my random variable and it can take one of these four values now i know that team b won because i saw the tournament and i have seen that they won so now how can i write this as a distribution what is the distribution comprised of it comprised of these probabilities assigned to each of these events and there are four such events here so how do i write this distribution so what you are telling me is i could write it as zero one zero zero so essentially they are telling me that all the probability mass is focused on one of these outcomes because that is the certain outcome that is already happened no one can change it so that is the outcome for this tournament so i know that the probability of that even is event is one and everything else is zero so in other words the probability that the random variable x takes on the value b is one and everything else is zero so what i am trying to tell you is that even for a certain event you could still write it in terms of a distribution where all the mass is focused on that event now again i will bring the same setup that i did not watch the tournament after the semifinals so now youask megivemeyourpredictionwhatwhichteamwouldwin or thisisthe prediction which i made before just after the semifinals or just before the semifinals that i think one of these teams is going to win the tournament and the chance of each of them winning is something like this so i know the teams i follow this sport and i probably know that ok b has a very strong team and they have a very good record in the past few months and so on so maybe they have a higher probability of winning so these are the numbers which i assign now again i have made an estimate was my estimate perfect when would it have been perfect if i had predicted with certainty won that b is going to win but i was not willing to bet everything on b so i said there is a very high chance it will win but there is still a chance that there could be some surprises now how wrong was i now again tell me can you tell me what is p and what is q here this is the true distribution and this is my predicted distribution and what am i interested in again the difference between them how wrong did i go and what again what is a simple way of doing this again square errors so again this is what my formula would look like so this is fine in this toy case but why do we care about in real life examples that we are going to deal with in machine learning so in watching learning will deal with a lot ofproblemswhichareclassificationproblemsandinclassificationproblemsyou would again have this setup where you have a label the good thing of the label as a random variable and it could take off one of many values so i will again assume that it could take suppose you are trying to take a picture of fruits and you are trying to classify them and i could again think that i have four fruits say apple banana cherry and dragon fruit ok and this random variable can take one of these four values depending on the image that i am seeing ok now i have been given some training data so for every training data i have been given an image and i have been given the correct label so for that training data what is this distribution suppose i have been given the image of a banana what does this distribution look like zero one zero zero right again i have seen it so i know it is certainly a banana so i do not have any confusion all the probability mass is focused on that now the same image we are going to show to one of our models ok and it is going to make a prediction and will again ask it to give us a distribution the model will give us values perhaps like this ok so this is the models prediction again the model has given us a distribution and we have a true distribution and we are interested in knowing how wrong the model was so that student correct the refer time ninetwentysix we can correct the parameters of the model so this is our dash function loss function right so a loss function is some notion of difference between p and q right and so far we have been dealing with a very simplistic notion of this difference which is just the squared error loss ok and we want to do something better than this right so what i have told you is that you could always have a true distribution always have a predicted distribution and you would be interested in finding the difference between them that is the one first part the second part is that even when you are given something with certainty you could still write it as a distribution such that all the mass is focused on that event which was which has happened right which was the label was banana in this case and then you could still predict this from your model and now you are interested in knowing how wrong you are model wind because that is the loss function that you will use and then you will try to update your parameters with respect to this loss function means that is the setup that we are interested in so that is so i made a case for why we need to find differences between two distributions how to do it in a more principled manner we have not seen that yet we will get to that ok so before i get that i also need to tell you something about expectations so let us written to as sports example where there were four teams and say based on pundits and that sport they have said that these are the probabilities of winning and now you are into betting and you bet place your bets on these teams and you place our bets in a way that suppose team events then you end up winning ten k rupees if team b wins then probably will end up winning five k rupees and if team c wins probably ten k and if one of the other team wins maybe will end up losing money or something like that now you want to know what is your expected reward so now let us see what is happening here this was a random variable which could take on one of these four values these are the probabilities of the random variable taking that value taking on the value a or taking on the value b c and d ok this is your value or the gain or the profit associated with the random variable taking one of these values so you have a random variable you have a probability associated with every value of the random variable and you have some gain or value associated with every value of the random variable now how do you calculate the expected gain or expected profit which is this there is a thirty percent chance that you will earn ten k there is a forty percent chance that you will earn five k there is a twenty percent chance that you will earn ten k and ten percent chance that you lose thirty k so the way you will compute it is that and this is the simple expectation formula which is the probability of now the event here belongs to abcd right this is one of the four teams that will win probability of that event happening in to the value associated with that event happen this is a fair computation you get the intuition that this is how you will compute the net reward that you have so this is how you compute the expected value with respect to a particular distribution so this is the background that we need now i will just go on to the next slide and now we will talk about entropy first perhaps information content be first then entropy and then cross sector ok so now what is information content so now again let us take the same case that we have a random variable which can take on values a b c d now let us what we are trying to say is that if i know a certain thing what is the information that i have gained so you and i are talking you tellme somethingandiwanttosee whethermyinformationwasenhancedwhethermy knowledge was enhanced that is how we will quantify information content so if you are talking to me and you tell me that my name is mitesh the zero information content for me right because i already know that there is no surprise in that ok but if you talk to me and you tell me that today there is going to be a lunar eclipse then there is a possibility there is some information content gain for me right because that is not a event which happens every day if you just tell me you will see the moon today and you live in a region where it is not typically cloudy and there is no information gain there right so what do you see here when is the information gained high when the event which happens is a very surprising event and how do you say supplies in terms of probability student refer time fourteentwentynine it is a very low probability event right so if there is again this tournament and say d was the weakest team in the tournament and a was the strongest team in the tournament ifyoucomeandtellmethatdwontheniwouldbereallysurprisedthatsome information which i had gained but if you tell me that a won then probably i already knewitatthebackofmymindbecausea isclearlythestrongestteaminthe tournament and there is no information gained for me so one thing that we are trying to establish that the information content i see ok the information content of an event is inversely proportional to the probability of the event there is a that is a fair intuition fine now i want am still talking in terms of vague things i am saying it is inversely proportional but i still need an exact function so that i can compute it so i want something i want a function where i plug in the probability of an event and i get the information content of the event right now i do not have that function i am just building some intuition towards that function ok but this is one requirementthat i want the function to satisfy this is something that all of us agree with ok now think of two events which are independent a and b ok so a is the event whether the ac is on here or not and b is a event which tells maybe so let us consider two different random variables so x is the random variable which can take on values zero and one sorry so x is again this random variable which can take on these four values abcd whether who won in the tournament and y is this another random variable which can take on the value on and off depending on whether the a cs on in this room or not what can you tell about these two random variables they are independent random variables so this is on or off and this is which team won the tournament now i come and tell you something about the random variable y and i come and tell you something about the random variable x ok so now i want you to tell me this what is the information content of x and y i tell you something about x and i tell you something about y and these two events are in these two random variables are independent then what can you tell me about the information content what is the condition that you would want you gain someinformationbyknowingthingsaboutxandyougainsomeinformationby knowing things about y so what can you tell me it should be the sum right because these two are independent events so whatever information i am getting from this random variable and this random variable which together enhancing my information right it is not cancelling out anything or is there is no common intersection there right if the two events were not independent then i would not expect this to hold because knowing something about the first event only tells me something about the second event right because they are dependent so then that case the information gained would not be additive ok so now let us see i already made a case that this function which tells me the information gain is actually proportional to the probability ok so that means this is what the input is going to be right and then what is the other condition that i want this is a fair thing right i just replaced information content by a function and i know that the function should depend on the probability because that is what we have established here so we know that the function depends on the probability we still do not know what this f is exactly but i am trying to impose some conditions on f one condition of f f is that this condition should hold ok now let us look at this condition which i have underlined this is f of is this fine because it two events are independent you can write them as the joint probability as p of x into p of y this is clear to everyone right you seem to be a bit lost arvind clear ok now what is happening here i have a function f of a into b and that is actually equal to f of a plus f of b what family of functions do you know which has this characteristic log right that is why log is a good choice for this that is why information content is going to be the log of the probability but i wanted to be inversely proportional right so it will be log of one by the probability ok so that is why information content of this thing is so you see this how did we arrive at this log formula and this log can just be to any base it does not matter so all of you get how we arrive at the formula for information contained now just give me a minute i need to think of what is the next thing that i have to say ok and so we have found out the information content of one of these events happening which was the x taking on the value a now let us think of this random variable x so here actually i should have said x equal to a probability of x equal to a ok it makes sense because the random variable is x and the event is x taking on the value a how much information content is in that so if i know that x was a how much will i be surprised by it ok now let us take this event this random variable x which can take on values a b c and d as i said with each value there is a probability associated with it such that this sums to one now i did not need to draw this diagram ok i should refer time twentyoneone so x is a random variable which can take these four values which each of these values i have a probability associated ok so these are the values these are the probability values now what do i also have i have the information content associated with each of these right and the information content actually tells me the surprise of that evening now if i ask you what is the entropy of this random variable x so remember i had this case where i was betting i am with every poor outcome i had a value associated with it i had the same situation here with every outcome have a probability and i also have a value associated with this and the value is the information content now if i ask you what is the entropy or the expected information content of this random variable then how will you compute that i am asking you for an expectation so i will compute summation i belonging to a b c d p of x equal to i information can take what is the formula for that student minus refer time twentytwotwentynine minus i will just take the minus outside ok so this quantity is called the entropy of the random variable right it is the expected information content in the random variable nowifyouseewhatwouldbetheexpectentropyofarandomvariableifitis corresponding to a certain event that means say the sun rises always in the east right so what is going to be the entropy of that zero why you will have one of the sums in that summation as one log one right and every other sum would be zero into log of something so zero into anything is going to be zero even though that quantity is not defined zero into anything is going to be zero so the total entropy is going to be zero ok so this is entropy now what is it that we are actually interested in cross entropy so we have not gone there yet ok so we need to perhaps add one more slide so far everything is clear ok so now we are interested in something known as cross entropy so there again the situation is that there is something which is the true distribution and something which is the predicted distribution now actually before going there so let me just erase this off how many of you have thought that entropy is related to the number of bits that you need to transmit something do you know why that connection exists no now again let us think of this that you are trying to transmit a message and that message is again a random variable which can take on four values a b c d so think of these as four commands that we are trying to send to someone right and then based on that command someone will take some action now in the digital case how will you transmit this encode it to bits so what is the encoding that you will use zero zero yeah we will come to that zero one one zero one one so how many bits are you actually using for every message two bits ok for every bit so maybe this is a this is b this is c and this is d so for every message you are using two bits let us see actually what when you are doing this what are you actually assuming so actually assuming that all of these are equally likely if all of these are equally likely can you tell me the information content of any of them it is going to be minus log of one by four ok that is actually equal to minus log and this is to the base two ok one by four is two raise to minus two that is equal to two so the information content is actually equal to the number of bits that you are going to use to transmit that message now let us see if this is just in this special case or in a different case also suppose this could take eight values how many bits would you use three bits right so you will have zero zero zero zero zero one ok and this would be a to h now what are we actually assuming here each of these is equally likely what is the probability one by eight what is the information content two raise to minus three that is equal to three so the number of bits that you actually use to transmit something this can you can talk of it in terms of the information content of that now suppose i want to transmit this over the long distance so i need to bit be a bit efficient in terms of number of bits that i use right so now in this one of these cases suppose it is of the following form right that let us look at the case where x can take one of four values and say let me just put the right values so i will say one by two one by four one by eight one by eight ok now what is the information content of each of these one two three three and this is the message that i am going to send so what am i doing here i am using a different number of bits depending on the probability of that event why does this make sense why is this a smart thing to do if you want to transmit something which you are going to transmit a lot of times you better use less number of bits for that and this is exactly what is happening here a was having the highest probability and you are using the lowest number of bits for that now what is the expected number of bits that i will use up if it is a i will use one if it is b i will use two if it is c three and d three so what is the expected number of bits that i will use again i have the same situation right i want you to cast it into the same situation i have the probability values and with each of these guys i have a cost or a value associated what is the cost one bit two bit three bit three bit so what is the expectation now can someone compute the expectation oneseventyfive actually let me just write it down it would be again i belonging to a b c d p of x equal to i into the number of bits that you will use so that is just equal to log of log to the base two of p of x equal to i minus one what does this quantity actually this is the entropy we just saw this a this is the entropy of the random variable and what is it telling us actually that the entropy is oneseventyfive so what is the meaning of this actually so on average you will be needing oneseventyfive bits whereas if you are assuming everything is equally likely on average you are using two bits right so you see that on average you are making some savings here right so that is what the entropy tells you if you know what the probability of these events is then you better use that to decide the number of bits that you are going to use to send each of these sonowletuscomplicatethisabitmorenowwehavetheentropy nowletus complicate it a bit more so there is some true distribution which exists there is some true distribution from which these messages are coming right but you do not know what that true distributions we never know the true distribution that is the entire problem that we have been dealing with in machine learning so what you will do is we will somehow try to predict this distribution and this then the and the recipe that you will use is the same as that i used for the example where i had an urn right so there are these thousands of tenzeros of messages which has going to keep comingon you do not have accessto the entirestreambut you have seen some thousand of those messages just as i had peeped into the urn and i had seen some balls and i had made an estimate that i think based on these messages that i have seen so far i think these are the actual probabilities so the true probabilities are say p one p two p three and p four corresponding to a b c and d i do not know what this two properties are but i can estimate them looking at some samples or basically using my domain knowledge right maybe i would know that if one of these messages is stopped and i am actually trying to talk to a computer or a computer program that maybe stop is something which are used very rarely only at the end of the program or something so you have some either some domain knowledge or based on some samples i can estimate the value of this probability and i just try to relate it to the exact example of urns where you had these tenzeros of balls but you could not see all of them you sampled some and estimated a probability here again there is a continuous flow of message you cannot have access to all of these because they are going to continue but i have seen some of those and based on that you estimate these probabilities now based on this estimation how many bits so now this is the estimation that we have now based on this you will decide the number of bits that you will use for each of these messages right because you have some estimate so you want to be smart you do not want to keep two bits for all of them so you will just say that i will use log qi bits for the ith message this is fair thing because i know that the information content is proportional to the probability in fact it is exactly given by this formula minus log of qi so based on my estimated probability i am going to do this ok and this is the number of bits that i have reserved now do you see a problem with this this is my estimation but the data is actually going to come from the true distribution it is not going to follow the distribution q it is going to follow the distribution p so now what is actually happened is this right this is the situation that we are dealing with we have p which was a true distribution that is the rate at which the data will come but with each of these events the value that we have associated is now related to q because q is what i have access to i do not have access to p i just have access to q so i have associated a value based on q does this make sense i should have actually used log p one bits log p two bits and log p three bits but i do not know what p one p two p three are i just estimated them based on some samples so that is q one q two q three and these are the number of bits that i am using now if i have to compute the expectation how will i do it i have to use p because that is the true distribution from which the data is coming sowhatwouldtheexpectationnowlooklikeeveryonegetsthistheactual probabilities are this but because i am poor at estimating them i ended up associating these values which could be wrong right because i would have overestimated the probability of one of these messages and hence i have reserved lesser bits for that or underestimated the probability of one of these events and hence reserved more bits for that or vice versa i could have assigned a wrong number of bits to them right p no so do we have access to p in the sense so someone knows that right i mean there is a again in the same case as in the label case right we have access to the true p there and we are estimating a q when we are given these images for the training data we know that the distribution is zero one zero zero if the image is b for banana student refer time thirtythreefortynine then it is validated right so now this is what is this quantity called this is called the cross entropy ok you get why it is called the cross entropy because now you have two different distributions involved here ok you have the q distribution based on which you made your decisions you assign values to these events based on the q distribution but the true distribution is the p distribution so the actual number of bits that you use up on average is going to be based on the true probability they try to understand that now what will happen is for event a you have assigned a certain number of bits now how many bits will get used up it depends on the actual probability of p if that message is repeated many times then that is how this summation would be computed so thisiscalledthe crossentropy butnowwhy isthisthe differencebetweentwo distributions that is what we wanted given two distributions we wanted to be able to find the difference between them now am telling you that cross entropy is a way of finding that difference why is it so so what would you want this difference to what is the property that you would want this difference to have if p is equal to q then if p is equal to q then student refer time thirtyfiveeight not zero maybe it should take the lowest possible value right so this function right this is actually telling you loss of p comma q right this is what this is and we are calling it as the cross entropy this function you take it is minimum value when p is equal to q right because now at that point you are not really making any loss that is the best you could have done does this function take it is minimum value when p is equal to q yes why how is that obvious but why there could be something else which is lower than the actual entropy right why how you have to we are trying to minimize something so you have to give me answer so yeah so let us do that ok so how many of you it is obvious that q is the answer i mean the answer is p is equal to q it is not ok now this is the part which i am a bit worried about but i will just do it anyways so let me see how do i put this ok so remember that we had a p and we had a q and we want to find a q such that this quantity is minimized that is what our objective is so we want to minimize this with respect to qi ok now how do you find the minimize suppose i have this problem how do i find the minimum value how do i find the value of x which minimizes this take the derivative and set it to zero ok and then in this case i will get x equal to zero is that value can i do the same thing here and suppose it was this so now this is a function of two variables again i could do the same thing i could take the partial derivatives and set them to zero and i will get the minimum value now here this is actually a function of how many variables k in general right so q one q two q three up to qk ok now can you try doing the same thing can you can you take the derivative and set it to zero this is again a sum right it is very similar to this situation it is actually let me just write it down it is p one log q one plus p two log q two up to p k log qk now i want to take the derivative with respect to one of these guys say q two what would it be p two by q two is equal to what will i do that is the derivative p two by q two i will set it to zero do i get anything what is it that i am doing wrong here there is something that i am deliberately doing wrong is this an unconstrained optimization problem there is a constraint on the variables what is the constraint so why my true optimization problem is minimize with respect to q is such that summation q is equal to one do you know an easy way of dealing with these problems how many of you know the lagrange multiplier how will i use it here what will my objective function become then summation of qi minus one lambda then minus ok how many of you understand the intuition behind this that is a good answer now let us let me try to explain why this makes sense right this is the constraint that we have to operate within this constraint what i have done is i have taken the so now if the constraint is not satisfied what will happen to this quantity if the constraint is not satisfied that means my summation is not equal to one that is what means whether the constraint is not satisfied what will happen to this quantity it will be a nonzero quantity right fine then what will happen to my overall objective and i think we have made a mistake this should be plus i should add it right should be plus no it does not oh the lambda can be ok sorry so let me assume this is plus so what i am trying to do is that this is my objective function which i am trying to minimize i have added another quantity to it if this quantity is not equal to zero then i will not be the absolute minimum i will be at the minimum plus something right but if this quantity if the constraint is satisfied then this quantity will go to zero then am actually at the minimum of the function do you get this right so this is the function that i want to minimize i have added some quantity to it now that quantity is actually related to the constraint that i do not want to violate if i violate the constraint this is going to be non negative right so whatever minimum value i achieved i will be slightly higher than that because some nonnegative value has got added to it ok is that fine but if the constraint is satisfied then i can achieve the minimum value so that is roughly the intuition behind using this lagrangian multiplier it is a very crude intuition but there is of course a lot of math behind that but i am just giving you the intuition behind this which one student refer time fortyonethirtynine yeah that is what you could adjust the lambda and ensure that it is not negatively ok so now now can you do the same thing can you equate this to zero can you take the derivative and equate to zero what will you get now this term will give you p i by q two as before oh sorry p two by q two as before plus lambda times yeah plus lambda times one ok fine so equal to zero so then what will you end up getting p two is equal to i think it is something wrong here now this should be minus p two by k so p two is equal to lambda times q two ok and then further actually you can show that lambda is going to be equal to one how can you show that your constant is fine so do you see how we will get lambda equal to one so what does it actually tell you then p two is equal to q two that means all in fact you can show that all p is are equal to q is that means the distribution p is equal to distribution q so this cross entropy term will be minimized when your true distribution or when your plated distributionisthesameasthetruedistributionandhenceitcapturesthe difference between the two ok and that is exactly what we were interested in we were interested in a quantity which can allow us to capture the difference between the true difference between a true distribution and the predicted distribution so we have arrived with that quantity and that quantity is cross entropy so therefore for all our classification problems where we have this scenario that we are given the true distribution where all the masses focused on one of the labels and you are estimating a distribution where you could give non zero quantities to many of those and you want to find out how wrong your estimates were with respect to the true distribution you can use cross entropy as a measure for that right so now your loss function which you wanted to depend on the difference between p and q it can just be the cross entropy between p and q"}
{"audio_filepath": "Data/Preprocessed/Sequence Learning Problems_103.wav", "duration": 479.0, "text": "in thi s lecture we will ta lk about sequence learning proble ms a nd in particular some neura l netwo rk architecture s whic h dea l with sequence s so recurre nt neura l network s is what we are going to seeso we will star t with the fir st module whic h is on sequence learning problem so what are sequence learning problem so so far we have dealt with two types of networks one is feedforward neural networks and the other is convolution neural networks and both t hese networks the input was always of a fixed size so what do i mean by that is if you take a convolution neural network you are feeding thirtytwo   thirtytwo images to it or two hundred and twentyseven   two hundred and twentyseven images to it and this size wil l always fixed all your training images all your test images were always scaled or cropped to this particular size ok similarly when we used feedforward neural networks so one example was wordtwovec the size of the input was always fixed we had this in put of size two v right or k v in general if you are looking at the k word window right so this input was not varying from one training instance to another training instance or one training instance to the test instance or anything and secondly each input to the network was independent to the previous or future inputs so i pass an image of an apple i get the prediction apple then i pass some other image to the network and i get a different prediction it does not matter whether my previous image wa s a apple or a car or a mango or whatever it just reads each of these inputs independently there is no dependence between the inputs and the size of the inputs is fixed but in many applications the input is not of a fixed siz e so and also successive inputs may not be independent of each other so let us understand this with the example of auto completion that all of us are used to while typing smss or whatsapp or other things so given the first character d i want to pre dict the next character which is e then once i have predicted e i want to predict the next character again and so on till i get the full word ok this is what my task is so let us notice a few things first successive input s are no longer independent if i know that the previous input was d and the correct input is e then i know that only a few things are possible right in particular if you know that the previous input was a z and the correct input is a e then most likely the next is going to be a b right but if you ignore the previous input which is z then after e there are many things which can appear right so the inputs are no longer independent of each other and the second thing is the length of the input is no t fixed because words could be of arbitrary sizes i am trying to type the word deep that is four letters or if i am trying to type the learn which is five letters machine which is seven letters and so on right so the input size is no longer fixed and the inputs a re now dependent on each other right there is some dependence between so now this is very different from what we saw in convolutional neural networks and feedforward neural networks so how do we deal with this and the third thing here is that eac h network now i am calling this as a network and i will just clarify some notations also soon each network is actually performing the same task it is taking as input a character and it is producing as output one character and now just remember that thes e networks i have drawn them vertically you are used to seeing them as this so this is input this is your hidden layer and this is your output so this is the green part this is the blue part and this is the orange input and this is the fully conne cted layer right so each of these boxes is actually this network ok i have just drawn it more concisely because i need to draw many such networks so everyone gets that just remember this mind that each of these orange blue green structures is a fully connected network like this so these problems are known as sequence learning problems where you have a sequence of inputs and then you need to produce some outputs and each input actually corresponds to one time step so this is the input at time step one time step two time step three time step four and so on so let us at some more examples of such sequence learning problems so one classic example is the task of predicting the part of speech tag of every word in a sentence right so i am given a sentence man is a social animal and for every word i want to predict whether it is a noun or an adverb or an adjective or a verb or any other part of speech type right and this is how it happens now notice that once we see an adjective in this case social we are almost sure that the next what is going to be a noun or at least we are sure that the next word cannot be an article or most likely it will not be a verb right there is a very high prior th at the next word is going to be a noun so that is why these inputs are actually dependent on each other so the current output not only depends on the current input it is also actually depends on the previous input right unlike the case of convolutio nal neural networks where i feeded an apple it is no dependence on whether the previous input that i pass to the network was an apple or a car or what not and the size of the input is not fixed because these sentences could be of arbitrary lengths i could have sentences as small as three to four words or as long as twentyfive to thirty words right so average wikipedia sentence for example is twentyfive words roughly twentyfive words and notice that and this case we are interested in producing an output at every time step because for every input i want an output and each network again this orange blue green structure is performing the same task its taking as input a word and its producing an output what is producing as an output part of speech tag so  here the two examples that we saw we were having an input and every time step and an output at every time step but there could also be cases we are interested in producing the output only at sometime steps or at the final time step so let us consider task of predicting the polarity of a movie review right sentiment analysis so i am given a movie review and after i have read the entire review i should give a prediction right otherwise it would be incomplete i cannot actually look at only this word and give a prediction does not make sense i does not also make sense to make a prediction here at this point because there could have been the movie was boring but i still loved it or but the action was amazing or something like that it because it coul d have already flipped after that so i need to look at the entire sentence and make a prediction but you are not interested and prediction as these intermediate time steps even in this case you can actually assume that every network is performing the same task its taking a word as an input and it is producing some output it just that till the end you do not care about you outputs you care about the output only produced at the final step you do not care about what the outputs are at this time step rig ht that is one way of looking at it so again at every time step we have the same network but you are only interested in some time steps of the network ok finally it is not always necessary that sequences are composed of only words what other kinds of sequences are you familiar popular sequences speech is one video is another so a video could be treated as a sequence of images and now you could have a video where some someone is performing surya namaskar and as you can understand that i need to look at the entire sequence and only then be able to make a prediction right if i stop at this point if i only consider this this is only namaskar no surya namaskar right so you have to look at the entire sequence and then decide what the output is and you do not care about the intermediate outputs i do not care what is the prediction till this point this of course is again some aasan but i do not care about that i care about the full sequence that i am dealing with  it is just to motivate that sequences can be of all types and i apologize to the speech people i do not really understand much of speech processing so i never give speech examples but video is something i understand so i can give examples on that"}
{"audio_filepath": "Data/Preprocessed/Perceptron Learning Algorithm_14.wav", "duration": 770.0, "text": "we will now go to the next module which is the p erceptron learning algorithm we now see a more principled approach of learning these weights and thresholdbut before that we will just again revisit our movie example and make it slightly more complicated now herewhatthesituationisthatwe aregivenalistof m moviesand aclass associated with each movie indicating whether we like the movie or not so now we have given some data of the past m movies that we have seen and whether we like this movie or not and now instead of these three variables we have these n different variables based on which we are making decisions and notice that some of these variables are real t hey are not boolean anymore t he rating could be any real number between zero to one ok and now based on this data what do we want is the perceptron to do actually so i have given you some data t hese factors i have also given you the label one and zero so if the perceptron if i tell you my perceptron has now learnt properly what would you expected it to do perfect match so whenever i feed it one of these movies it should give me the same label as was there in my data and again there are some movies for which i have a label one which are positive and s ome movies which i have a label zero so i am once again looking to separate the positives from the negatives so it should adjust the weights in such a way that i should be able to separate so that is the learning problem that we are interested in sonowwiththatiwillgiveyouthealgorithm t hisistheperceptronlearning algorithm w e have certain positive inputs which had the label one we have certain negative inputs which had the label zero and now i dont know what the weights are and i have no prior knowledge of what the weights are going to be i need to learn them from the data so what i am going to do is i am just going to initialize these weights randomly as i am also going to pick up some random values for this so this should be small n so this should be small n and now here is the algorithm while not convergence do something so before i tell you what to do can you tell me what is meant by convergence when will you say that it has converged when it is not making any more errors on the training data right or itspredictionsare not changingon the training data so thatisthe definition of convergence now here is the algorithm i pick up a random from point from my data which could either be positive or negative so it comes from the union of positive negative basically all the data that i have i pick up a random point from there if the point is positive right and this is the condition which happens what does this tell me i f the point was positive what did i actually want greater than zero but the condition is less than zero that means i have made an error so i have made an error then i will just add x to w i see a lot of thoughtful nodding and i hope you are understanding what is happening let u s see so what is w actually a dimensional n dimension n plus one right because w naught is also inside there so actually there should be w naught also here right and what is x again n dimensional right and that is why this addition is valid so let us understand that w and x both are n dimensional n ow let us look at the other if condition c an you guess what the other if condition is if x belongs to n and summationisgreaterthanequaltozerothen sothatmeans youhavecompletely understood how this algorithm works w ell that is so now consider two vectors w and x so remember what we are trying to prove is or get an intuition not prove actually get an intuition for why this works ok so we will consider two vectors w and x and this is what my vectors look like very similar to the case that we are considering wzero to wn and one to n so this again x naught is just one now this condition that i have been talking about is nothing but the dot product h ow many of you have gone through the prerequisites for todays lecture o k good so it is just a dot product now we can just read write the perceptron rule as this i nstead of the dot product i mean instead of using that summation thing we can just say that it is a dot product now we are interested in finding the line w transpose x equal to zero so that is our decision boundary which divides the input into two halves now every point on this line satisfies the equation w transpose x equal to zero what does that mean actually so just a simple example is that if i have the line xone plus xtwo equal to zero then all the points which lie on the line satisfy this equation so you could have one minus one two minus two and so on but two two is cannot be a point on this line a t every point lying on this line satisfies this equation so every point lying on this line actually satisfies the equation w transpose x equal to zero so can you tell me what is the angle between w and any point on this line h ow many say how many of you say perpendicular w hy dot product is zero so if the dot product is zero they are orthogonal so that means if i take this line then my vector w is orthogonal to this it i s orthogonal to this point or this point to this point to every point on the line which is just the same as saying that the vector is perpendicular to the line itself right as simple as that  so the angle is ninety degrees because the dot product gives you the cos alpha and that is zero right and since it is perpendicular as i said to every point of the line it is just perpendicular to the line itself so this is what the geometric interpretation looks like this is our decision boundary w transpose x and the vector w is actually orthogonal to this line and that is exactly the intuition that we have built so far now let us consider some points which are supposed to lie in the positive half space of this line that means these are the points for which the output is actually one now can you tell me what is the angle between any of these points and w or you guys are actually trying to tell me the angle we have got some measuring stuff no so i will give you three options i e equal to ninety greater than ninety and less than ninety less than ninety it is obvious from the figure now if i take any point which lies in the negative half space what is the angle going to be between them it is greater than ninety again obvious and it also follows from the fact that cos alpha is w transpose x by something and we know that for the positive points w transpose x is greater than equal to zero that means cos alpha would be greater than equal to zero that means the angle alpha would be less than ninety degrees and for the negative points w transpose x is actually less than zero that means cos alpha would be less than zero that means alpha would be greater than ninety degrees so it actually follows from the formula itself but it is also clear from the figure  so keeping this picture in mind let us revisit the algorithm so this is the algorithm now let us look at the first condition which was this now if x belongs to p and w transpose x is less than zero then means that the angle between x and the current w is actually greater than ninety degrees but what do we want it to be less than ninety degrees and our solution to do this is but we still do not know why this works now anyone knows why this works so let us see why this works so what is the new cos alpha going to be it is going to be proportional to this it is going to be proportional to this i will just substitute what w new is fine t hat means if cos alpha new is going to be greater than cos alpha what is alpha new going to be it will be less than and that is exactly what we wanted this angle was actually greater than ninety degrees so you want to slowly move it such that it becomes less than ninety degrees it is not going to get solved in one iteration and that is why till convergence so we will keep doing this i will keep picking xs again and again tillit reaches convergence that means till we are satisfied with that condition let us look at the other condition x belongs to n and w transpose x was greater than equal to zero then it means that the angle alpha is actually less than ninety degrees and we want it to be the opposite i will just quickly skim over this w minus this x ok i forgot to mention that this is actually a positive quantity i mean that is why that result holds that means cosalphanewisgoingtobelessthancosalphaandthisslightbitof mathematical in correctness i am doing here but that does not affect the final result so i will just gloss over that and you can go home and figure it out but still it does not take away from the final intuition and interpretation so now the new cos alpha is going to be less than the original cos alpha that means the angle is going to be greater and that exactly what we wanted so we will now see this algorithm in action for a toy data set so this is the toy data set we have and we have initialized w to a random value and that turns out to be this i just picked up some random value for w and ended up with this particular configuration for w now we observe that currently w transpose x is less than zero for all the positive points and it is actually greater than equal to zero for all the negative points if you do not understand w transpose x it is just that the all the positive angle points actually have a greater than ninety degree angle and all the negative points actually have a less than ninety degree angle  so this is exactly opposite of the situation that we want and now from here on we want to actually run the perceptron algorithm right and try to fix this w how does it work remember we randomly pick a point so say we pick the point pone do we need to apply a correction yes why because it is a positive point and the condition is violated so now we add w equal to w plus x and we get this new w  so notice that we have a new w w e again repeat this we again pick a new point and this time we have picked ptwo d o we need a correction yes at least from the figure it looks like the angle is greater than ninety so we will again do a correction we will add w is equal to w plus p this x is actually sorry ptwo and this is where we end up now again we pick a point randomly none do we need a correction so this is what our w is this line here and none so we need a correction n ow what is the correction going to be it will be minus and then the w changes now we pick another point nthree d o we need a correction no at least on the figure it seems like the angle is greater than ninety and we continue this for ntwo we do not need a correction now for pthree again we do not need a correction the angle looks less than ninety sorry actually it is we need a correction the angle is slightly greater than ninety and this is our correction and now we keep cycling now as i keep cycling over the points i realize that i no longer need any correction it should be obvious from the figure that for this particular value of w now all my positive points are making an angle less than ninety and all my negative points are actually making an angle greater than ninety t hat means by definition now my algorithm has converged so i can just stop it  so i can just make one pass over the data if nothing changes i will just say it has converged n ow does anyone see a problem with this it will never converge in some cases  so can someone tell me why w e are considering only cases where the data is linearly separable th at we already assumed so what you are trying to tell me is that you are going over these points cyclically  so let me just rephrase and put words in your mouth that what you are trying to tell me actually is that i take a point i adjust w but now for the next point i maybe go back to the same w because that point asked me to move it again and i keep doing this again and again and basically end up nowhere that is why this will never converge that is exactly what you are trying to tell me now that is exactly what i am forcing you to tell me so that is not the case this algorithm will converge"}
{"audio_filepath": "Data/Preprocessed/Biological Neuron_1.wav", "duration": 373.0, "text": "hello everyone welcome to lecture one of csseven fifteen which is the course on deep learning intodays lecture is going to be a bit non technical we are not going to cover any technical conceptsorwe only going to talk about a brief or partial history of deep learning so we hear the terms artificial neural networks artificial neurons quite often these days and i just wanted you take you through the journey of where does all these originate from and this history contains several spans across several fields not just computer science we will start with biology then talk about something in physics then eventually come to computer science and so on so with that let us start so just some acknowledgments and disclaimers i have taken lot of this material from the first people which i have mentioned on the bullet and there might still be some errors because its dates as back as one thousand eight hundred and seventyone so maybe i have got some of the facts wrong so feel free to contact me if you think some of these portions need to be corrected and it would be good if you could provide me appropriate references for these corrections so let us start with the first chapter which is on biological neurons as i said its spans several fields will start with biology and we will first talk about the brain and neurons within the brain so way back in one thousand eight hundred and seventyone one thousand eight hundred and seventythree around that time joseph von gerlach actually proposed that the nervous system our nervous system is a single continuous network as opposed to a network of many discrete cells so his idea was that this is one gigantic cell sitting in our nervous system and it is not a network of discrete cells and this theory was known as the reticular theory and around the same time there was the some breakthrough or some progress in staining techniques where camillo golgi discovered that a chemical reaction that would allow you to examine the neurons or the nervous tissue so he was looking at this nervous tissue using some staining technique and by looking at what you see in this figure on the right hand side the yellow figure even he concluded that this is just once single cell and not a network of discrete cells so he was again a proponent of reticular theory so this is about camillo golgi and then interestingly santiago cajal he used the same technique which golgi proposed and he studied the same tissue and he came up with the conclusion that this is not a single cell this is actually a collection of various discrete cells which together forms a network so it is a network of things as opposed to a single cell there so that is what his theory was and this was eventually came to be known as the neuron doctrine although this was not a consolidated in the form of a doctrine by cajal that was done by this gentleman so he coined the term neuron so now today when you think about art hereaboutartificialneuralnetworksorartificialneuronsthetermneuronactually originated way back in one thousand eight hundred and ninetyone and this gentleman was responsible for coining that and he was also responsible for consolidating the neuron doctrine so already as you saw on the previous slide cajal had proposed it but then over the years many people boughtthisideaandthisguy wasresponsiblefor consolidatingthatintoa neuron doctrine interestingly he is not only responsible for coining the term neuron he is also responsible for coining the term chromosome so two very important terms were coined by this one person so now here is a question so around one thousand nine hundred and six when it was time to give the nobel prize in medicine what do you think which of these two proponents say there are two theories one is reticular theory which is a single cell and then there is this neuron doctrine which is a collection of cells or collection of neurons that a nervous system is a collection of neurons so what do you think which of these two guys who are proponents of these two different theories who would have got the actual nobel prize for medicine so interestingly it was given to both of them so till one thousand nine hundred and six in fact way later till one thousand nine hundred and fifty also this debate was not completely set settled and then the committee said both of these are interesting pieces of work we yet do not know what really actual what the situation is actually but these conflicting ideas have a place together and so the nobel prize was actually given to both of them and this led to a history of a some kind of controversies between these two scientists and so on and this debate surprisingly was settled way later in one thousand nine hundred and fifty and not by progress in biology as such but by progress in a different field so this was with the advent of electron microscopy so now it was able to see this at a much better scale and by looking at this under a microscope it was found that actually there is a gap between these neurons and hence it is not a one single cell it is actually a collection or a network of cells with a clear gap between them or some connections between them which are now known as synapses so this was when the debate was settled so now why am i talking about biology why am i telling you about biological neuron and so on so this is what we need to understand so there has always been interested in understanding how the human brain works from a biological perspective at least and around this time the debate was more or less settled that we have this our brain is a collection of many neurons and they interact with each other to help us do a lot of complex processing that we do on a daily basis right from getting up in the morning and deciding what do we want to do today taking decisions performing computations and various complex things that our brain is capable of doing now the interest is in seeing if we understand how the brain works can we make an artificial model for that so can we come up with something which can simulate how our brain works and what is that model and how do we make a computer do that or how do we make a machine do that so that is why i started from biological neurons to take the inspiration from biology"}
{"audio_filepath": "Data/Preprocessed/Proof of Convergence of Perceptron Learning Algorithm_15.wav", "duration": 893.0, "text": "in this module we will talk about the proof of convergence for the perceptron or the learning algorithm that we saw in the previous module so we have some faith and intuition that it actually workswe just need to formally prove it that it actually convergesso that is what we are going to do in this module so before that a very few very simple definitions so if you have two sets of points p and n in an n dimensional space and we call say that these points are absolutely linearly separable if there exists some n plus one real numbers which has wzero to wn such that every point which belongs to p right p is the case where the output is one then these set of weights satisfy this condition and every point which lies in the negative set the set of weights satisfy this condition  so nothing very different from what has we have been saying so far it is just formally defining it now our proposition is that if the set p and n are finite and there is a fixed number of points in that which was the case in the toy example that we were doing and which will be the case in most examples that we do and linearly separable the perceptron learning algorithm updates the weight vector ok before i go there ok let me not give you the definition and let me ask you the definition so now i have given this definition the first definition and given this part of the proposition can you tell me what do i need to prove if i need to prove that the algorithm converges that is one way of looking at it but what was happening in that wrong argument which was i was making that it continuously kept toggling that means i am not making a finite number of updates right i have to keep changing again and again and this process continues in a loop so that is how i am going to define convergence that the perceptron learning algorithm updates a weight vector of finite number of times it only needs to update it finite number of times and it will reach a configuration such that now it is able to separate the p from the n ok that is what the proof of convergence means so in other words if you are going to pick up these vectors randomly from the set p and n cyclically as we were doing in the toy example then a weight vector wt is found after a finite number of steps which will separate these two steps these two sets so that is what we are trying to prove so that is the definition of converge does it make sense so proof is on the next slide and it is going to take me around five to ten minutes to prove it so just stay focused all right so here is a few set up right so i am going to before i go to the actual proof i am going to make a set up so that it becomes easier for us to prove it so the first thing that i am going to say is that if there is a point which belongs in negative set then the negative of that point belongs in the positive set and that is very clear because if the point belongs in the negative set then w transpose x is less than zero but then w transpose minus x would be greater than equal to zero right so i take the negative of the point i can just put it in the positive set so instead of considering these two different things p and n i am just going to consider one p prime which is an union of p and all the n points negative ok will the set up clear if this is a setup then what is the condition that i need to ensure for every point in p dash student refer time threefiftyseven w transpose p should be greater than equal to zero right so i do not care about the negative case i have just made everything positive now and it is i am not done anything wrong here it is just a simple trick ok and now this is how the algorithm will look in this setup these are the inputs with label one inputs with label zero n minus contains a negation of all the points in n and p prime is a union of these now again i start by initializing w randomly while convergence i will do something i will pick a random p from p prime now what is the if condition less than zero do i need the other if condition no right because everything is now positive ok and the other small thing that i am going to do is i am going to normalize p ok so that again does not mean because we are talking in terms of angles and i am not changing the direction of the vector i am just shrinking it right so i am just or maybe scaling it also i am just making it unit norm so that does not change anything so it is still everything still holds and in particular you can see here so if this condition was true this condition will also be true ok so so far just i am done some simple tricks to make things easier for me lateron so now p has been normalized now rememberthat thisdata islinearly separable that is what we started the proposition if p and n are linearly separable then theperceptronlearningalgorithmwillconverge sonow ifp andnarelinearly separable irrespective of whether we have the perceptron learning algorithm or not what do weknow there exists a w star which is the solution vector right there exists at least one w star which is the solution vector right such that it will separate the p points from the n points so this vector which we do notknow but we justknow that it exists so you can refer to it so we will call this w star fine now we start the proof so w star is some optimal solution which weknow exists but we do notknow what it is right now suppose you had a time step t so remember that this algorithm is going on while convergence so you have time step one two three you are picking up points so we are at a time step t at which you pick up a random point pi and you find that the condition is actually violated so this should actually be less than zero if i know the condition is violated so now what will you have to do w is equal to wone so i will just call it the new w wt plus one is equal to the old w plus pi ok now what i am going to do is i am going to consider the angle beta between w star and wt plus one i do not know what w star is but we can still assume it exists and make some calculations based on that so what is the angle between w star and wt plus one its beta and what is the cost of that angle this and remember that we do not have w star here because we had assumed that it is the normalized vector so we do not need that but this is actually equal to one ok so now if i just take the numerator w star in dot product wt plus one now i am going to expand wt as wt plus pi fair that is exactly what i did on the previous step now now what is pi actually it is so what you had is you had these pone ptwo pthree my hand writing is really horrible and up to pn right  so i have just picked one of these pis ok now what i am going to define is now suppose this is my these are my pis so these are all the vectors that i have now suppose i have this w star suppose this was the w star that i am interested now for each of these i could compute w star pone w star ptwo and so on up to w star pn and i could sort them now what i am doing is that for whichever of these points w star pi is the minimum  i am going to call that value as delta suppose w star p one is the smallest quantity out of w star pone w star ptwo w star pn and i am calling that quantity delta so i have this quantity here and my delta is the minimum of all the possible values that it can take it can make w star pone ptwo up to pn so delta is the minimum quantity so here i have an equality now are you ok with this this is the minimum quantity right so any pi that i put in here it is always going to be greater than or worst case equal to delta now again this wtwo itself i could write it as wt minus one plus pj because that also would have come up from some update in the previous step ok again this is there which i could call it as delta and still retain the greater than equal to here ok fine so let us see where are we heading with this now notice that we do not make a correction at every time step when i was running that toy algorithm i was not making a correction at every time step we were only making a correction at those time steps for which the condition was violated so now if i am at tth time step maybe i have made only k which is less than or equal to t corrections at max i would have made t corrections but it could have been less than that also so now every time we make a correction we are adding a value delta to this so at the time step t what would happen i had started off from w naught i have reached time safety and i have made a case that i have not made t updates i have made k less than equal to t updates so how many deltas would get added k delta so i could say that with respect to w naught where i had started from this is what this quantity is ok is that fine anyone has a problem with this so far what are we shown we started with this this condition was true again not less than equal to and hence we made the correction and this was the point that we picked up at the t th step and thence we made that correction and we also showed that the numerator is actually greater than equal to this quantity we showed it by induction fine now let us look at the denominator and particularly let us look at the denominator squared ok is a step right this is actually wt plus one dot product wt plus one but wt plus one can be written as wt plus p i this bracket needs to disappear right is that ok fine now what is what is this quantity that is less than equal to zero so now can you guess what is the next thing that i am going to write that is correct yeah it is a negative quantity  so that is going to be less than equal to this so that is fine and what about pi square o r this term because this is less than right that is why correct is this fine ok now what is pi square one now can you guess what i am going to do by induction so what is wt square again just this wt plus one square was wt square plus one wt square is going to be wt minus one square plus one right and how many such ones will get added k of those right starting from w naught ok so what have we shown the numerator is greater than equal to this t he denominator is less than this ok now if i put them together i actually get that cos beta is going to be greater than equal to the numerator over the denominator ok now what is this quantity proportional to k k square k cube square root of k k by two student square root of k square root of k right you have i mean roughly speaking you have a k here you have a square root of k here so i could roughly speaking say that it is proportional to square root of k so as k grows what will happen to cos beta it will grow and that is fine right it can keep growing student refer time thirteenthirtyone only until one right so cos beta is going to be proportional to k what is k the number of updates that you make now if i were to take that degenerate case which you guys were hinting at where that it will keep changing again and again what will happen to k it will keep going to infinity can that happen no because cos beta will blow up right and that is not allowed so k has to be finite so that cos beta stays within its limits right hence are we done how many if you think we are done how many if you are satisfy that we are done so yeah so this says that we can only have a finite number of such k updates that we make and after that the algorithm will converge so we have a proof of convergence n ow coming back to our questions this is where we had started at one point what about non boolean inputs so perceptron allows that we took imdb rating and critics rating as an input do we always need to hand code the threshold no in our perceptron learning algorithm are all inputs equal no we now assign weights to input what about functions which are not linearly separable we still do notknow so that is where we are headed now not possible with a single perceptron but we will see how to handle this"}
{"audio_filepath": "Data/Preprocessed/Nesterov Accelerated Gradient Descent_36.wav", "duration": 674.0, "text": "l et us look at nesterov a ccelerated g radient d escent refer slide time zerosixteen so now we know that momentum based gradient descent is good at these gentle regions it moves really fast  but we do not still do not like it because it has this problem of oscillations i t has this problem that it overshoots its objectiv e it s goal and then it has to take a lot of u turns  so can we do something about reducing this oscillation so the answer is always yes so let us look at nesterov accelerated gradient descent refer slide time zerofortyone so the idea here is very sim ple look before you leap ok now remember that this was the update rule for momentum based gradient descent ok and i will write it down again wt plus one is equal to wt minus gamma into update t minus one minus eta into the gradient at the cur rent point so you see that actually i am taking two steps one is this step and then one more step and i could just this is one way of visualizing right that i move according to the history and then i move a bit more according to the current gradient  so everyone sees that there is a two step movement happening here now can you think what could have been done look before you leap  so we will see what we can do refer slide time onethirtyfour so we know that we are going to move at least by this one and that is fixed we know that our history is telling us to move at least by this one and then we will mo ve a bit more by the gradient so now can you think about it i am at least going to move this much what if i had some way of looking ahead and then do something at th at point this is what you are saying of course i can verify it  but i am sure it will become clear once i show the equations  but i just want you to think about it a bit wait it i s very simple it will become absolutely clear once i show you the answer but just think about it a bit so here is the answer it why not compute the gradients add this look ahead point right so you are again adding it in two steps minus the history and then minus the current gradient  so take this value call it the look ahead point i know that i am going to move by this much  so let me not compute the gradients at the current point let me move by this much then compute the gradients and see what happens at that point refer slide time twofiftytwo so this is the equat ion right that first i move by that one step i had to make a two step movement  so i will move by that one step right then i will compute the gradient at that position not at my current position right this was earlier gradient at point t now i have a lready moved a bit  so i can compute the gradient there and then move in the direction of that gradient so you understood this that there is a two step movement right wt minus history minus the current gradient gradient computed at time step t ok now you know that you are already going to move by the history right so why not just move there and then compute a gradient at that point y ou are anyways made some movement you compute the gradient at that point and then decide which is the direction to mov e in right so that is what this look ahead value is i know it i s still not clear to many of you and i am very confident it will become clear in the next five minutes we will show you one more visualization for this  but this stay with stay with me for a while as long as you get the intuition i am fine i will move ahead and then i will explai n it again in a different way this is fine ah that should become clear good that you asked that question ok  so ask me again on the blank slide that i have and the n i it should be complete so for right now let me just show you what will happen with the code and then i will again explain it with a different way refer slide time fourthirtyfive so this is what momentum based gradient descent it ok now let us see wha t nested or accelerated gradient descent will do again the code is simple you can just read it up and i have started executing you see this blue curve coming over there fine ok and now i keep running this now what will happen you see that all the u tu rns of the blue curve are inside the u turns of the red curve so the objective is being achieved at least empirically i have showed you that right its taking shorter u turns what is probably not clear to all of you is why is this happening is it clea r to everyone why is this happening can everyone visualize that ok so let us see why this is happening i will give you an a lternate explanation for this refer slide time fivethirtyeight so suppose this is my error surface right on a two by and i have a s ingle variable with respect to which i am trying to optimize  so this is my w i started off wit h some initial value w naught now what is the gradient at this point positive negative negative right because when i am going to increase w the function i s actually going to decrease right  so right  so the slope is negative  so where will i move this is the number line right  so this line is actually the number line because it i s a single variable  so where will i move positive side of the number li ne or the negative side of the number positive side the derivative is negative i am going to move in the direction opposite to the derivative  so i am going to move in the positive direction right  so i will end up somewhere here is that clear fine with everyone so now i am somewhere here what is the derivative at this point now what is the derivative here positive negative negative right w hen i am increasing w my loss function is decreasing  so my dl by dw is going to be negative this is pos itive this is negative  so again i will move in this direction so what is happening a lot of negative updates are getting sorry a lot of positive updates are getting accumulated and now because of my momentum i am not going to move only by this deriva tive i am also going to move by the history right  so i will end up somewhere further so now at this point what is the derivative again negative when i am increasing w the function is decreasing  so what is my update positive or negative positive  s o now you see that a lot of positive updates are getting accumulated right my momentum is building up so now what will happen now if i just move further then again i will get a let me just put it here right  so i am again moving largely in the pos itive direction because this guy is also positive all my history was also positive  so i have moved in the positive direction now what will happen at this point what is the derivative here no it i s still its negative sorry  so again i am going to mo ve in the positive side of the number line ok now at this point i want you break down the movement into two points one is what my history was telling me which was all these positive updates  but of course i will not make such a large update because i am waiting them exponentially right so  but its telling me to move in the positive direction ok and i know that the gradient at this point is negative but i want you to ignore that for now i just want you to focus on the history if i just move accordin g to the history where will i end up  i will end up somewhere here right because the history is very positive  so i will keep moving in the positive direction and this is my w look ahead now what will happen if i compute the gradient here s tudent pos itive t he gradient is s tudent p ositive p ositive  so where will i move s tudent negative n egative  so you see now why momentum works because you are able to look ahead to this point instead of what should i have actually done is i should have look ed at the gradient at this point the history is positive the gradient is also telling me to move positive  so i would have moved a large positive and i would have ended somewhere here instead i just moved by the history i checked where i end up i end up here now let me see whether what is the gradient at this point have i already overshoot my overshot my objective when would i overshot my objective it has the sign of the gradient changes right it became from negative to positive and now since its positive because as i am increasing w the loss is also increasing  so now where will i move negative so now what is the second step actually its again bringing me close to here  so instead of taking this large u turn i end up taking this small u turn is this clear to everyone now how many if you still do not get it how many if you get it now good sure ok  so this is what and now we can relate it to what was happening on the figure so let us go back right  so you saw that i was making these smaller u turns because when i was at this point right i already moved by the history i knew i would land up somewhere here where i would need to go back right  so i already accounted for that and made a very small movement is this clear everyone ge ts this how the nesterov of accelerated gradient descent works sure raise your hands refer slide time tenfortytwo so looking ahead helps nag in correcting its course quicker than momentum based gradient descent right so it i s already looking ahead whe re do i land up and already making a correction if required if not required it will again move in the right direction right  so the update is this guy plus the gradient and my update happens on the original value not on the look ahead value so her conf usion was perhaps that i am doing w look ahead minus update where this update again has this quantity you know that is what your confusion was  but i am not doing w look ahead i am using wt there everyone gets this  so that is where ah now it i s clear that why the oscillations are smaller in the case of nag and it i s able to correct ing its course quicker"}
{"audio_filepath": "Data/Preprocessed/Deep Learning(CS7015): Representation Power of Multilayer Network of Sigmoid Neurons_22.wav", "duration": 2090.0, "text": "before we move on to the next modulate so me small corrections fr om yesterdays class s o one was this partial derivative it should have been do u w square s o we already taken one derivative with respect to w and now you are taking another derivative it is the gradien t of the gradient a nd similarly should this should have been dou b square and th is should have been dou w dou b refer slide time zerofortythree the other small thing which i wanted to say was so when i was executing this algorithm right s o i forgot to m ention that just notice what is happening is the black dot that you see the black dots that you see right and which are very close to each other a ctually because you are just making small movements those are the changes in the w comma b values and the r ed dots are the corresponding loss to that w comma b values right just to clarify s o that is why you see a movement on the w b plane which is this movement and as you keep changing that your loss function changes and it becomes better and better right that means it goes closer to zero refer slide time onetwentyfive so in this module we are going to talk about the representation power of a multilayer network of sigmoid neurons right s o i am going to compare these two things which are written in the title s o first tell me what was the representation power of a multi layer network of perceptrons ok i roughly hear what you are saying and basically what you are telling me is that a multi layer of network of perceptrons with a single hidden layer can be use d to represent any boolean function precisely right n o errors that s what we saw with that illustrative proof where we actually constructed once it s network n ow what is the representation power of a multi layer network of sigmoid neurons s o multilaye r network of neurons with a single hidden layer can be used to approximate ok s o just see the difference in the language s o this was a represent  that means exactly this is approximate  that means i will tolerate some error any continuous function i nstead of boolean function to any desired precision s o this was not this was precisely with no errors this is up to any arbitrary desired precision s o what does this mean actually what is the meaning of this s o there is a guarantee that for any func tion ok which takes our original x from r n to r m what is the m that we have been considering in all our examples one right we just care about one output but it can be r m also w e can always find a neural network with one hidden layer containing enough neurons s o that is the operating trace here enough neurons whose output g of x so that means you would have a network it would take as input n x it would produce some y hat and that is what i am calling as g of x right that g of x would be very clo se to the true function f of x s o remember that we said that there is this true function f of x which gives us the true y s and we are trying to predict this y hat s o the true why i am calling by f of x and the y hat i am calling by g of x and you can come up with a neural network which can give you values which can predict values which are very close to your true values does that make sense d o you see the value of this theorem what is it trying to tell me tell me can you can you give me an interpr etation of this why is this so useful do you know what this theorem is called universal approximations here and we did that in the history right refer slide time threefiftyseven so this was one thousand nine hundred and eightynine ok w hat is the significance of this w hy do we care about s uch arbitrary functions and what does this theorem telling us actually i t is of course telling us something about the representation power of a multilayer network of sigmoid neurons but why is this important s o we will see that refer slide time four twentysix s o this the remainder of the lecture i have borrowed ideas from this url you should actually read this it is a very interesting book it is available online for free very illustrative s o please take a look at it ok refer slide time fourfortyone s o now actually what we are interested in is we are interested in knowing whether a network of neurons can be used to represent any arbitrary function like the one shown in the figure ok s o let me put some labels on this so they understand what i am try ing to say s uppose this is salinity again i go back to my oil mining example and i say that my decisions are based only on a single variable which is salinity and this is actually how the amount of oil varies right as the salinity increase i t is a very arbitrary function it is definitely not a linear function it is not even a quadratic function it is not an exponential function it is just some arbitrary function  but a mathematical function this is possible i t is quite likely that salinitie s has th is influence or in oil production or maybe it does not but i am just taking that as an example n ow what do we want the network to learn i f i take some data and train the network at the end of training what do i want s o if i feed at this point after training what should happen i t should give me this value right that is what training means and that means i should be able to approximate this curve if i do that that means i have learned from the training data s o let us see refer slide time six zero n ow we make an observation that such an arbitrary function can actually be approximated by a lot of something that we call as tower functions t hese are all single i mean pulse functions which you have many of these and you could have an approximat ion right and you can see that this approximation is bad at many places  but still it is an approximation it largely gives you the same shape as the original curve w hat would happen if i increase the number of such tower functions refer slide time sixthree one s tudent refer time sixtwentynine t he approximation would improve right i f i keep increasing it the approximation would go more and more better right s o now just try to keep things in mind whether i write in the theorem right you can make it arbitr arily close to the actual value  that means you can keep doing something s o that your approximation becomes better and better and you already see something of that sort t his is still in the sense of a figure we need to relate this back to a neural net work  but you see that as i am increasing these tower fun ctions i become approx arbitrarily close to the actual function refer slide time sevenfive n ow this is what is actually happening right i have multiple such tower functions i am adding them up all of them are shifted in time s o this tower function is actually this one this tower function is actually this one and so on right and i have not drawn the remaining ones i am taking all of these tower functions adding them up and getting my original fun ction right and the more such tower functions have the better is the approximation refer slide time seventhirtyfive n ow you make a few observations right all these tower functions are actually the same what is the only difference they just shifted and their magnitude changes right  but they are all tower function right s o let us think of this that if i know how to make a rectangle then i can make any rectangle right i just need to change the size of the rectangle and maybe shift it or oriented differently or something right s o they a re all similar i just need to learn how to draw a tower right that is what my quest is n ow if i take the original input salinity pass it through multiple such blocks each block is capable of making a tower function and ea ch of these would give me one of these towers that i am looking for and i am looking for so many of these right if i have as many such tower makers then i could get these towers i could just add them up and then get the original funct ion back  a nd the more these i have the better is my approximation right s o i am taking as input the salinity and trying to predict the oil d oes this make sense s till we have not figured out a neural network way of doing this we are still building intuitions of how to do this n ow our job now is to figure out what goes in this black box that is the tower maker and how does it connect to neural networks if you figure that out then our story i s complete t hen we know that a neural network can actually do this and that p recisely proves the statement which i had made that it can it can represent arbitrary functions s o we will figure this out over the next few slides refer slide time ninetwelve n ow if you take the logistic function and set w to w to a very high value w hat will we get just try to think about it the answer is already written  but i want you to imagine it w covers what student refer time ninethirtyone t he slope right  a s i make w very high what will happen is i will get the s o let us try changing the v alue of w ok i just increase the value of w and see what happens to the sigmoid curve refer slide time ninefortyseven s ome error here actually there is some problem the w value should have increased and that is how the sigmoid slope increases not the b val ue the b value comes later on s o actually sorry about this the w value as i keep increasing s o do not think that b is increasing think that the w is increasing i t will become sharper and sharper and it will come very close to the step function  right it will not become exactly the step function that will only become in the limit  but if i keep increasing i will get very close to the step function everyone agrees with this n ow what happens if i increase the value of b it will shift everyone is conf ident about that can you tell me why student refer time tenthirtyfour what will shift actually  t he point at which the transition happens right s o what is this point actually student refer time tenfortythree t his is the point at which i get that half value right and let us look at our function this is a function w hen will i get that half value when w x plus b is student zero zero right so that means x equal to minus b by w that is why it is proportional to b s o as i keep increasing the value of b t his will keep shifting ok i s that fine everyone ok with this refer slide time elevensixteen n ow what if i take two such modified sigmoid functions which are shifted differently and both are very close to the step function right s o here is where one thr eshold is here is where the other threshold is and now i subtract this one function from the other what will i get y ou know the term y ou will get a tower right is that fine everyone gets this right s o these places up to this point both are zero s o zero minus zero will be zero at this for this small range this is one and this is zero s o that one minus zero and then afterwards both are one s o one minus one would be zero s o you get that tower function so now i have my tower maker refer slide time twelvefive n ow can we come up with a neural network to represent this operation i want a sigmoid neuron i was working with a sigmoid neuron with some arbitrary weights right so that i recover that step function can you imagine n ow given x i want this tower function and that is exactly what one of the blocks was right s o what i am asking you is oh god s o i am asking you to give me a neural network for this c an you think of it c an you try imagining it two neurons in the hidden layer  h ow many of you agree with tha t ok can you can you take some more time to imagine what it would be a nd i have already ok right refer slide time thirteenseven s o this w one b one if i set it appropriately i will get this step function i f this w two b two i set it appropriately i will get this step function n ow i needed to subtract one from the other right s o i will do plus one minus one this is just a simple addition and i will get this is that fine everyone agrees with this this is just a adder right this is just an aggregator ever yone gets this s o now i have given you the tower maker i f you put enough of these tower makers and learn the w s appropriately what will you get t hat function that we were needed s o you can approximate it arbitrarily to any precision that you want a s long as you keep increasing the number of these units right s o these units actually give you one tower more of these units that if you have actually this much this is the input ring t he more such tower makers that you have the more is the bars that y ou will get and then you can approximate everyone gets the intuition behind this f ine ok t his all is always good in one dimension refer slide time fourteentwentyone n ow what will happen in two dimensions w hat if we have more than one input w hat is the towe r there d o you do you guys all do all know what is the tower there i f you say no i will give you a zero on the assignment remember the last question of the assignment d id you all make a tower d id you all make a twod tower d id you all copy that no s o what if we have more than one inputs suppose you had again trying to take a decision about whether we will find oil at a particular location of the ocean right and suppose now we base it on two two right s o say this is salinity this should be x one sh ould be x two should be y and this is pressure n ow just observe about the red and blue points s o the red points are where you will not for those configurations of salinity comma pressure you will not find oil and the blue points are for which you will find oil w hat is the one thing that you can tell me about the red points and the blue points not linearly separable right  but we still want to be able to learn this is that fine a single perceptron cannot do it i will also make a case for a single si gmoid neuron cannot do it and then i will show you that i n fact first i will show you that with a network of neurons we can do it and then i will show that with a signal single sigmoid neuron you cannot not actually do that s o now this is again a vali d problem you could have we could imagine that you will get this kind of data where you have two factors and your function is some arbitrary function of these two factors i t is not a need linear boundary between the blue and red points e veryone sees that the blue and red points are not linearly separable you cannot draw a plane such that all your red points lie on one side and the blue points lie on the other side everyone sees that ok  but the solution which i have plotted here that is a good solution i t makes sure that all the red points are in this region and the blue points are outside s o it will predict a high value for these red points and a zero value everywhere for the blue points is that obvious h ow many of you understand that figure o k go od refer slide time sixteenfortysix so now i want to show that even in two dimensions i could come up with arbitrary  i could come up with a neural network which could actually approximate this and again what will i look for a tower maker right i just want s omething which can make towers a nd approximate it refer slide time sixteenfortyeight s o this is what a two dimensional sigmoid looks like slightly incorrect because i have what i have done is i have actually said w two to zero s o if you actually i would want you to do this go back and plot this for w one equal to three and w two equal to three j ust go back and plot this and see what you get you will not get such a smooth such a nice looking s  but you will still get something which looks like looks like a snakes ho od right s o in still get that s shaped function it just that it would be bent at some points and it be thinner at some points and broader at the other points s o just go back and see and then you will realize what is happening right refer slide time seventeentwentynine s o here again what we want to figure out is from the single sigmoid i was able to take you to a tower function right from a two dimensional sigmoid what does a tower mean here and how do i take you to the tower s o that is what i want to do s o i have sa id w two equal to zero and i will it will become obvious why i have done that s o just understand what the figure is doing right s o this if you just look at this is like the cross cut right so you are looking at the front view of this figure and that is just the sigmoid function without the w two right a nd now since i have said w two equal to zero no matter what i set x two to the same function will get repeated throughout that axis do you get that s o that is why this entire function is just getting repeate d throughout this axis and then you just get a similar s shape function everyone gets that how many of you do not get that how many of you get that s o this if you look at the front view this is the sigmoid of one variable  but since i have said w two to zero no matter what i change x two to the function is going to remain the same s o it will just get copied throughout the x two axis is that f ine with you n ow what will happen if i increase w two sorry w one s ame thing right it will just keep shifting till it becomes almost like a twod step function ok n ow what will happen if i increase b s hift i can do the same thing here also same logic applies here also ok refer slide time nineteentwelve now what is the next step that i am going to do take two of the se which are shifted by some point a nd then subtract what will i get e veryone had this figure in mind s o just see right s o this portion both are zero s o zero minus zero would be zero this portion this is one  but this is zero s o that would be one minus zero and again in this portion both of them are one s o one minus one would be zero s o you will get this kind of function would you like to live in such a tower i am very serious yes or no n o why it is open from two sides right you cannot live in this tower s o you wan t something which is a closed tower right s o how will you do that give me an intuition w e will do the reverse thing w hat will be set to zero w one ok refer slide time nineteenfiftyeight a nd this is how it would look the orientation would change and again so n otice that this is your sigmoid function and since i have set x one to zero no matter what i change along the x one axis the same function gets copied and you get a nic e looking a sigmoid function n ow again i will do the same thing i will increase the w i wil l get a close to a step function i will increase the b i will move along this axis refer slide time twentytwentyfour next step take two of these subtract get what another tower function this is also not a tower that you like to stay in s o what do i do now add them sure add this tower to the other tower refer slide time twentyfortyfour s o now what will happen if i add these two will you get a tower function w hat will you get y ou will get a tower function with a parking floor right is that what you will get everyone understands why this is s o these portions both are zero s o you get zero same logic applies for all the four corners right is that fine  n ow for this portion or rather this area right s o this guy is zero this guy is one s o you will get a one the same logic applies for all these four corners in the centre both are actually one s o one plus one would give you two s o this is two this is level two this is level one this is level zero is that fine s o what am i done so far i have taken my x one x two passed it through some transformations right this what are these transformations we will see  but transform it through these multiple hoops right where i adjusted a w s and b s and i have got some z right and this is how that z behaves for different values of x one comma x two i will get these different values and these values range from zero to one to two is this pictured clear i have taken my original x one comma x two passed it to some of these transformations and irrespective of what my x one to x two is this tells me the entire range of values that i will get f or some combination of x one comma x two i will get zero for some combinations i will get one for some combinations i will get two and some combinations also between one and two right s o these places where it transitions is that clear is that picture clear to everyone s o now i can treat this as the output z ok n ow from here how do i go to a tower threshold it  h ow will you threshold it w hat do you want you only want this much part to exist right this without the pa rking floor how will you do it any output which is greater than equal to two you want to keep it any output which is less than two you want to make it zero i f i do this will i get a tower r ight sorry greater than equal to one any value which is greater than equal to one you want to keep it anything which is less than one you want to make it zero s o this entire thing will get demolished how do you do this  this is an if else ok i f else if something is greater than equal to zero do something else do something e lse what is that perceptron right  but we do not want to use perceptron w e want to use sigmoid neurons have you learned an approximation from a sigmoid neuron to a perceptron very high w right  y ou get the intuition let us see what we do on the nex t slide refer slide time twentyfourseven s o i take this any z which comes from here i will pass it through a sigmoid neuron which are very high slope such that the threshold is at one anything which is greater than one will pass out as one anything which is less than one will go to zero s o everyone sees how we took this structure and converted it to a tower w e have this tower now now what do i do with this refer slide time twentyfourthirtyfour i lead multiple such towers and i can approximate this i could put a tower here here here and so on  i could have these multiple towers a nd here of course all my towers would be of zero height right in this region right s o now i can cover the entire twod space with a lot of tower functions and approximate this exactly that is a very weird statement approximate this exactly i mean approximate this to arbitrary precision everyone gets this do you see why we constructed these tower functions and now we can put them inside this cone and approximate it refer slide time twentyfivenine n ow all this is fine i was making some towers there s o can you now give me a complete neural network which does this i want you to imagine that remember you are taking what i am asking you to do is this x one x two give m e this such that i get this two dimensio nal tower i do not know how to draw it something like this ma ybe whatever s o i want this two dimensional tower what is this network of perceptrons going to look like j ust go back to all the operations that we did and try to imagine in your mind n o we w ill not use perceptron because we can always use a sigmoid neuron instead of a perceptron with the high w i do not expect you to answer this i just want you to imagine right we just try with a there is something known as a pen there is something on a p aper ok s o here is the solution refer slide time twentysixeight s o what is happening here you have this salinity and oh actually this is slightly wrong i do not know why you guys saying it is correct a ctually at both places i need both the inputs it is just that in one case i do not care about that input because i have said w two t o zero s o i learn these weights w one w two b w one w two b of course here the network should learn that w two is equal to zero right and then you get this one tower do not needs this to be mod ified this figure is incorrect s o we need x one x two both as inputs we need to label it with w one w two equal to zero and b and so on it s o we will discuss this later anyways  but you get the idea right that you take these two inputs make one tower take the inputs again make another tower add them up to get this function pass it through this step neuron function step sigmoid function so that you get the tower s o this is one block y ou will have many such blocks each of which will learn different w s and b s s o that they get shifted and then you will place them all together you have an aggregator on top of this which will combine them j ust a minute how many of you get this ok g ood y es s o that is a good question i am going to come to that right s o i have very conveniently given you a solution where i have what is the bad thing that i have done i have hand coded these things right i have hand coded w ones w twos and b s is that fine in practice n o i mean that is where we started off and we d o not want to hand code these right s o now you know a learning algorithm for a single sigmoid neuron n ow what you have is a network of neurons right for this network of neurons i need to give you a learning algorithm driven by the objective function that whatever output it gives would be very close to that arbitrary function that you are trying to model i f i give you a learning algorithm then you would be convinced that if this has to be minimized and the weight configuration which need it needs to a rrive at as w two is equal to zero then the algorithm should be able to do that right because we saw we have some faith in these algorithms in the case of a signal sigmoid neuron that with the right objective function it will give me a principled way of rea ching that objective function i n this big network my objective function is to arbitrarily to approximate this of this true function right s o now if i give you that as the objective that whatever outputs the network generates so the network might gene rate something like this s o that has to be very close to the true output that is the objective function that i am going to use in that learning algorithm and if that learning algorithm works which will prove then you should be able to arrive at the neces sary weights to make this approximation right i s that clear and i n fact there might you might not even have to do these multiple towers in practice a ll i am trying to prove is that there is one solution which exists if there is one solution which exis ts i can say that locate the network can learn that is the only claim i make i am not saying this is the only solution right same as in the case of the boolean functions where i said that one solution exists where you have to raise to n neurons of the hidden layer that was a sufficient solution that was not a necessary solution for the and function we were actually able to do it with a single sigma neuron right s o just keep that in mind i am just giving you a sufficient solution a nd the network c ould actually learn something better than this all right this is again a very bulky solution why i t scales with the number of neurones proportional to number of input variables that you have s o that is for a sufficient solution  but you would want som ething better than that all i am trying to say is that it can approximate i am just telling you the representation power and just as we had the catch there that the hidden layer is very large the same catch applies here also is thi s story clear to everyo ne s o i have given you a solution i have not told you how to learn the weights i have given you a network n ow later on we will discuss a learning algorithm for this network a nd we will have some confidence that given a particular objective function th at learning algorithm can strive to go to minimum error or minimize the quantity of that objective function that is going to come in two lectures from now is that fine refer slide time thirtythirtytwo a nd that was for the tower function now i could have act ually directly done this right s o i wanted to approximate these functions s o i could have placed a lot of these kinds of things here and approximated it right s o that instead of that very high slope sigmoid function i could just use a normal sigmoid function also o k a nd again there is a error here  but i hope you get the picture it is just that you feed both the inputs to them refer slide time thirtyfiftysix s o for one dimensional input we needed two neurons to construct a tower f or two dimensional input how many neurons did we need i am just counting these because these are simple aggregators right and this is one constant at the end s o how many did we need actually o of two n i mean o of i mean s o for n how many would we need l et us try to work th at out ok s o i will ask you that in the quiz how many do we need for n dimensions refer slide time thirtyonethirty n ow why do we care about approximating any arbitrary function we will again try to close the loop now we saw that we can arbitrarily we can approximate any arbitrary function b ut now again i want to come back to the point why do we want to do this and can we tie this back to the classification problem that we were dealing with refer slide time thirtyonefortyseven a nd this is the data which i had gi ven you which was there were some points some values of x and y sorry this should be x one and x two it is where this is pressure and salinity or salinity tendency and this is the output which is oil n ow there was this is what the function actually looks l ike now what would have happened if i had used a single sigmoid neuron to try to approximate this function try to represent this function and sigmoid neuron in two dimensions right so the two dimensional sigma what would have happened can you give me one s olution for this r emember earlier i had said that perceptron cannot handle data which is not linearly separable  but then i anyways used it for data which was not linearly separable a nd we got some line such that we got some errors the red points and the blue points are not clearly separated s o i am asking you for a similar thing here i force you to use a sigmoid neuron what would you give me refer slide time thirtytwofortyseven i s this fine t his is one of the possibilities of course it could have been orie nted differently and several things what is happening here is that for these blue points it is acting correctly  but for these red points it is not acting correctly i am assuming red is positive and blue is negative i think that should have been the oth er way round  but let us assume re d is positive and blue is negative a gain now for these red points this part is working fine  but it is misclassifying all these blue points s o all these bad locations is actually saying that you can find oil and for al l these good locations here it is saying that you cannot find oil that is what a sigmoid neuron would do and you could have multiple solutions are possible here right  but all of them would have this problem that will make errors on some red points and so me blue points right but the true solution that we wanted is something like this again there are multiple solutions possible right you could have anything there are you could have even finer one side you could just have this much there many things poss ible this is one such solution w hat the illustrative proof told you is that you can actually use a network of perceptrons and approximate this arbitrary function which exists between the input variables and the output variable s o if this is the functio n which exists between the input variables and the output variables n ow yo u could take these multiple two dimensional tower functions and approximate it with the catch that you might need many of these in the hidden layer  but you can still do that ok s o that is why this in theorem important because now any problem that you take right any problem that you will have in machine learning would always want you to take an x learn a function of x which takes you to y this function will be have s ome the functio n will have some parameters right and now what this theorem is saying is that you could adjust these parameters such that you can arbitrarily come close to the true function right s o that is the significance of this a ny machine learning problem that you can think of in the sense of classification or regression you would find that this is useful and i am giving you a very powerful tool to do that of course with the catch that i am not giving you any bound on the number of neurons that you will need i am just saying use as many as you want"}
{"audio_filepath": "Data/Preprocessed/PCA : Interpretation 2_46.wav", "duration": 983.0, "text": "so that is what we look at in the second interpretation of pca right refer slid e time zeroseventeen so again we have the same setup that given n are linearly independent for n orthogonal vectors we can represent x i exactly as a linear combination of these vectors what do i mean by exactly perfect ok if you actually describe the who le things in words ok so that is exactly what i mean right so you are going to write x i as alpha one i into p one plus alpha two i into p two and so on and when you do the summation on the lhs on the rhs you just get back the lhs when you do the summation o n the right hand side yo u get back the left hand side so that means it can exactly be represented when you use all the n eigen vectors now if i start chopping of stuff what will happen student refer time onefour it will just be an approximation ok now we this is what i meant and this is this the equation holds that means this is exact and we know how to find the alpha is because p js are conveniently orthonormal so we know how to find that easily ok now what if we consider only the top k dim ensions what is going to happen there is going to be some error in the reconstruction i am not capturing all the information in my original data but there is some error which i am not being able to capture and i made a conscious decision that that erro r is not important i am willing to let it go hence i want to represent the data using fewer dimensions ok so this is exactly what you do in pca when you take the top k dimensions is this fine ok so now we want to select p i s such that we minimize th e reconstructed error ok and this is again erratic actually we should try to write it as x i minus x since these are vectors and the square of vec tors would just meet this right so but you get the point right were just tryi ng to do the element wise squared error loss were trying to minimize that  we want to do this so now let us try to see that if you are aiming to do this what is the condition that we arrive at ok so no i thought i would ask for some changes on t his for a minute all of you can you just bear with the fact that these are actually vectors and not scalars so this square actually does not mean anything it actually means x i minus x i hat transpose x i minus x i hat so when i use square with vector s th is is what i mean is that  everyone can work with that notation fine so now what is x i actually the real point right the correct point which can be obtained by the full reconstruction if you consider all the n dimensions wha t is x i hat just an approximation where you are considering only the k dimensions remember that each of these quantities is a vector fine ok now what is happening here let me just try to say this  so let me just do this way so this is your original x and you are actually writing it as a linear combination of your p s somewhere you will have alpha k pk and then all the way up to p n so this is p k alpha n ok now what is this full thing this is x and what is this x hat ok you see the picture what is the equation trying to tell you ok now what is the difference between these two then these guys right if i want to take difference between x and x hat everyone gets that it is the remaining term say that means alpha k plus one into p k plus one up to al pha n into p n is that clear so can i write it as yeah can i write it as this ok so you get this right so i am only taking these guys because the rest will get subtracted so one is the full n dimensions the other is only k dimensions so if i take the difference between them what remains is k plus one to n dimensions and that is exactly what i have written here ok and now i am coming back to the proper notation where this is a vector right so i am writing the square as the dot product between the same vector is this ok these are the m data point right this sum this is overall the m data points you need to minimize that is that clear ok so this is fine now want just some rearrangement so i have just expanded out t hat summation this is what it would look like right i have just expanded out these two summations now let us try to do this in your head and see what are the kind of terms that you get there are two different types of terms that you will get so first of all let us understand that when you expand this you will end up with a lot of dot products you will get a dot product between this and this and this and so on right so can you split those terms into two different types student refer time fivethirtyseven squ are terms so one where i is equal to j and one where i is not equal to j is that clear fine so let me just write it as that so i will have k plus one to n right that means n minus k terms where i would be equal to j right so that means pk plus one w as getting multiplied by k pk plus one pk plus two was getting multiplied by pk plus two and so on and then i will have these remaining terms where i is not equal to z right so these are the dot product between the other vectors is it fine you see why i have split it this way what will happen now the second term will go to zero ok and wha t about the first term alpha ij square ok now what is alpha ij actually how did you find alpha ij student refer time sixtwentyeight it is a dot product between we did this ri ght finding any of these components is just taking the dot product between x i and that dimension so x i transpose pj is that fine ok is this fine and again this is slight abuse so this is actually what no this is ok right a this is ok sorry i am ju st going to write it as this is this fine i just written it twice and i can change the order since it is a dot product now what i am going to do is so this is actually summation over an index i and a summation over an index j and i can change the two summations i can interchange them ok so that is what i am going to do now is this fine i will push the summation all the way inside what is this actually this entire thing actually m times covariance of student refer time seventwentyfive so is this i is t his what you are telling me that this is m x transpose x is this fine how many if you do not get this i see a lot of blank faces how many if you do not get this quite a few so  this is so i is equal to one to m right so you are going over the data points ok so this what is the dimension of this actually student n cross one n cross one and this is one cross n what does this product give you student refer time sevenfortynine n c r o s s n what are the entries in this matrix so this was say x one one up to x one n an d this is again x one one up to x one n ok so that is going to be x one one square or rather let me just write it in the generic form right so it is going to be x one i into x one j right is that fine and how many such matrices are you adding student refer time seventeenseven m of these so what would you get then what would the first let us so ok so  let us do this so the first entry of this matrix is going to be x one one square what about the first entry of the next matrix in this series student refer time eight forty x two one x two one square right  so this is slightly tricky to demonstrate let me just a give me a minute i will just collect my thoughts and do it properly ok let us take a small example ok so x one one x one two x one three suppose we have a three dimensional matrix three dimensional data so i am taking a sum of m s uch matrices ok i equal to one to m that means this is going to vary this indexes the first index is going to vary from one to m now let us see the first matrix and let us look at the first element of that m atrix the first element of this matrix is going to be x one one square ok now let us look at the next matrix what is the next matrix going to be it would be x two one x two two x two three right and multiplied by x two one x two two x two three what is the first element of this matr ix going to be student refer time ninefortyfive x two one square what about the third one x three one square this is fine so far now you are adding all these matrices so what is the first element of the resultant matrix going to be x one one square plus x two one square pl us x three one square what is this actually this is the dot product of x one with itself right and what does that give you the variance if the data is zero mean right ok now can you make a similar argument of the ij th entry is going to give you the covariance be tween the i th and the j th entry is that clear right you could do a similar analysis you can actually work it out after going back how many of you have found comfortable with this there is still many who are not ok so let us look at an ij th entry ri ght so can someone help me with say that one comma two entry ok or the first matrix what is it going to be x one one into x one two right for the second matrix student refer time tenfiftytwo x no this is some yeah correct and for the third matrix three two ok now what is this sorry what is the summation of these when you take the full sum you will get these three as as what is this in this summation tell you student refer time eleventen covariance between student first and second the first column and the second column i s that clear now is it with everyone now ok fine so what you have here is actually the covariance matrix you seems to be lost is it with you sure ok fine so what we have here is something of this form ok refer slide tim e eleventhirtyseven so now what we want to do is we want to minimize this quantity subject to the following condition is that ok what is the solution for this if i did not have the summation ok suppose i just wanted one dimension so i want to minimize say p s ig p transpose sigma p such that p transpose p is equal to one what is the solution for this student refer time twelveeight smallest eigen value of sigma right and you can show by induction that if you want k such things that here i am looking for n minus k such things right then these would be the n minus k smallest eigen values of sigma but now i am talking about the smallest eigen values but in the first solution i said we need to pick the largest eigen values so what is the difference student refer time twelvethirtyfive these are the ones we are throwing away these are the ones along which the error is going to be minimum if we throw these away the error is going to be minimum so we will throw away the last n minus k dimensions which means well keep the f irst k dimensions is that clear so you arrived at the same solution is that right so that means in pca you are actually trying to pick the dimensions in a way such that your reconstruction error is minimized and this was exactly what our reconstructio n error was so do not worry about this math bit just see that we started with this quantity this is what we wanted to minimize ok and we did some trickery and we came to this formula that minimizing that error is equivalent to minimizing this quantity  and for this we know the solution that the solution is the smallest eigenvalue and we want n minus k such things that means there would be the n minus k smallest eigen vectors is that clear that means we are going to keep only the k largest eigen vecto rs ok that means you are going to project your data on to k largest eigen vectors now so the key idea here is this right minimize the error in reconstructing x i after projecting the data onto the new basis refer slide time  thirteenfortythree so let us take an example and we will work with our toy example again so this was the data that we had and suppose i give you a new basis which is one comma one and minus one comma one ok this is a new basis this is an or thonormal basis orthogonal basis you can see that u one transpose u two is equal to zero ok now i need convert it to an orthonormal basis so i have just divided by the magnitude is it ok fine now consider the point threethree comma three this was our original point ac cording to which coordinate axis x comma x that means this was threethree and this was three ok now i can find the alpha is right because this is an orthonormal basis i can directly find the alpha is now the perfect reconstruction would be this so actually if i do this i get back the original point now what would happen if i throw away the second dimension because the second dimension had corresponds to a smaller eigen value ok i will get this so you see that the point is still close to the original point i have not actually lost much right what has happened is i have actually projected the boy lie point on this line right the line x equal to y that is why i get x equal to y and in doing that i am not losing much information from the original data is this clear right so you understand what happens when you reconstruct the data there is no end to this ok so just to recap the eigenvectors of a matrix with distinct eigen values are linearly independent and we use this fact con veniently at least in the case of square matrix where the also happen to be orthogonal so we know that they can form a very convenient basis and pca exploits this to find the top k eigen vectors which to be retained and while doing this they have seen t hat two things are at least ensured one the covariance between the dimensions is zero because that is exactly how we formulated it and found the solution we saw that it turns out that we need to diagonalize a certain matrix and the solution is the eigenvecto rs we also saw a different interpretation where we saw that it is the same as throwing away the dimensions along which the error would be minimum right and both these interpretations led to the same solution which was project the data onto the eigen vecto rs of the covariance matrix of the original data ok and this n minus k dimensions current contribute very little to the reconstruction now what is the one thing which i have not proved yet what was our wish list student variance and covariance varian ce and covariance right high variance low covariance i proved low covariance i have also proved something with respect to reconstruction error because that is something i require for auto encoders so just remember this bit about reconstruction error"}
{"audio_filepath": "Data/Preprocessed/Dropout_68.wav", "duration": 952.0, "text": "refer slide time zerofifteen s o in this module we will look at dropout now refer slide time zeroseventeen s o the intuition that we have developed in the previous module which was about ensemble methods is what that is that ensemble makes sense in most cases b ecause you do not expect the errors of these k models that you a re using to be perfectly correlated a nd we saw that whenever they are not perfectly correlated you are going to get some advantage n ow how do you do this in the context of neural networks s o remember what was bagging multiple instances of the same network trained on differe nt subsets of the data w hat is the problem with this in the context of neural networks e ach of these neural networks is very complex training each of these is going to take time and i going to train k of them is that fine right s o you decide ok s orry s o one option that you have is you train s everal different neural networks having different architectures right  but this is going to be expensive because you have to train k of them t he other option that you have is you train the same network but o n different subsets of the data this is also going to be expensive s o whatever ensambling sampling techniques you can think if in the think of in the context of neural networks which are essentially t hese two techniques different architectures and take a n ensemble or tr a in the same architecture on different subsets of the data both of them are going to be expensive right s o now how do you go about it a nd it is not just training time expensive it even if we manage to train it at test time again w hen y ou a re given a test instance you have to pass it through all of these complex neural networks each of which is going to take some computation and then take the ensemble of the outputs right s o even at test time it is expensive it is not just that that training time it is expense refer slide time onefiftyseven s o now dropout is a technique which addre sses both these issue s which issues t rain time computation as well as test time c omputation s o it effectively allows training several neural network arc hitectures without any significant computational overhead s o we will see how that works and it just not training time as i said it also allows us to do this quickly at test time refer slide time twotwentyone s o again let us see  so again here ok i wi ll g et to it when i know s o drop out actually refers to dropping out units from the neural network s o this is my original neural network and i a m just talking about one neural network forget about ensemble s just one neural network is what i have n ow what dropout says th is y ou dr op out some units from this neural network  that means drop o ut some neurons and when i drop out some neurons i am also going to drop out the incoming and the outgoing edges o therwise where are they headed right  so i am just dro pping out s o basically what is effectively happening here i am getting a new network architecture right a t least that is clear that is what dropout effectively does but i have already made a case that i do not want so many architectures t hat because it is a headache to train all of them and again a test time i have to pass it through all of them right s o i need to still fill that gap  but drop out says that drop some units and you wi ll get a new architecture b ut how does that simplify life we wi ll see that a nd now each node is actually retained with a fixed probability for the hidden nodes and even further input nodes s o then we were not wrong in actually dropping out the visible node b ecause you can do dropout at the visible nodes also ok a nyways yeah s o for the hidden units you would drop them with a probability fifty percent and the input units you will drop them with a probability of twenty percent typically it a gain is some hyper parameter that you will have to tune but typically this is wha t you will do and i hope you see that dropping nodes from the hidden unit f rom the input unit is same as corrupting the input data right it is same as adding noise to the input data is that fine refer slide time threefiftyeight s o this is the idea now let u s see how to actually implement this idea ok s o suppose a neural network has n nodes using the dropout idea e ach node can be retained or dropped an example in the above case i have dropped some five nod es to get a thin ned network s o if there are n nodes w hat are the total number of thin networks t hat i can get from it a nd so  that means i can get two raise to n different neural networks a m i happy about this or sad about this s ad there i s just too many neural networks h ow can i train them actually rig ht s o how do i do this i a m just creating a lot of suspense without giving you the answer ok s o first trick is share the weights across all these networks ok w e will see what that means and the second trick is sample a different network for each tr aining instance ok n one of which is clear at this point i can see i can read your faces i a m good at it ok s o let us see how to do that refer slide time fourfifty s o we initialize all the parameters of the network randomly or whatever may be used an d start training w hen i start training i wi ll pick up the first training instance or the mini batch or whatever i am doing we apply dropout resulting in this network w hat will i do and they forward prop forward propagation right ok n ow ok we compute the loss and back propagate how s ome weights are missing right how do i do back propagation now i ha ve deliberately dropped up some of these connections they did not participate in the forward propagation t his back propagate which are the parameters w hich will update now o nly the ones w hich actually participated right s o i wi ll just do back propagation j ust look at the red arrows i wi ll just do it over the paths which are actually present in my network fair enough right t hat is what you meant by normally ok that is normal ok s o i wi ll just do it over the weights which actually participated that is fair enough that is the only thing you could  obviously do refer slide time fivefiftyone n ow i take the second instance again i apply dropout and qu ite naturally i wi ll get a different thinned network as you see the figure three in this slide ok w hat would i will do now s tudent f orward propagation f orward propagation then compute the loss back propagate to compute the loss ok and then s tudent b ack propagate b ack propagate again back propagate only to the s tudent a ctive nodes a ctive nodes  so these o the r nodes which will get activated s o what is happening here is now trying to relate it to what we were doing in bagging right w here we are try ing to train these different networks on different subsets of the training data right d o you see something similar happening here t here are many such thin networks e ach time i am sampling a different network and updating it right s o it is equivalent t o training these large number of networks on different subsets of the data right but then the problem is that some of these networks may never even get sampled t here are two raise d to n of those my amount of data is definitely to be less than two raise d to n s o some of these networks might just not even get sampled t hen what is happening o r they would get sampled very rarely right f or example what is the probability that again i wi ll end up with the same network we are computing it g ood it is very less ok i am fine with that at seven hundred and thirty right s o it is a very less right s o it is quite likely that this network will never be sampled again  that means for that network the parameters are getting updated very few times am i fine with it y es i am w hy b ecause the same weights will get updated for a different network i am just using the same weight matrix throughout remember that  my w matrix or w one w two is the same throughout i t i s just that at different depth subsets different instances i am just t ouching some portions of this w one and i a m not touching the other portions of w one s o now what would happen  so i have shown you two training instances right w hat would happen to the weights w hich were active for the first training instance as well as the second training instance it will get updated twice and which are active only once student refer time sevenfiftynine o nly once right s o over a period of time many of these weights are shared across all these networks that i am sampling right s o even th ough a particular network is sampled only a few times its weights will get updated many times via these other networks w hich are similar to it d o you get that h ow many o f you get this o k g ood so what is happening i wi ll just repeat that i have just one weight matrix i am sampling a thinned out network which only uses some of these weights s o for that training instance i wi ll update those weights n ow i know that the likelihood of the same network getting sampled again is very less  but i do not c are about it because i could sample a different network  but i am sure that some of these weights will again repeat in that right a nd in that i told they wi ll get updated s o even though each of these networks is seemingly getting very few updates o ve rall all the weights shared by these networks are getting updated as much as they should be is that fine everyone gets this idea o k fine and while i am also taking care that similar things like early sto pping or weight regularization l two regularizatio n w here i am not allowing a single weight to continuously grow or something o therwise because these weights will be off for some networks i s that fine y ou see the connection between early stopping l two regularization and this is that ok refer slide time nineeighteen a nd s o each thin ned network gets trained rarely or sometimes even never  but i a m not worried about it b ecause it is weights will get updated through some of these other thin networks refer slide time ninetwentyseven t his is all finite train ing time a t training time what is happening is this is one of these blue guys introduce on with the probability p  that means the weights going out of it who are available with a probability p right and other times they were not available n ow what do i do it test time i cannot let me finish this ok i cannot take an ensemble of d ok the answer would have been that at test time instantiate all these two raise d to n networks pass the training passed the test example thro ugh all of them and then take a n ensemble right but of course that is probablitivly expensive s o what will i do at test time w hat is the simple trick that i wi ll do s o he says that just use this network a nd just use the final net matrix that you had no  but then you have guessi ng out of the two raise d in the sample some small number of those and do it a ctually drop out uses something very simple than this w hat it says is that each of my nodes was present only p fraction of the times in the training data ok  that means one way of looking at it is that s o imagine that you could think of this as the analogy is that all these nodes are participating in a discussion right w here they trying to see how to do this job properly but with probability p they all sleep off right s o at the end of the meeting you will trust each of them only with probability p s o that is the simple trick with dropout uses it says that just scale their weights by p b ecause that is how much i trust this node it only participated in p faction of th e decisions s o that is the confidence that i have in it s o if it is saying that with wone weight do this i wi ll only do it with p into w one weight does that make sense ok a nd there is again a squared egg with vacuum kind of explanation for this ok w hic h was there in the quiz last year which is very convoluted it does not really give you the true picture because you can derive some math a nd so that this is mathematically proper  but that again works in very specific conditions  but at least if you ge t the intuition that is fine that what we are saying is that t hese nodes will leave an active a few number of times s o i wi ll only trust them that much and i will just scale their weights by that factor s o at test time i wi ll just pass my test instanc e through one network which is the full network with the weights scaled according to the rule which i just said that is exactly what dropout does refer slide time twelvethree s o what dropout actually does is we will apply some kind of masking noise to the hidden units right s ince the same as seeing that you a re computing the hidden unit  but then you a re masking it off ok s o what is the effect of this i wi ll give you the answer and i like i like you to think about it t he answer is that it prevent s the neurons from becoming lazy w hat do lazy people do t hey depend on others yeah actually yeah they depend on others no w s o let me answer that give the answer for this and then tell me whether that is still contradict ok s o let us see right consid er this layer of neurons a ll of these are collectively responsible for what happens to this guy right n ow y ou see what i mean by neurons becoming lazy i could just see ok i will not give my input these other ne urons will take care of it they will adj ust their weights s o that they eventually it will fire or not fire or whatever right you see that could happen but now these neurons cannot rely on their neighbors b ecause they do not know when their neighbors are going to ditch them right t hey wi ll suddenly drop off ok a nd now i was waiting for my neighbor to actually do something and he i s not going to do it s o i have to be alert alw ays do you get the analogy s o these guys are collectively responsible for something and they know that some peopl e in the collection are going to betray them s o each of them has to be more careful so the more technical term for this is that d oes not allow the neurons to co adapted s o it does not allow them to get into this mutual agreement t hat you take care of certain things i will take care of certain things and together we will do the job right y ou do question one i wi ll do question two i a m ok it does not allow them to do this s o let us just concretize that intuition a bit for s o essentially a hidden un it cannot rely too much on other units a s they may get dropped out at any time e ach hidden neuron has to learn to be more robust right i t has to do the job as if it is the only guy responsible for the job ok and let us consider one of these neurons h i refer slide time fourteenzero a nd let us see that a h i learns to detect faces s orry it learns to detect a n o se s o i a m trying to do face detection whether an im age is about a face or not and h i is the feature which fires i f there is a face somewhere if there i s a nose somewhere in the image is that fine n ow if all these guys start acting lazily ok t his guy is going to detect a nose  that means definitely face will be there s o i do not need to do anything right what would happen now s uddenly t his guy is going to go away dropped out  so then these other guys need to do one of two things either add redundancy  that means one of them should also take responsibility for detecting a nose or do it in a different way t ake responsibility for detecti ng the lips or the eyes or some other part do you get that r ight b ecause you know that i cannot co ado p t e d with my other neurons i cannot say that ok in these front facing faces you just detect the nose and wi ll be done and we wi ll all keep quiet right i do not know whether you will do your job properly s o i will have to add more redundancy y ou detect a nose i will also detect a nose or you detect a nose and i will detect something else which helps detecting the fea ture right s o that is why thes e networks become more and more robust as you add this dropouts refer slide time fifteentwentysix s o that is all that i had to say i still do not know whether i ha ve answered your question or not a ll of them try to detect nose see as long as that helps reduc ing the final loss it is fine i t is just the case that you would have some training images where the nose is not visible maybe that person is drinking something right s o for at least for those training instances someone else has to take care that you d etect from the other images right o therwise a loss would not be zero for that training instance s o as long as you have some training instances see if all your training instances can be detected just by detecting the nose t hen there is nothing wrong in all of them trying to detect the nose s o if the training it is like that it will happen  but the hope is the training data is not like that right is that f ine  so we will end here"}
{"audio_filepath": "Data/Preprocessed/Principal Component Analysis and its Interpretations_45.wav", "duration": 1497.0, "text": "so in thi s module we wil l talk about principle c omponent analysis an d it is different interpretati ons i n this mode l we will look at one interpre tation and then in the re st of the module some other interpretations so the story i add is going to be this we will talk about pc a and it is interpretations ok so now let us try to motivate pca first consider the following data ok in what dimension is this data student two dimension two dimensions it is r two ok and each point here is represented as it i s x coordinate and using it is x coordinate and it is y coordinate ok now it means that were using x and y as the basis right that is clear that is the standard way that you would do any data point you will just represent using that basis now what if w e choose a different basis let me give you one basis and then let me ask you some questions on this suppose we chose this basis so in the previous modules we made a case for the x and y coordinate axis there is nothing sacros anct about it you could use any basis the only condition on the basis that the vector should be linearly independent and in fact if they are ort hogonal it is even better right so now i have given you a different basis now what do you make any observatio n here so they have all the points here have a very small component along the u two axis right so now this so far this point right if i consider at this point then this is the component along the u one axis so that is it is u one coordinate as akin to the x coordinate and this is it is u two coordi nate akin to the y coordinate  is a are the arrows clear here so that means there u two coordinate is very small and it is also very small for all the data points right so it is almost as if there is some noise there it is all within some epsilon now so it seems that the data which were actually represented in r two can actually be represented in r one by getting rid of this noisy dimension right so if you had chosen a different basis you realize that with just one dimension you could have captured everything that was there in the data and the other dimension was just adding noise it was redundant there is h ardly any information there so now can you state this more formally because t his is this intuition but can you stated more formally in terms of things that you have learned and say probability for it for example what is wrong with the direction u two the spread of the data points along the u two direction is very small what is the spread mean the v ariance right so we do not care about u two because the variance in the data along this direction is very very small ok and in particular right if i were to build a classifier then would u two have any predictive power because along this dimension the points are indistinguishable ok so  think of it that you are trying to find out whether you have so you have say one hundred candidates and you want to decide whether they would be good basketball players or not and quite naturally all the peopl e that have shown up are say six foot two and six foot three inch and so on and there in a very small height difference between them and all of them are sixtwo is the average and very close the spread is not much so this feature is not going to help you decide wheth er this person is going to be a good basketball player or not you will have to rely on other features where the variance is more for example how many teams has he participated in the past how many matches as he won as a team as a member of some team and so on it so those who expect some spread to be there all these one hundred candidates might have different things right but if the height is the same for all of them it is not going to be a good predictor and that is exactly what is happening along the u two di rection the points are almost indistinguishable there that is why it does not matter so in general given any data now this was a simple case where the data was r two i am talking about the general case where the data is rn ri ght and you will find this situation in higher dimensions also so you would not want to use that entire n dimensional data where you know that there are some columns along with the variance is very small so you want to represent the data with fewer d imensions such that the data has high variance along those dimensions now let me just clear a confusion here right so i am not saying that take your n dimensional data ok find the variance across each of these dimensions and then throw away the colu mns which have the lowest dimension in this particular example if you had done this what would happen could you have done that think of the original dimension s x and y along these two dimensions there is enough various in the data right the x coordi nates vary from here to here and the y coordinates also vary from this point right up to that point right so there is enough spread in the x and y coordinates so in your original data i am not saying that pick look at each column and see if there is no variance along that column then throw it away that would not work because you might end up with the situation that there is enough variance across each of these dimensions it is just that when you look at the data from a different angle that means yo u projected onto a different basis this becomes clear right so you see the difference i that is not the same these two things are different operations so what i am looking at is projecting the data to a different basis that is exactly what i did with u one and u two and then some things became clear about the data now this projection along a different basis i would be interested in doing that only if i can get rid of the number of dimensions right if now i had already had one basis where i had n dimen sions now if the new basis is also going to be that all these new n dimensions that i have come up with are important then you are not gaining much i do still have this high dimensional data but you would like to project it in a way that you get rid of the lower variance dimensions so you might project it onto n dimensions but you want to rank these dimensions according to variance and then throw away some of these dimensions is that clear is the objective clear ok fine is that all that we care ab out n dimensions project to a new basis and throw away the key dimensions which have less variance is that all what else would you want people have done the mlpr course no i would not so i am not going to classification or anything i just want a be tter representation of the data at this point i am not really thinking about what i want to do with the data maybe you are talking in terms of classification and we have already seen even if the data is not linearly separable we have solutions for deali ng with right so that is not a critical point ok so there is something else that very intereste d in and let us look at that now consider this data i have three dimensional data ok do you find something odd about this data st udent refer time sixfiftysix y and z are student refer time sevenzero are highly dash student correlated correlated right do you want these dimensions can you think of any practice such dimensions occurring height in centimeter and height in inches someone would have just given you data right or if you if you take the credit card a credit card fraud detection case right someone would give you the salary and it would also give you the income tax now these two are highly correlated right so then you do not really care if you have one you could probably almost with certainty predict the other right modulus some rules right because you get some tax exemptions and all that but still so you can have this in practice but even in our oil mining case yo ur salinity pressure density those things could be related right so z is not adding any new information beyond what y is happening so the two columns are highly correlated so actually yeah this is the formula for correlation all of you know this anyon e who does not know this formula good so not nothing is a stupid question right so you can always ask so y hat is the mean of this column ok  sorry y bar z bar is the mean of this column and this is how you compute correlation this is just the formu la ok so from every entry you subtract the mean ok so this is known as centering the data so if you do this what would the mean of the new data be student zero zero right so that is why it is called centering the data ok so i will have zero mean zero mean and you so what does this what is the intuition behind this formula does anyone know can anyone tell me so this is a summation ok so this quantity is going to be high if the summation is high it is a summation of some n terms now these terms cou ld be positive or negative if all the terms are positive what would we happen to the sum student refer time eightfifty it would be high if there are some terms which are negative it would be low now when would all the terms be positive whenever y is abo ve the mean z is also above the mean right therefore this quantity is positive this quantity is positive whenever both are below the mean again the product would be positive when one is above the mean the other is below the mean then there is somethin g wrong happening right and in that case you will have a negative term right so for more details of course you can refer your other textbooks and so on but this is just the intuition an important step here is to zero mean the data right we are computing the subtracting the mean of the data ok another way of saying this is that the column z is actually linearly dependent on y ok it is almost linearly dependent i of course have some noise twoone zeroseventysix and so on but it is largely linearly dependent i can get i can write z as some c times x fine ok so now can you tell me the refined goals that we have we are interesting the representing data using fewer dimensions such that remember that when i say fewer dimensions i mean a ne w set of dimensions right it is not throwing away dimensions from the current data we are looking for a new set of dimensions what are the conditions that we want from these new set of dimensions student refer time tentwelve one there should be high v ariance along these dimensions the new dimensions and student refer time tenfifteen the dependence are linearly independent or uncorrelated ok fine and even better of course if they are orthogonal why student refer time tentwentysix because we are look ing for a new dash student basis basis and the most convenient basis is student orthogonal basis orthogonal basis ok fine so now let us assume someone has given us this new basis ok and let us call this p one p two pm so  instead of this x y z and so on someone has given us this new basis eventually we will of course figure out how to find the basis but let us assume that someone has given this new basis right and they are both linearly independent and actually it is redundant actually so yeah this example of a redundant feature such an orthogonal vectors is sufficient they are linearly independent let p be an n cross n matrix such that p one p two p n are the columns of p right same thing as we had put the eigen vec tors in a column and probably i have unknowingly given out the solution but ok and let x one to xm be the m data points given to us ok so we are given this data as usual we have this x matrix each one of them belongs to rn and we have m such data points right that is the standard thing that we are operating and you always write this as a matrix ok and we have already done the data is zero mean and unit variance actually unit variance is not required but the data is zero mean fine that we will sorry i am goi ng to deal with covariance as a unit variance is not required so the data is zero mean is what i am going to assume but what if the data is nonzero mean i can always make it zero mean right so just to remember this is an important trick that you will alwa ys have to use whenever you are doing any large scale machine learning so whe never you are participating in k aggle competitions almost the first thing that you do is standardize the data that means make it zero mean and unit variance so why is that impo rtant student scaling right scaling issues would not be there right so if i have something in centimeters and some other unit in kilometers right now remember that always you are doing somewhere this linear operation w transpose x you might add a no n linearity on top of that but now if your xi dimensions some of them are in the range of zero to ten thousand some of them are in the range zero to ten right then there is some abnormality here right some dimensions are winnings in terms of their magnitude and some dimensions are losing out right that is why you always make it unit variance and you also make it zero mean you center the data ok so we will assume this and if we all understand if the data is already not zero mean and unit variance we can always m ake it zero m ean and unit variance  just scale it and make it center e d now we want to represent each xi right so xi is one of these data points that we had that means one of the rows of our matrix ok and you want to write it as a lin ear combination of this new basis so if you have any basis any vector you can write it as a linear combination of that basis  is it fine so far it is ok ok now for an orthogonal basis we know that we can compute these alphas just by taking a dot prod uct of the vector with the dimension ok and just re peating some of the things right so now let us see what this means for one of the dimensions this is my data point xi which i want to transform ok  for one of the dimensions i just had to take the dot product with that dimension and this will give me how many values one value that means the coordinate along p one i want to do it for all the n of them i can write it as this vector matrix multiplication right what is the dime nsion of this n cross one how many if you get that ok so this oh not many why student refer time thirteenfiftyfive one cross n fine that is fine yeah how many of you get this ok fine yeah so this will give me all the n alphas is th at clear for this data point so it will give me alpha i one to alpha i is it now i want to do this for the entire data right so i have done it for x one i also want it to be done for x two and all the way up to x m for each of these i would have such an ope ration where i have a vector multiplied by this matrix if i just stack all these vectors i get back my matrix x and the whole operation i can write as x into p ok is that clear to everyone ok what is the dimension of x into p student m cross n m cros s student n n right so for all the m data points i have alpha oneto alpha n is that clear anyone who does not understand this so x hat is the matrix of the transformed points is that clear i have now the new coordinates instead of the original coordina tes according to the coordinate axis i have the new coordinates in this matrix now i will just go through some very simple theorems or rather results and i will not prove them you can prove them on your own or other proof is there in the slides we can look at it later on right so if x is a matrix such that it columns have zero mean and if x hat is equal to xp then the columns of x hat will also have zero mean is this obvious to most of you not really is it how many of you thi nk it is obvious ok then let me just go over the proof so for any matrix a one transpose a right so that means you have this vector this is a vector or a matrix yeah this is a vector right so i have a vector of n one so one this is nothing but a vecto r of n ones so what is this product actually going to give me it will give me a vector containing n elements what is each element student sum of that column sum of that column right is this fine ok this is very obvious to see from if i have this sup pose i have two three one and three six seven ok and then of course the corresponding so if i do this multiplication i will get a two dimensional output which would be just seven and sixteen right so that is just the sum of that column student refe r time sixteenthirtythree so now we have this x hat that is the transform matrix now let us see if i do this operation i x hat what happen i can write it as this i can club it as this what is this it will be all zeros because the origina l matrix was mean zero that means the of the elements of all the columns each column independently was zero that what this is going to be a zero vector so zero vector multiplied by any matrix is going to be zero now is it obvious i hope this is obvious x tran spose x is a symmetric matrix i st ill have the proof for that now if x is a matrix whose columns are zero mean then a matrix sigma which i am going to call as a covariance matrix which is given by this is actually the covarian ce matrix how many of you agree with this how many of you have seen the covariance matrix before so all of you agree that this is the covariance matrix if you do not please raise your hands if you do not you will not understand the rest of the stuff n ow y ou have to be given the right incentives s o let us see be the covariance matrix of x now what is the covariance matrix actually first of all tell me that if i say that i have an n cross n matrix x l et me not make it any cross n let me make it m cross n ok what does the covariance matrix actually capture what is the dimension of the covariance matrix first of all student n cross n n cross n ok and what does each entry of the covariance matrix capture the covariance between the i th column and the j th column student refer time eighteennine so the entry ij of the covariance matrix captures the covariance between the i th column and the j th column is that fine now what is the formula for covariance suppose i give you two columns right let us see i have give you x one one x one two x one three and x two one x two two x two three can you give me a formula and of course i will go u p to k or rather m so what is the formula summation student refer time eighteenfiftytwo i equal to one to student m m student refer time eighteenfiftytwo mu one student refer time eighteenfiftythree mu two anything missing student by m by m anything else in the denominator no no is it fine ok so an what is mu one mu one is just an average of this ok so this is the covariance formu la now if the mu s are zero then what does this boil down to student refer time nineteenfourteen x one i into x two iwhat is this quantity actually student refer time nineteentwentytwo this is the dot product between the i th column and the j th column fine ok now tha t is pretty much the explanation right so now the c ij th entry is supposed to be given by this formula if the means are zero you are just left with this formula and this is nothing but the dot product between the i th row and the j th i mean the i th co lumn and the j th column is that fine and now if you write it as a matrix then you can just say that it is the ij th entry of the x transpose x matrix everyone gets this no one has any confusion the people who raised their hands fine good refer slide t ime twentynine so we now this is where the we are so far that we have assumed that someone has given us these dimensions p one to pn which we have put in the matrix p right and we have also made a case that x into p which is what i have written here actu ally is just a projection of the original data onto this new basis right everyone gets that ok and i am calling that new projection or the new result that i get as x hat so that is what my transform data is what is missing here student refer time twentyfortytwo we do not know what p is that i am assuming someone has given me that p now i need to figure out what is the p here now using the previous definition we get that this is the covariance matrix of the transform data  so let us just write that th is is fine this is fine what is this student refer time twentyonethree covariance matrix of the original data ok so i will just write it as sigma fine ok now each cell ij of the covariance matrix towards the covariance between columns i and j of x hat wh ere x hat is the transformed data what is the property that you want to hold i give you two conditions or i will give you only one condition for now when i is not equal to j student refer time twentyonetwentyeight zero ok so what should the covariance matrix look li ke student refer time twentyonethirtyseven remember that this is what is this this is the covariance matrix of the transformed data right that is what i started with right this is the covariance matrix of this transformed data what do i want this covariance mat rix to look like student diagonal matrix a diagonal matrix ok because i want every non diagonal element to be zero right and this point i am not telling you what i want the diagonal elements to be i am just telling you i do not want them to be zero well if it is zero what would that mean student refer time twentytwofive that is the variance right if you take the along a diagonal what you get is the variance it is if it is not clear right now well return back to that right now we just know that the off diagonal elements are the c ovariance between the i th and j t h column and we want that to be zero so we want this condition to hold this is something very new that you have never seen in this course before they have actually not seen in this course before have you seen this or not student refer time twentytwothree thank god fine so what is this student diagonalization the diagonalization of which matrix this matrix right and what was this matrix it was x transpose x this is clear so what is the solution all row s always lead to student eigenvectors eigenvectors right so we want p transpose sigma p to be a diagonal matrix and we know which are the set of vectors which i put in p such that they will diagonalize sigma student ei genvectors eigenvectors of student refer time twentythreesix x transpose x right ok wait why did i put this it is the matrix of the eigenvectors right so it is a matrix of the eigenvectors of x transpose x so now have we finished it do we know principal component analysis now so we started with the intuition that we wanted to transform the data ok i cannot stress enough that we want to transform the data not chopped off dimensions from the existing data ok that means we need to project the data to a new basis and we had a couple of conditions the variance should be high and the covariance should be zero we have satisfied one condition which is the covariance is zero and we arrive at a solution which says that the eigenvectors forms the basis that you sho uld project on so  that the covariance would be zero so we have a solution we know exactly which basis to use to represent the data ok so that the covariance condition is satisfied what about the variance did we do anything about the variance student  refer time twentyfourten so we will come back to that ok fine why is this a good basis what does the what is a good basis the best basis student orthogonal orthogonal right because the eigenvalues of x transpose x are line arly independent that ok and they are also orthogonal because x transpose x is a dash matrix student refer time twentyfourtwentyfour ok good real symmetric so this method is called the principle component analysis for transforming a data to a new basis and tha t where the dimensions are non r edundant because they have low covariance and not noisy because they have high variance the second part i have not proved right and i will get to that at some point fine no that is what we saw right no what is i did not get that now in practice how many eigenvectors would you have student n eigenvector n eigenvectors do you want to keep all of them which ones will you throw away student refer time twentyfourfiftyeight the low variance ok and now in the next interpretation ac tually we will try to see what is the what happens when you throw away the least important dimensions right what do you mean by the least important dimensions"}
{"audio_filepath": "Data/Preprocessed/Better activation functions_71.wav", "duration": 1644.0, "text": "let us start with b etter activation functions re fer slide time zeroseventeen so before i get into activation functions right let me first tell you why i care about activation functions w hy do i actually want to come up with better activation functions s o will start with the following question what mak es deep neural networks powerful among other things what is this one thing which makes it powerful so let me give you this intuition this i have a deep network ok and do not worry it is a thin network but i could have had a wide network also but just for illustration i have taking a deep network thin network now imagine that each of these neurons that you have if i replace the sigmoid in each layer by a simple linear transformation a by the way this is technically incor rect so orange is always input so this should not be a sigmoid there right either add one more layer there or let us change the figure s o suppose i replace all these s igmoids by linear transformations what would y be can you write y as a function of x what would it be g ive me the function will we just be this right so first we will do w one of x which is this right then will take w two of that then w three of that and w four of that so i could actually have written this just as y equal to w x where w is equal to w four w three w two w one so there is no depth here there is actually only one weight which i could have learned you get that right if you just have all linear transformations then essentially you do not have so many weights you just have one weigh t throughout you get that make sense so what you are learning eventually we will just y as a linear function of x and initially at some point we started off with such linear func tions right w transpose x in the case of perce ptron and mp neurons so what does that lead to what kind of decision boundaries does that lead to linear decision boundary so if you do not have these nonlinearities we cannot have these arbitrary decision boundaries will only be left with linear decis ion boundaries in particular will not be able to solve this problem that we had right w e were given some red and blue points and there was no way to draw a line such that the red points are separated from the blue points what we needed is some kind of circles or ellipses to separate the red points from the blue points that cannot be done with linear decision boundaries that can happen only if you use a deep neural network with non linear decision boundaries and we actually h ave a proof for that what that proof the universal approximation theorem actually towards r ight so that is why nonlinearities or the activation functions clear a very important role in the success of deep neural networks ri ght hence you want to examine them very closely and see what are the newer kinds of nonlinearities that have been proposed so we always start with the basics so will start with sigmoid see what are the problems with sigmoid and then see what we can do to solve some of these problems so this is what the sigmoid function looks like you have seen it a million times and it actually const rains the input to zero to one right so it takes some input and it constrains it two values be tween zero to one now since we are alway s interested in gradients right because the entire training and that is why i did that precursor in the first module the training always depends on gradients so it is always important to look at what does the gradie nt look like so we know what the gradient looks like we have computed this is just sigmoid o f x into one minus sigmoid of x  so now let us see what happens if you use such a sigmoid ne uron in a deep neural network this is a de ep neural network and without loss of generality i am going to use a thin deep network but the same holds for a deep for a wide deep network also so suppose you are interested in computing the gradient with respect to w two right at some point in the ch ain rule you will have this term how many of you are convinced about this ok a nd that will lead to this c ould that cause a problem so at some one of the terms in your chain rule is going to be this dou h three by dou a three i am assuming all of you are co nvinced about that and i have given you the exact formula for dou h three by dou a three will that lead to a problem student refer time fourthirtytwo good so what is the consequence of this to answer this we need to understand the concept of saturation right re fer slide time fourforty so a sigmoid neuron is said to have saturated if it is output is one or zero or rather close to one or close to zero ok what would happen in that case to the gradient student refer time fourfortyeight it will vanish right because sigmoid of x into one minus sigmoid of x so it either extremes is going to vanish and you do not even need the formula for that you can just see it from the diagram right because the gradient here is going to be zero that is obvious right it just a what horizontal line so this gradient would be zero so fine why does it bother us what is our entire training premise based on gradients right what does our update rule what happens if this guy is zero n o update e the weights just stay where they are right that me ans the training gets stalled right so think about this right i f all the neurons in your network have saturated that means all the weights the gradients will be zero that means all the weights will remain where they are you pass another input nothing is going to change right it still be zero so if this neurons have saturated your training will just stalled ok so that was one of the reasons which is to cause problem in training dee p neural networks earlier right so that is one of the reason why it was not converging because these weights used to these neurons is to saturate so this is one problem with sigmoid neurons a saturated sigmoid neuron c an cause the gradient to vanish but why woul d the neurons saturated i mean what would cause them to saturate ok this saturate find their gradients will vanish but why would they saturate we should be able to get some hints from the figure that has been drawn so this is actually that x needs to be changed so on the x axis we have x quite obviously but that has to be something else so what it is what is happening is what does the sigmoid neuron do it takes this aggregate it or someone just disappear refer time seventwentysix so is it very b oring today no right so you have this aggregated sum of the inputs once you have that aggregation you applied the sigmoid now tell me when would it saturated student refer time seventhirtynine when the aggregation is very large that means one of the two thi ngs could happen either the xs are very large or the ws are very large would the x is x is be large i see a lot of you saying no why student refer time sevenfortyfive good  we normalize them right we make sure they are between zero to one so we do not allow those arbitrary large values of p ressure density and so on right we make sure they are between zero to one so then the weights can be a problem right now why would the weights be lies move later first student refer time sevenfiftyeight if i initialize the wei ghts to a large value if i initialize all my weights in my infinite wisdom to a large value w hat would happen right from the first training example itself w i x i would take on a very large value and your neurons will star t saturating so imagine if al l the weights throughout my network are initialized to large values then right from training instance zero my neurons will start saturating and i will not be able to train anything how many of you experienced this while doing back propagation and the othe rs did not do the assignment they copied it please raise your hands how many of you experienced it now many more hands will be raised still now ok honest people that is a paradox consider what would happen if you use sigmoi d neurons a nd initialize the weights to a very high value they will start saturating and hence you will have this problem of vanishing gradients  o k everyone gets this so this is a problem at this sigmoid neurons the other problem with sigmoid neurons which is very interesting is that they are not zero centered what do i mean by that they are not zero center that is what it ok so sigmoid is are not zero centered what do i mean with that mean by that they are not zero centered th e value is between zero to one right so the average cannot be zero it is always going to be above zero ok sigmoid neurons are always going to take on positive values between zero to one so why is that a problem so that is an interesting explanation oh did i say th at did i put the acknowledgements somewhere so all of this material that i have been talking about it is taken from andrej karpathys lecture notes so here is this interesting explanation for this so now consider this particular network ok and i am go ing to focus only on this part that means the output layer and just the layer before that and the layer before that has these two weights wone and w two i am going to focus on that so to update these weights i need to compute so what do we need to comput e gradient ok now you will answer so we need to compute the gradient with respect to wone and w two and this is what it is going to look like what is the red part and blue part why red and blue the red part is dash for both common for both r ight so th is is going to be common i do not know why i did that ok refer time ninethirtynine so this red part is common for both and what is the blue part actually what is dou a three by dou w one h two one and dou a three by dou w two so dou a three by dou w one is just h two one and dou a three by dou w two is just h two two ok so let me just plug in those values and note that h two one and h three are between zero to one so can you make some interesting commentary on this interesting but useful not just philosophical stuff that these two derivatives are for t he weights at a given layer i have just taken two weights but i could have taken n weights and the same thing would have hold because i know that the derivative is proportional to the input that it gets and the rest of the part is going to be constant b ec ause that is coming from the chain rule up to the previous layer right so now what is happening because of that just to make fun of you guys i mean if you get that sorry good yeah it is not very straightforward but let us see so if the first comm on term in red is positive right then what would happen to these two guys they would both be positive right because h two one and h two two are positive now the first common term in red is negative then what would happen to these two guys both negative so th at means the gradients of the weights at a particular layer where either all be positive or they will all be negative you get that that is because of this common part and the blue part the blue part we know is positive so what matters is the common p art and that common part can either be positive or negative for all of them together right t hat means for a given layer all the gradients at a layer are either positive or they are all negative so let us see what is the implic ation of that r ight refe r slide time twelvefourteen so this actually restricts the possible update directions so which is the quadrant which has all positive first ok sorry for embarrassing yeah and all the negative is the third quadrant that means your movements can only happ en in the first quadrant and the third quadrant so do you see a problem with this right so you are going to actually try to move that your theta which is a collection of w one and w two is theta minus eta into the gradient right and you know that this v ector which is the gradient vector can either be positive that means can lie in the first quadrant or it can lie in the third quadrant these movements are not possible that means there are certain turns or certain movements or certain directions that i am not allowed to take so what would this mean it would take a dash time to converge student l onger time longer time to converge right b ecaus e i am restricting my movement s o imagine you have to go from destination to destination b and i say that you can never take a right turn right and there is some going to be some problem it will take longer to reach there unless the directions are to our left right unless your destination is refer time thirteentwentytwo but that will not happen refer slide time thirteentwentyfive so suppose this is the optimal w star and we start with some random initialization because that is why we are going to start then the only way i can reach it is i may by making a series of this kind of movements ri ght as the exact pattern is what will have to take because these are the only movements which are allowed or some movements which are allowed a nd it will lead to a certain cryptic pattern and i will not be able to have the complete freedom of moving in the direction which would have directly taken me to the optimal so that is a problem with something not being zero center  and lastly sigmoids are expensive to compute because you have to do this exp right it is not something as easier as something else that we will see in the lecture today ok so these are some problems with sigmoid functions student refer time fourteenfifteen so this is some issues that were they with sigmoid functions so this pointed that ok maybe we should try better activation fun ctions that is why tanh become very popular but tanh is not something which happened post two thousand and six right so this was like ninetytwo or ninetythree when i think yan lacunae had started moving to tanh from sigmoid functions right now again here other inputs are compressed between minus point to one ok where inputs are now zero centered which takes care of this problem which i mentioned at the end that these directions of movements are constrained and was the derivative of this function one minus tanh square right what happens at saturation even without looking at the formula the g radient would vanish to zero right so the vanishing gradient problem is still there what you have solved is a problem of zero centering and that itself used to give better re sults than just using a sigmoid function but it is still computationally expensive because you still have to do these e raise two components right the you still have to compute the se exponential powers so it is still computationally expensive refer sl ide time fifteenseventeen so then in around two thousand and twelve i guess is when this relu was introduced in the context of conv olutional neural networks right and this is what the relu function actually looks like is this a non linear function i t just looks like a line rig ht why is it a non linear function it is a non linear function right because x is you cannot write x the output as a function of i mean a s a linear transformation right so you have this zero in fact if you take two relu functions smartly refer slide time fifteenfifty you can actually get the sigmoid i mean you can get an approximate for the sigmoid function so you c an go back and check this right so if you take these two functions and subtract one from the other w hat is this this is a relu function this is also a relu function right so i define relu as max of zero comma x so both of these are relu functions some variant of that and now if you subtract one from the other you will actually get a approximation of the sigmoid function right and this cannot happen if you have two linear functions take any two linear functions you will not be able to get this kind of an approximation so relu is a non linear function what are the advantages of relu one is it does not satur a te in the positive region right it is computationally very efficient the output is either zero or x there is no powers nothing like that right and it practice it converges much faster than sigmoid and tanh so that is what this two thousand and twelve paper show and now relu has actually become more or less the standard in all convolutional neural networks but there is still a caveat while using relu ok so the derivative of relu we can see that if x is less than zero then the derivative is go ing to be zero right  a nd if x is greater than zero then the derivative is going to be one and that straight away follows from the definition of relu which is zero or x so when it is zero the derivative will be zero and when it is x the derivative will be one so now co nsider this given network and let us assume and this is not a very far faced assumed assumption it can happen in practice that at some point a large gradient causes the bias b to be updated to a large negative value so what i am saying is that somethin g happens and b gets updated to a large negative value now what would happen to this quantity remember this quantity which i have circled is actually the input to the blue colored relu neuron that i have so i am asking you w hat would happen to that input that input would become negative so the neuron would output zero and i am calling it a dead neuron why if the input is zero i mean is a input is negative then the relu functions output would be zero what would happen to the g radients during back propagation zero that means what would happen to the weights student refer time eighteensix would not be updated right now but that is fine right if you give some other input this will recover why am i calling a dead means permanent right unless you are in some fantas y world but dead is dead right so why am i saying that it is dead i could might as well i would give it a next input and then probably things would be ok bias is still very negative because nothing is getting updat ed right or bias is still very negative you know that x one and x two are constrained because you have normalized them right and w one and w two have not been updated so still the situation does not change so what happens is that once a relu neuron dies b eca use somewhere in the chain rule you got a zero it will stay dead forever ok it will never be able to come out of that it will always produce a negative output that means that output will be clamped to zero that means no gradients will flow back and that means all the weights will not get updated connected that neuron so in practice when you train a network with relu you will observe that a large fraction of the units can die i f the learning rate is set too high why this i f condition student refer time nineteenthirty what was the assumption that i made t hat the bias receives a large negative update and that is possible if your learning rate is very high because you got some small negative gradient but your learning rate b lew it up now what is the practical implication of this if a training a network and a large number of your relu neurons have died what does it mean most parts of your network are dash useless they are not learning any feature nothing right is all zero that means you have this large number of parameters versus getting wasted because they feed into a relu you function and the relu function just keeps outputting zero so if you have n neurons in the particular layer and most of them are zero that means y ou are not really learning an n dimensional feature representation you are just learning a much smaller feature representation right so can you give me a simple way of one simple way of avoiding this among many other ways student refer time twentyoneseven no dropout is statistical right it is probabilistic this is like always dead one thing is to update the weight to a large to a positive value and zeroone mind you is a large positive value right later on we will see y but zeroone is reasonably large ok so were going to initialize the bias to a positive value so that even if this large negative gradient flows through there is still a chance that it will not become very negative and hence it will not mess up the things the way it does that is one solutio n to that right but still you will find that even after that the relu neuron a lot of those can die but still in practice they work better for a deep convolutional neural network ok and we can also use other variants of relu so there have been to avoid this dead neuron problem there are other variants of relu which have been proposed and that is what we look at next so there is something known as a leaky relu i s it obvious from the equation what it does r ight so inst ead of producing zero it will just produce a very small value proportional to the input now what would happen to the gradients they will not saturate right will have the gradient would be if the input is negative what would the gradient be student ref er time twentyonefiftysix zeroone right so that means some gradient will still flow through how many if you get this right so that means if you use a leaky relu neuron some gradient would still flows through so just understand this trend right that ah and this is i mean all this stuff is simple there is nothing great in this but just put it in context right so in two thousand and six to two thousand and nine people realized ok now we can trained networks and maybe whatever we have done with unsupervised pre training actually corresponds to better initializations or better optimizations or better activations and so on so now let us try doing research in that so that led to the discovery of relu now people started observing problems with relu and then proposed a variant of it which is l eaky relu right so that is how this area has now become very prolific and grow right so we started off with this seed idea that it is possible to train these deep neural networks and now we are trying to make arrive at better and better ways of doing it making it more and more easier to train them and take care of some of these irregularities which existed earlier so one of them being sigmoid not being a very neat function to optimize with right so that is what all this is about individually all of these are probably easy for you to understand once you go back and look at the slides you all this is nothing great in this but what i want you to really understand is this bigger picture of what is happening here as long as you get that time frame w ith and of course leaky relu is again computationally very efficient there is no exponents no squares nothing like that and it is close to zero centered and it is still not zero centered but close to zero centers because you have outputs on both side and then someone came up with a generalization of this which is parametric relu so y zeroone make it alpha x and alpha will also be a student p arameter parameter it is a trainable parameter it is not a hyper parameter ok how many of you know the difference b etween parameter and hyper parameter ok you have used this in the back propagation as i am right so it is a trainable hyper parameter it will get optimized along with your other parameters in the network so then someone sa id leaky relu fine parametric relu is fine let us try to do exponential relu ok so it has all the benefits of relu it ensures that at least a small gradient will flow through even when your inputs are negative that means it avoids this dead neuron pr oblem again close to zero centered outputs but it is expensive because now we have added this exponential right so these are all ideas which came out during this period and all of them were shown to work better than the other and so on and of course at t he end i have to tell you a final conclusion right whenever i give you so many possibilities so i have given you sigmoid tanh relu parametric leaky exponential now what do you use right this the idea is not to confuse you but to give you one s olution which would largely work yeah what regularization student refer time twentyfourfortyfive yeah you could have done yeah that refer time twentyfourfortyeight there is exactly so a lot of this research right which has happened in this period it is not a lot of it is juristic right you solve one problem with relu ok the neurons and saturated ok just make it something which does not saturated so that is there it is possible that the other solutions would also go there is not that thi s is the only solution which works now then someone came out with max out neuron which is a generalization of relu and leaky relu why do i say it is a generalization what was relu that means w one equal to b one equal to zero w two equal to one so it is a spe cial case of the max out neuron what about leaky relu this was parametric value but again what about so now what is happening w one equal to alpha b one equal to zero w two equal to one b two equal to zero so you see how it generalizes r ight so this is how these variants keep kept coming up now the problem of course is doubles the number of parameters right b ecause you earlier had only w transpose x plus b now you have w one transpose b one w two transpose b two and so on right so it is actually doubling the number of parameters that you have so now coming to the final conclusion of all of this right w hat you need to remember is that sigmoids are bad so no one uses sigmoids in convolutional neural networ ks they still use somewhere i am i am sorry about this relu is more or less the standard unit for convolutional neural networks so any standard cnn that you will pick up it will use relu as the activation function if you want you can explore leaky re lu max out elu and so on but it will require a lot of careful tuning say if you want to use something out of the bulk box relu is just fine relu just works fine i n practice despite all this dead neuron and other problems student refer time twentyseventwenty y eah so then the argument for that is that how often when you reach the point x equal to zero r ight so the chance of that having is happening is very very low and if you get there you can always approximate it by some epsilon or something and for that tra ining instance just go on right any ways you are making so many approximations with stochastic and mini batch and so on so this is one more approximation that is how people typically deal with it but in most cases it will not come in that point app earing is very low but the question is valid and tanh sigmoids are still used in lstms and rnn s which you will see at some later point in the course ok so there are a couple of more modules that i need to do so we just take a break here"}
{"audio_filepath": "Data/Preprocessed/Error and Error Surfaces_13.wav", "duration": 216.0, "text": "before we go to the next section which is on learningi just want to introduce the concept of errors and error surfacesand tell youwhat it relatesto these multiple solutions that we were talking about so for simplicity what we will do is we will just set the threshold to minus or minus w to one which is setting the threshold to minus one and now i will try different values of wone and wtwo ok so i was saying that there are multiple values of wone and wtwo possible and these are all real numbers we are not constrained by having them as boolean values so now this is one solution which i tried i tried setting wone to minus one and wtwo to minus one what is wrong with this linedoes it lead to any errorshow many  just one error so this makes an error of one out of the four inputs now let me just try some other values of wone and wtwo this line again one error what about this linenot four three because zero zero is anyways on this side of a line so now given this now tell me that i my quest is to find these w so i would want to find wone wtwo and so on given this discussion on errors can you tell me a condition that i am looking for i want to find w one wtwo or up to wn such that e rrors are minimized and in the best case errors are zero so that is what i want so this just i want to make a case that these search for ws is driven by certain objective and this objective is to minimize the error so now since we are doing this let us plot the error surface corresponding to different values of w naught wone and wtwo once again for simpler analysis we will just keep w naught to be fixed a t minus one and now what i have so just do not read this bullet as of now even this one so i have this wtwo here so that is my one axis and i have wone here which is my another axis n ow what i am going to do is i am going to try different values of wone and wtwo so this axis can go from minus infinity to plus infinity o f course for showing the sake of showing here i have just had it from minus four to four so now what i am going to do is i am searching for some values of ws wone and wtwo so that my errors is zero and let us do a brute force and i will just try every value between minus four to four ok  in fact one of the solutions which i proposed actually was this oneone oneone right t hat is the line which we saw on the previous slide and which led to zero errors and that is the dark blue surface here so how did i compute this error actually i just substituted minus sorry oneone oneone here and then i put in all the four values combinations for xone x two and i realized that i am able to satisfy all of them so i do not get any error n ow instead of that if i had put something different so let me just go back to the previous slide w hich was see minus one minus one which is i think yeah somewhere around here right minus one minus one i guess so for that i am in this light blue region where the error was one i make errors for one of the inputs so it is a very brute force way of finding this and this is not going to work because we have lots of inputs to check but this is just to give you an intuition that we are looking at errors and we are trying to find a value of wone wtwo which minimize this error so that is the idea behind errors and error surfaces"}
{"audio_filepath": "Data/Preprocessed/Backpropagation: Computing Gradients w.r.t. Parameters_29.wav", "duration": 720.0, "text": "before we move on to the next module  a quick summary of what we have done so far so we introduced feed forward neural networks and we wanted to learn the parameters right from the last layer to the first layer and we figured out that what we can do is that we can just us e the gradient descent algorithm as it is except that we have this small problem that we have so many parameters now and located at differ different points in the network right some at the initial layer some at the final year and you want to compute t he derivatives or the partial derivatives with respect to all of these if you can do that put them all in this large matrix then we can jus t use gradient descent as it is s o that is what we figured out and then we wanted to find out the gradients wit h respect to or the partial derivatives with respect to all these parameters so then we realize that this can be done using chain rule because there is a path from your output which is the loss function to any of these weights so we just need to follow that path and apply this smart this chain rule smartly and just sum up the derivatives across all the pa ths that lead to that weight  so in that process we started from the output layer we just treated it a bit special because the output function is sp ecial and this is the last layer so we just first computed the gradient with respect to the output layers then we figured out how to compute the gradients with respect to any of the hidden layers and now if you are at a particular hidden layer now the weights that feed into this layer we could or we have not reached there so now the next thing that we need to do is that we have computed the gradients with respect to any of these hidden layers and now we want to find the gradients with respect to the parameters which is the weights and the biases so it is the do you all remember this or it is all long history or the story is back right fine so now we are at the last point which is computing gradients with respect to parameters refer slide time  twofour so again this is the overall picture we were in this chain rule and we have come all the way to the last point where we are ready to now compute these quantities so now start by recalling that a k is equal to b k plus w k h k minus one right this is our activation formula pre activation formula right so i am talking about these light blue guys ok which is clear in image and now i what have i done so far i have been able to come up with a formula to write th e gradient of the loss function with respect to any of these light green guys right that is what where we ended last time right w here we are able to compute the gradients with respect to the sorry light blue guys ok  and now i want to compute the gradi ent with respect to any of these parameters or any of these parameters so any parameter it does not matter am at some i th activation layer pre activation layer and i just want to compute the gradients with respect to the weight s which feed into this layer  and that is what we are interested in so we are just taking any layer k and you want to find the gradient with r espect to the weights there now can you tell me so can you tell me what is what is the thing that am going to do here or what is th e recipe that we have been following i need to move what is the recipe that we have been following apart from yelling at people who come late we find the element wise partial derivatives first and then put them all together to get the gradient ok what is the element here what is what am i looking for right now i want to compute this fill this blank what goes here student w w any of these w is right and in particular say w k that is what i am looking for so what is the first thing that i am goin g to attack student wkij good w k i j and once i have this for one of these guys i just know a generic formula with respect to i j and k and i can just put it into a gradient vector ok is that fine ok so now can you ok now from here to here if i wa nt to reach from here to here so this is what i am interested in right now how is the chain rule going to look on look like based on whatever you have already seen till where have you already reached you already know this quantity right now if i w ant this how am i going to write it student refer time fourfiftynine i will find up to the light blue guys which is this i already know how to compute it and then from the light blue guys i will go to the this is fine right so this is the quantity that i am looking for ok now what is one element of this guy dou a k by is it fine ok what is the dimension of this actually is it a scalar a vector a matrix matrix or a tensor what is the tensor what is it is it a matrix what are the dimensions w hat does this derivative mean or this gradient mean i change one element of w k how much does one element of a k change how many elements are there in ak n  how many elements are there in w k n cross n so how many partial derivatives which i have n cross n cross n what is this student tensor a tensor right so this is going to be a tensor ok so when i say one element of this i mean this ok so this is one element of this gradient ok now can you tell me the formula for this what is this quantity hk minus student one refer time sixtwentyseven h k minus one or hk minus one j or student refer time sixthirtyone everyone gets this hk minus one i how many of you get this so let us do it right so you have akone aktwo akthree that is your ak vector ok you have bkone bktwo bkthree plus wkone one yeah i know again this is one of those silly things but if everyone does not raise their hands and compelled to do this so  h k minus one one hk minus one two hk minus one three ok so let us take one of the se guys right so a k one can you tell me the formula for that student refer time seventhirty plus first row ok one two this one three now can you tell me this quantity so what is i here one ok so i want this by w k i j right so i is one so i can take any of the j so let me take j equal to two so what is it going to be this will go off this is constant this is constant only this term remains and the derivative is hk minus one two which is j right so that is what the formula says so i have a formula for one o f these guys ok and that is a generic formula so always remember if you cannot figure out what it is just write it down in scalar terms just add up all the terms and you will get the formula right so now this is what the chain rule i s going to be refer slide time eightthirtyseven so this is what it is going to be this is one element of that tensor this is how that entire thing is going to look i have just flattened it out and put it here now let us take a simple examp le of wk belonging to r cross  three cross three everyone is fine so far right or anyone who everyone is fine please raise your hands i mean fine i mean not in life but with the lecture fine  so this is what it looks like right for a three cross three matrix now let us se e we already found out that this guy is equal to hk minus one comma j right so this is what this matrix looks like nothing rocket science here right so each of these quantities is actually can be written in this form where i appropriately substitute i k and j and i know that this quantity can be further written as this quantity right that this is our clear right so i have written it as this now can you simplify this i do use a lot of this ok can you simplify it is it look s similar to something that you did on the assignment does this look like matrix which has some very regular patterns yeah i can see someone doing this and this everyone gets it so let us see so this the first column the second term in the produ ct is all same throughout all the rows right what i mean is all these guys are similar same thing happens in the second row the third row right ah that is sorry the second column and the third column what about the rows these are all equal right so what does this look like actually the outer product of two vectors everyone gets this raise your hands ok good so i do not need to do an example so it is fine right this is an outer product of these two vectors one happens to the quantity to be the q uantity that we already knew right and the other happens to be a quantity that we can figure out i mean we already know this what is we know how to compute the hidden representations right the hks we can compute so fine so finally we come to the biases this is what one entry looks like this is exactly the sum which i had written out now i take the derivative with respect to b k i of the loss function so i could write it into as this chain rule where the first qua ntity is something i already know i have computed the gradient with respect to the pre activation layers what about the second quantity anonymous roar is what i was expecting student one one ok fine we can now write the gradient with respect to the bias what would it be what is this what is this it is just the gradient with respect to the pre activation layer right simple so now we are done with all the gradients that we were interes ted in"}
{"audio_filepath": "Data/Preprocessed/Convolutional Neural Networks (Contd.)_89.wav", "duration": 1014.0, "text": "so we will start fro m where we left yesterday so this is what we had seen yesterday we saw whats the difference between a convolutional neural network and a feed forward neural network and we focused on two main properties one is sparse connectivity and the other was wei ght sharing that is about it and then we saw that this representation of i mean we saw this diagram about how to you could have multiple kernels and each kernel would apply across the entire image and the weights would be shared for that kernel refer slide time zerofortythree so far we have only focused on the convolution operation and even when you have seen the neural network or the convolutional neural network we have only seen the convolution layersright so there is something more in a typical co nvolutional neural network and that is what i was about to start yesterday so we just continue from there so this is what a full convolutional neural network looks like so ignore these things for now all these parameters etcetera as a just ignore for them i will just walk you through what is important in this diagram right so your input is an image and the tasks that you are dealing with here is digit recognition or handwritten digit recognition right and what you see here is that you have taken an input which is a two dimensional input and then the next layer you actually see one two three four five six outputs so what does that mean student refer time onethirtythree you have use six filters apply them throughout the filter thro ughout the image each filter gave you one feature map and so in this layer you have six such feature maps so the original two dimensional input has now become six two dimensional outputs ok after that there is something known as a pooling layer we will see w hat a pooling layer is in detail now what i want you to understand is lets assume that what the pooling layer does is it does some kind of a shrinking it takes the original output and shrinks it how it shrinks it we will see in a while but let us s ee what happens after that so now you have one two three four five six as your input so now you have this is your input this volume is now your input i call it a volume because it has depth height and width and now you are again going to apply convolutions to that and what do you see how many outputs do we have now we have sixteen outputsright so what does that mean student refer time twotwentyseven i took this threed input applied sixteen threed filters on it each threed filter give me one feature map one twod feature map why one twod feature map student refer time twothirtynine because we are doing a twod convolution we are taking a threed filter but we are doing a twod convolutionright and then after that again this becomes my input and then do a max pooling on top of that and then so mething else happens after that so there as which we will come to later so right now i just want to say that there is this input then you come out will come with some output after applying convolutions now this becomes the input for your next stage where you have done pooling now the output of pooling becomes the input for the next stage so you will take this as the input apply some convolution on that get some output and continue in this way right and we will come back to what these are refe r slide time threenineteen so now let us dive deeper into what is pooling what does the pooling layer actually do so here is your input again it is a volume and now when i say input you should not just think of the input as t his remember that all of these can be inputs right so now at every stage once you have got an output for the next stage that becomes input that is typically how it was even in the feed forward neural network right once you compute a hidden represent ation that is the input to the next layer so i have some input at one of these layers either the input layer or any of the intermediate layers and i apply a filter on that and that filter gives me some twod output it gives me one feature map and let us sa y this is what the feature map looks like now what does the pooling operation do so i would apply two  two pooling with a stride off two so let us see what that will do that means i look at this two  two region i will pick up the max value from there that is why this is max pooling ok so the max value is eight i will just keep that then i am going to do a stride of two that means i am not going to place it on this block i am just going to shift to the next block again i will take the max from there which is four right and i continue this and i get this is the output so you see why the shrinkage happens because i am taking a two  two area and i am shrinking it by a picking up o nly one value from there right so it actually kind of half the width and half the breadth so total of one by four reduction is what you get but you could also use a filter with stride off one so this is what it would look like so you will place it here take the max value then the max value then the max value and so on right so in that case you will get a lesser reduction and instead of max pooling you could also do average pooling that means instead of taking the max of these four you could take the average of these four is it fine so is the pooling operation clear and how it results in the reduction of the size of your input ok so now what we will do is now that we have some idea of what a full convolutional neural ne twork looks like so it looks like alternating convolution and max pooling operations we know what a convolution operation looks like in particular we know that a threed filter applied to a threed input results in a twod output because we are not applying the co nvolution along the depth we just applying the convolution along the width and the height right so that is what we know so far and based on this knowledge now we are going to see some success stories of convolutional neural network right so we will start with the first one which is lenet five so this was already the fifth version this was around ninetyseven or ninetyeight or something and i had mentioned this when we were doing the history lecture so this is the input now you have dec ided to apply six filters you have said that the stride is going to be one that means you are going to place at every location the spatial extent is going to be five and the padding is going to be zeronow the question that i have for you is how many paramet ers does this convolution layer have what are the parameters here student the weights the student refer time sixtwentyfive the filters the weights in the filters how many filters do have student six six how many ways does each filter have student twentyfive twenty student five twentyfive right five cross five is a twentyfive so the total number of parameters is one hundred and fifty now i want you to appreciate something here so the input was actually thirtytwo cross thirtytwo which i believe one thousand and twentyfour and the output was twentyeight cross twentyeight right that is what the outpu t you got that is i guess seven hundred and eightyfour yeah so in a feed forward neural network if you had x belonging to r one thousand and twentyfour and your h belonging to r seven hundred and eightyfour  how many parameters would you need one thousand and twentyfour  seven hundred and eightyfour how many parameters do you have here stude nt one hundred and fifty one hundred and fifty much much smaller right this is because of student refer time seventwentyone both sparse connectivity as well as weight sharing right so now you appreciate the difference between the twook and thats one of the reasons that even before this i s for example from ninetyseven ninetyeight one thousand nine hundred and ninetyseven one thousand nine hundred and ninetyeight right so even before the deep learning wave of two thousand and six or the revival after two thousand and six right convolutional neural networks must still being trained for even deep networks four to five layers because they had much fewer parameter s and that is why it was relatively easier to train them as compared to a very dense feed forward neural network so i want you to appreciate that fact now after this i have the pooling layer when i am going to use a stride of one and f equal to two that means i am going to pick up ok so now i am going to use a max pooling layer where i am decided to use a stride of one and the max pooling will happen in the region of two  two and again k six because th ere are just six filters so max pooling happens on every feature map independently it does not happen across the depth that means i am not going to pick the max along these six layers i am going to pick the max along each of these feature maps so it is a per feature map operation ok and this since it is a two  two filter it will result in a size reduction and from twentyeight  twentyeight i will get to fourteen  fourteen how many parameters is the max pooling layer have student refer time eightfortytwo wow good there is no parameters in the max pooling layer because you are not having any weight matrices just taking the input and applying a simple max operation on that there is no w transpose x or any kind of a transformation happening there now this becomes your input whats the size of this volume student thirtytwo fourteen cross fourteen cross student six six so all the filters that i am going to use from now on what is the depth of those filters going to be s tudent refer time nineten what is the depth going to be student six i want to everyone to answer student six six right because we are always going to assume that the depth is equal depth of the filter is equal to the depth of the input refer slide time ninenineteen now here they decided to use sixteen filters and by the way you did hopefully notice that this twentyeight how did you get it from which formula student w n minus refer time ninethirtyfour wn minus f plus twop plus one right so that is the formula so now we have a fourteen cross fourteen input and you have sixteen filters so what is the depth of the output volume going to be student refer time ninefortysix what is the depth of the output volume going to be student sixteen sixteen right and a ok so you have sixteen and you ha ve a spatial extent of five  five just a minute spatial extent of five  five how many parameters does this layer have i want everyone to say it student four hundred four hundred ok fine so that i s student refer time tenten there are sixteen of these each is five  five student refer time tenthirteen oh into six into d ok ok good then we made a mistake here also no there the depth was one are you do you get it how you got two thousand four hundred ri ght we forgot about the depth so each of these filters is five  five  depth right what is the depth six the same as the input right so each of these filters is twentyfive  six which is one hundred and fifty  sixteen yeah fine is that ok did i confuse you or everyone back on track pooling layer edge should have been two because of that half reduction using student refer time tenfortynine yeah maybe can we just check this i think it should be two ok yeah there was a question student refer time elevenzero student refer time eleventwo go from the pooling layer to the next layer from here to the next layer ok so now what is the depth of your input volume six and what is the width and height fourteen cross fourteen so now every filter that i going to apply at the next layer is going to have a depth of six so i have decided to apply sixteen such filters so what is the depth of this layer going to be the depth of the output is equal to the numb er of filters so the depth is going to be sixteen and all my filters are five  five but what we forgot is that when i say the filter is five  five it is actually five  five  six because the depth is also there right so it is width into height into depth so that is this number of parameters in each of my filters right and that is one hundred and fifty and i have sixteen such filters so that gives me a total of two thousand four hundred parameters is it fine  ok ok so now we have a volume of size sixteen cross ten cross ten now i am going to do max pooling on that maybe again this should be two there was the same doubt you had fine so it will result in a reduction in the output and now what is the volume what is the size of this volume five  five  sixteen and the parameters is zero max pooling layer does not have any pooling now after this what we have is something known as the fully connected layer ok so now as i said the size of this volume is sixteen  five  five its arranged in these feature maps but i can always flatten it to get one single vector do you get that so from these sixteen feature maps each of five  five size i can flatten it out and get the single vector of size four hundred do you get that ok so thats what i do in the fully connected layer so now i am going to flatten this treated as a single vector and then fully connect it to the ne xt layer what do i mean by fully connected dense connections no more sparse connection so now we have a feed forward network from this point of view so you have four hundred and that connects to a layer of size one hundred and twenty so what are the number of parameters stu dent four hundred into one hundred and twenty four hundred into one hundred and twenty plus we will have one hundred and twenty biases so that is what this number is fine so this is one fully connected layer of size one hundred and twenty after that i have another fully connected layer of size eightyfour so the numbe r of parameters would be one hundred and twenty into eightyfour plus eightyfour and after that i have the output layer so the output layer this was twentysix or whats the output student refer time thirteenthirtyone oh but the this is digit this is alphabet recognition right ok so probably they ha ve done the computation using ten but it should have been using twentysix as the output layer because you want to predict one of the twentysix alphabets so you can assume this is twentysix so it would be eightyfour  twentysix twentysix right that is the size of the output layer right now do you observe something immediately is a something very striking immediately in terms of the number of parameters student refer time fourteenzero the fully connected layers clearly dominated right here we were dealing of orde r two thousand four hundred and max and here we just start with fortyeight thousand itself right so just keep this in mind that the fully connected layers have the largest number of parameters that you have and we will try to come back to this and see if we can solve this problem ok s o now when you see a convolutional neural network you should be able to reason about the following things at each layer what is the size of the input volume what is the size of the output volume what are the number of filters being used and what are the number of parameters in that layer right if you can reason these things and you have really understood what is actually happened and unless you can reason these things i do not see how you can efficiently code it up right so you should be able t o know that this is the size of the input this is the size of the output and so on and i guess all of us are comfortable with others so now how do we train a convolutional neural network what is the answer your nodding t hat means you do not know or you know it is too trivial to even ask this question how will you train it student refer time fifteenzero but how do you back propagate through a convolution operation that is a very nasty looking operation refer slide ti me fifteenseven a cnn can be implemented as a feed forward neural network with what being the difference student refer time fifteenfifteen only some of these weights would be active all the gray weights will not exist only the colored weights will exist right now can you back propagate to this network have you seen something similar before student dropout dropout right so if you could do that you can do this also so now if you take this view of a convolutional neural network where so this is just to giv e you an intuition or make you feel confident that once you know the backpropagation algorithm there is no nothing much different from training a convolutional neural network operation ok so everyone is fine with that how many of you agree with that that you can actually train using the same algorithm with some smart coding required to make sure that these weights are not active and so on but in practice of course you will not do this in practice you will define the convolution operation you wil l also define the derivative of the convolution operation and then use that right because that would be much more efficient this is very inefficient right because you are assuming that there is a fully connected network and then some things do not exist there so that is not thats the whole point i mean you wanted to avoid such dense connections right but in principle you could have just used this and trained the convolutional neural network in practice you do not need to worry because you just ne ed to define your forward convolution operations and people like google and all who release tensor flow torch and all will do the hard work of doing the back propagation for you right so you never have to write back propagation in your life apart from what you have already written in the assignment right that is why i make you go through that torture once so now afterwards afterwards whenever you use any of these platforms or pytorch or torch or tensor flow the back propagation comes for free you ju st need to write the forward progression that means you just need to write convolution operations and you dont need to worry about how the derivatives will be computed but i what i want you to understand is that conceptually it is the same you can sti ll use or in fact you still use the same back propagation algorithm to train a convolutional neural network also everyone is fine with this pk ok so now we know how to trained a convolutional neural network also"}
{"audio_filepath": "Data/Preprocessed/True error and Model complexity_60.wav", "duration": 483.0, "text": "so now we will t ry to see tha t how does this true error tha t we see depe nd on the model complexity so using s teins le mma and some t rickery we can show t he following w hat is s t e ins l emma so i had t his deal with my students last year yo u do no t ask me what s teins l emma is i will not ask you w hat st eins l emma is ok  so it i s some lemma which t ells us t hat t his quantity what wa s t his quantity t he last t erm which was t roublesome rate t hat covariance t erm which was t roublesome  that i s t his quantity  t his quantity is actually equal t o t his qua ntity so let u s buy t hat  let u s all of us agree t hat s teins l emma is correct and it t ells us t hat t his is t he case ok and you saw t he quiz one paper ok fine f rom last year i mean ok  so now we will work with t his premise and we will see what it actua lly t ells us  n ow when will t his quantity be high so what t his is t elling us  i mean jokes apart let u s t ry t o focus again t hat t his quantity is actually equal t o t he summation of t his quantity now let u s t ake one t erm in t his summation when would do u f hat x i by dou y i be large  w hat does it actually t ell you if i change one of t hese y i s a bit when t he prediction for it is going t o change by a lot  d o you get t hat  h ow many of you get t his s ome of you do no t get t his  j ust t hink about it  w hen w ould t his be high w hat does t he derivative capture  i f t he derivative is high that means a small change in t he denominator is going t o lead t o a large change in t he numerator wh at i s t he denominator  a ctually t he t rue y t hat we have observed  w hat i s t he numerator  that is t he predicted y  so what you are saying is t hat if t here is a small change in y i t hen t here is going t o be a large change in t he prediction ok  w hen would t his happen w ould t his happen for simple models or complex models co mple x models how many of you say complex models so t his is t he link t o model complexity rate and i will make a more intuitive case for th is  but at least some of you get t his t hat if your model is very complex that means it i s even one of your data points changes and t he prediction of t he model is going t o change largely s o now relate t his back t o t hat sinusoidal model t hat we had and we had t his complex model every model t hat i was t raining which was strained on a different set of twentyfive examples  t he mode l was vastly different and that i s exactly what was happening  w hen you were changing even one data point your predictions were changing largely  t hat means your model was changing largely  d o you get t hat intuition so indeed a complex model will be mo re sensitive t o t he changes in t he observation whereas a simple model will be less sensitive t o it and hence we can say t hat t he t rue error is actually equal t o t he empi rical t rain error plus something which relates t o t he m odel complexity refer slide time threeseventeen now let us first verify t hat indeed a complex model is more sensitive t o minor changes in t he data  so t his is some data t hat i had sampled from t he same distribution and i t rained one simple model which is t he green line which you see t hat was a linear model and i t rained one complex model which was a twentyfive degree polynomial which you see ok n ow what i am going t o do is  i am going t o t ake one of t hese points and change it a bit and i retrain t he model wh at happens t o t he simple model it does not change much  but what happens t o t he complex mode l it is more sensitive t o t hese observations t hat i have and that i s exactly t he quantity t hat we were interested in  th at means a complex for a complex model which is more sensitive t hat summ ation t hat we care about is going t o be high  t hat means t hat difference between t he t rue error and t he estimated error is going t o be high refer slide time fourten so that is why instead of minimizing t he t rain error we should always minimize t he t rain error plus some quantity which is linked t o t he model complexity  t his is t he basis f or all dash methods r egularization method so now you see where t his comes from  so ok where o mega t heta would be high for complex models and simple for simple mod els ok you get t he intuition for t his and t he rest of t he lecture we will spend in t aking various cases where we will actually show t hat omega th eta would be high and we are t rying t o control f or o mega theta  t his quantity for t he rest of t his lecture an d for t he rest of t his course i will assume t hat we all know how t o deal with w e have done enough of t his  w e have done a lot of back propagation we have done enough derivations of t he laws with respect t o t he output layer and so on everything right s o all of us understand how to deal with l train theta where l train t heta is t his l equal to one to m squared error loss or your log likelihood or any of t hese losses right so we all know how t o deal with t his t oday  we are going t o focus on t his other t erm which brings in t he regularization refer slide time fivefifteen so what o mega theta does is actually acts as an approximation for t his  so what i should have actually t ried t o minimize i s not just l train theta  but l train t h eta plus t his other qua ntity which was t here in my equation  y ou get t his my t rue equation was t hat my loss is equal t o l t rained t heta plus t his t erm right which we approximated using s teins l emma  so i should have t r ied t o minimize t his quantity  but i do no t know how t o rea lly compute t his quantity so i am going t o just substitute i t by o mega t heta and ensure that o mega t heta is such t hat it is high for complex models and low for simple models  d o you get t he recipe e veryone gets t his h ow many of you understand t his fi ne so w e can show t hat lone regulation ltwo regularization early stopping all of t hese are actually special cases of t his particular formulation t hat we have refer slide time sixnine and r emember t hat t his is t he sweet spot t hat we were aiming for ok an d t his gap is actually t his quantity because we are making a very optimistic estimation of t he error whereas t here is actually t his quantity which we have been ignoring and t hat i s why we see t hat t he validation error is high ok  so is t he full picture in t erms of t he diagram and all t he equations t hat we have seen so we should ensure using o mega t heta t hat t his gap is also minimized  th erefore our function should be minimized l t heta plus o mega t heta  so essentially what we are t rying t o do is mini mize t his g ap and hence t he model would generalize better on t he t est data i s t his intuition c lear t o everyone refer slide time sixfiftyseven w hy do we care about t his bias v ariance t radeoff m odel complexity t his is not a course on machine learning  t hey a re highly complex models t hey have many parameters many non linearities  in fact now can you relate t his back t o t he universal approximation t heorem w hat is t he universal approximation t heorem say give me any data  i will give you a deep neural netwo rk which will exactly over fit t he data right and t hat i s exactly what we want t o avoid th at i s why regularization is important in t he context of deep neural networks fine  it is very easy for t hem t o over fit t he data and derive t raining error equal t o zero and that is why we need some regularization refer slide time seventhirtyfive so t oday we are going t o look at different forms o f regularization starting with ltwo regularization some simple t ricks  so some of t hese are going t o be mathematically motivated some of t hese are just going t o be heuristics or empirical stuff  so data set augmentation is one such empirical stuf f h ow many o f you t r ied data set augmentation for t he immunized assignment or t he back propagation as parameter sharing and t ying is so mething t hat  no i am not p lease do no t give me t hat look ye a h i am not suggesting t hat adding noise t he inputs adding noise t o t he outputs early sto pping ensemble methods and drop off right so t hese are t he t hings t hat we are going t o t alk abo ut t his and all of t his is in t he context of regularization where you want t o avoid some kind of model complexity"}
{"audio_filepath": "Data/Preprocessed/From Cats to Convolutional Neural Networks_4.wav", "duration": 167.0, "text": "i will talk about the history of convolutional neural networks and i call this part of history as cats and it will become obvious why i call it so so around one thousand nine hundred and fiftynine hubel and wiesel did this famous experiment they are still i think you could see some videos of it on youtube where there is this cat and there was a screen in front of it and on the screen there were these lines being displayed at different locations and in different orientationssoslanted horizontal vertical and so on and there are some electrodes fitted to the cat and they were measuring trying to measure that which parts of brain actually respond to different visual stimuli let us say if you show it stimulus at a certain location does the different part of the brain fire and so on so and one of the things of outcomes of the study was that that different neurons in brain fire to only different types of stimuli it is not that all neurons in brain always fire to any kind of visual stimuli that you give to them so this is essentially roughly the idea behind convolutional neural networks starting from something known as neocognitron which was proposed way back in one thousand nine hundred and eighty you could think of it as a very primitive convolutional neural network i am sure that most of you have now read about or heard about convolutional neural networks but something very similar to it was proposed way back in one thousand nine hundred and eighty and what we know as the modern convolutional neural networks maybe i think yan li kun is someone who proposed them way back in one thousand nine hundred and eightynine and he was interested in using them for the task of handwritten digit recognition and this was again in the context of postal delivery services so lot of pin codes get written or phone numbers get written on the postcards and there was a requirement to read them automatically so that they can be the letters or postcards can be separated into different categories according to the postcard according to the postal code and so on right so or the pin code so that is where this interest was there and one thousand nine hundred and eightynine was when this convolutional neural networks were first proposed or used for this task and then over the years several improvements were done to that and in one thousand nine hundred and ninetyeight this now howfamousdatasetthemnistdatasetwhichisusedforteachingdeepneural networks courses or even for initial experiments with various neural network based networks this is one of the popular data sets which is used in this field and this was again released way back in one thousand nine hundred and ninetyeight and even today even for my course i use it for various assignments and so on so it is interesting that an algorithm which was inspired by an experiment on cats is today used to detect cats in videos of course among other various other things is just i am just jokingly saying this"}
{"audio_filepath": "Data/Preprocessed/Line Search_39.wav", "duration": 516.0, "text": "so we were looking at these different variants of gradient descent  w e saw that gradient des cent has this problem that it finds it difficult to navigate the gentle slopes  so we came up with tricks on momentum based gradient descent and also n esterov accelerated gradient descent t he trick in momentum was that if lot of your history is telling you to move in a direction then just continue to gain momentum in that direction  so instead of just updating based on the current gradient you also update based on the history right and there we saw that this is always going to be a problem that you will end up taking u turns and we had this analogy of how you look for directions and you just overshoot your destination and have to come back and take a u turn and come back and so on so to prevent that we realize that the update done by momentum base gradient descent is two step update y ou actually the first step is based on the history and then another step based on the gradient at the current time step right so then instead of doing th ese two steps at one go why not just update based on the his tory see what the gradient that tells you and then we saw this nice figure  i hope it was nice and where you saw that if you look ahead point then you will be immediately corrected with respect to your errors  so that was about nag and momentum t hen we saw the stochastic versions of these algorithms where we realize that if we do the batch version then you go over a million points and then make only one update which could be very slow in cases where you have large data  so we then decided to the s tochastic version where we just update for every point that again had these oscillations because we were taking greedy decisions we were just relying on one point to tell us which was the right direction to go o n a nd you saw that these esteem has become b etter as you increase the value of this k so k equal to one is the most stochastic version and then k equal to two you get the mini batch version and then you could just have different values of k  so that you have more reliable estimates of the gradients and in the limit if you have the entire data then you are just doing the full batch gradient descent right  th is is the vanilla gradient descent a nything else did we cover then we had some tips on the learning rate and the momentum t hese are again he u ristic  i gave you some ideas and you could try these in your back propagation assignment and see which one works better for you y ou could see you have any peculiar observations while implement the back propagation assignment so now there are a few more things left in this lecture  so i will start with the line search first  so this is one more thing before you move on to some more interesting algorithms w hich are the current state of the art and lot of deep learning solut ions refer slide time twofortyfour s o most people that you read would look at would have algorithms that we will see after ten minutes refer slide time twofiftyfour so now this is where just to contest contextualize things right  so we are still trying to see what i s the light rig ht l earning rate  to use a line search is one such method where instead of just doing one learning  so you can look at the code and just focus on this part and tell me actually what are we trying to do how many of you get what the algorithm is trying to do  so far what we were doing is we were just having a single learning rate ok and we saw that this learning rate can make a lot of difference right because if you are on the gentle part you want larger learning rate and i f you want steep part you wan t smaller learning rate so just fixing the learning rate to one value does not really help because then you will make you will suffer on one of the two cases are either on the gentle case or on the steep case  n ow what line search does is instead of j ust using one learning rate at every step now whether it i s vanilla gradient descent which is the batch one or mini batch or stochastic right just use a bunch of learning rate  so i have used five different learning rates here and i have computed the grad ients  th at part remains the same  t he computation of gradients does not change now you have the value  n ow you want to be conservative you want to multiply the gradients with this eta right  but you know that you do no t always want to be conservative in fact in some cases when you are on the gentle slope you do no t want to be conservative at all  y ou want actually blow up the gradients so now try these different learning rates and update w and b ok  so if you have five learning rate s you will g et five different updated values for w b now plug in all these w b values into your loss function right and see whichever is the minimum retain that w b value and repeat the process  t hat means again you will compute the gradients with respect to th is new value of w b and the new loss function a gain try out these five different learning rates and continue e veryone gets that so now are we using a fixed learning rate at every step n o and now do you see that if we are at a gentle slope it would p ick probably this as the learning rate and if we are on steep slope which should probably pick one of these as the learning rate and even lesser than that  i f you have the disruption it does no t make sense  y ou see the advantage of this now  y ou are in so me way heuristically trying to adapt to the slope of the error surface right by just giving a different learning rates so try all of these and w hichever works best pick it up  o k so that i s about it  w e are trying different values now what i s the f lip side of thi s n ow if you have k different learning rates that you are trying then at every step you have now increased your computation k times  so earlier you just add one learning rate u just going by that  but now i have k  so now this is again a trade off which is you have to see  n ow  i will give an example where this trade off clearly works so now if you are at the gentle slope  n ow making k more computations and moving out of that slope is definitely worthwhile as compared to just sticking to that slope where even after hundred more computations you will not really move out of that slope  so remember that gradient descent algorithm that we have seen where you just stick to the gentle slope after hundred iterations also right  but instead i f i tried five different learning rates and there is a high chance that i could have moved out of the gentle slope  d oes that make sense  y ou see the advantage of this refer slide time sixthirtyone so this is something that i have to talk about when i bac k to second order optimization so i will see when to teach that  so let us see line search in action  so this is again gradient descent  t his black curve which is visible there  t his is the one i a m talking about which is run for few iterations and is just stuck on the steep curve  y ou know this story now and it i s just get stuck there now let u s see what happens if i run  so now i will start running the line search based gradient s descent  so what do you expect now so it will just move very fast right so on the first step itself it is crossed wherever gradient descent was stuck after fifty iteration or so i will keep moving fast now here is an interesting question w ould you see oscillations here so when you see oscillations it is w hen your loss is actually increased from whatever it was currently w ill that happen in line search t he answer is always no i t could happen when could this happen so it depends on the learning rates that you have chosen right  so if you have chosen the lear ning rates  so suppose at one point to really be effective you needed the learning rate to be zeroone ok and now if zeroone learning rate was not in your set right that means everything that is there in your set is faster than zeroone so that it will again hav e the same problem as momentum because you will move faster than what you should actually move  so it depends on this careful choice of the learning rate set so that i s all i have to say  so there is a slight convergence would be faster than vanilla gr adient descent  th at is obvious and we see some oscillations ok and the statement is actually wrong  w e need to remove that ok  w e see some oscillations and these could be the similar wants to the once with that we see in momentum because we overshoot bec ause we have not chosen the right set of learning rates on e of the learning rates which was actually needed at a particular point right say at this point s uppose i needed to move very slowly and that very slowly say zeroone and that was not in my set then any other learning rate is always going to be m uch faster th e n  so you could see oscillation"}
{"audio_filepath": "Data/Preprocessed/The Curious Case of Sequences_6.wav", "duration": 351.0, "text": "so iwas talking about successes in image speech pattern recognition even natural language processing and so on so one interesting thing here is about sequences so i will talk about sequences now sequences are everywhere when you are dealing with datasoyou have time series which is like say the stock market trends or any other kind of a series time series then you have speech which is again a series of phonemes or you have music you have text which is a series of words you could even have videos which are the series of images right one frame each image each frame can be considered to be an image and so on so in speech data one peculiar characteristic of speech data is that every unit in the sequence interacts with other units so words on their own may not mean muchbut when you put them together into a sentence they all interact with each other and give meaning to the sentence right and the same can be said about music or speech or any kind of sequence data so all these elements of the sequence actually interact with each other so there was a need for models to capture this interaction and this is very important for naturallanguageprocessingbecauseinnaturallanguageprocessingyoudealwith sequence of words or all your texts or sentences or documents or all sequences of words so that is very important and the same in the case of speech also so if you take up any deep learning paper nowadays it is very likely that you will come across the term recurrent neural network or lstms which are long short term memory cells and so on so this is also something w hich was proposed way back in one thousand nine hundred and eightysix so a recurrent neural network is something which allows you to capture the interactions between the elements of your sequence i had said at a very layman level but of course you are going to see this in much more detail in the course and this was also not something new even though you hear about it a lot in the past three to four years the first recurrent neural network and what you see here is exactly a very similar to what we are going to cover in the course was proposed way back in jordan by jordan in one thousand nine hundred and eightysix its variant was proposed by elmen in one thousand nine hundred and ninetyso this is again not a very new idea t his has existed for some time but now there are various factors because of which it has been possible to now start using them for a lot of practical applications a s i said one you have a lot of compute time and the other you have a lot of data and the third is now the training has stabilized a lot because of these advances which i was talking about in terms of better optimization algorithms better regularization better weight initialization and so on so it has become very easy to train these networks for real world problems at a large scale so that is why they have become very popular and hear about them on a regular basis but it is again something which w as done way back so from one thousand nine hundred and ninetynine to one thousand nine hundred and ninetyfour actually people also looking at various problems will be training neural networks and recurrent neural networks and so that this problem which is known as exploding and the vanishing gradient problem which is again something that we will see in the course in reasonable detail w e have this problem and it is very difficult to trainrecurrentneuralnetworksforlongersequencessoifyouhaveaverylong sequence or a time series you cannot really train a recurrent neural network to learn something from that and to overcome these problems around one thousand nine hundred and ninetyseven l ong short term memory cells were proposed and this is again something that we will cover in the course and this is now almost de facto standard used for training for a lot of nlp work lstm are used as one ofthebuildingblocksandanothervariantsoflstmswhichareknownasgated recurrent units and some other variants sothisisalsonotsomethingneweventhoughtheyhavebecomeverypopular nowadays like almost any article that you pick about to talk about any article on deep learning that pick about to talk about recurrent neural networks or lstms or gated recurrent units this is not something which is new lstms had come way back in one thousand nine hundred and ninetyseven  but again due to various compute and other issues which i said at that time it is not so easy to use them but by two thousand and fourteen b ecause of these parallel progresses which i mentioned in terms of optimization regularization and so on people are now able to use rnns lstms for large scale sequence to sequence problems and in particular a very important discovery at this time are very important model which was proposed at this time which is attention mechanism which is used in a lot of deep neural networks nowadays which enabled to deal with a lot of sequence prediction problems for example translation where you have given one sequence in one language and you want to generate the equivalent sequence in another language so this is known as a sequence to sequence translation problem so for that people proposed a sequence to sequence attention network and this was one of the key discoveries which then led to a lot of adaptation of oradoption of deep neural networks for nlp a lot of research in nlp happened which was then driven by deep neutral networks so a lot of existing algorithms which are non neural network based algorithms which are traditionally used for nlp was slowly replaced by these deep neural network based algorithms ok andagainthisideaofattentionitselfissomethingthatwasexploredearlieralso somewhere around one thousand nine hundred and ninetyone or so and it was something known as reinforcement learning which was used for learning this attention mechanism what attention basically tells you is that if you have a large sequence and if you want to do something with this sequence what are the important entities of this sequence or elements of this sequence that you need to focus on so this is again something that we will look at in detail in the course"}
{"audio_filepath": "Data/Preprocessed/Selective Read, Selective Write, Selective Forget - The Whiteboard Analogy_108.wav", "duration": 513.0, "text": "so we have been looking at a new kind of or a different kind of neural network which is recurrent neural network and last class we spent some time on showing that it is how to train theserecurrentneuralnetworksbecauseof the specificproblemof explodingand vanishing gradients in particular we saw that if you want to propose if you want to back propagate the gradient from time step t the final time step to a n arbitrary time step k then you have this multiplicative term in the back propagation whic h could explode or vanish so todaywe are goingto try to see if weare tryingto focuson somethingwhichcan help us solve thisproblemto whateverextentrightso thatis whywe willlook at lstms a long short term memory cells and gated recurrent units ok so let us start with that so first we will introduce the idea of selective read selective write and selective forget and then we will try to build on this intuition and see how could you could realise it by using lstms and gated recurrent units and whether that help in solving the vanishing and exploding gradient problem so recording the state of an rnn records information from all the previous time steps that was the whole idea that you add this recurrent connections so as you kee p going on at this time step the cell is not only recording information from the current time step but it also has some kind of an accumulated history from all the previous time steps right but now the issue is that this state or this blue coloured vecto r that you see is going to be of some finite size right we will say that it is a one hundred dimensional vector or a one thousand dimensional vector but whatever be the size it is going to be some finite dimension now as you keep writing information to that cell you a re morphing the information that you had written at the earlier time steps right do you get that so now that is the problem right because on one hand you are saying that you want to record the information from all the previous time steps but the at the same you at the other hand you just have a finite amount of memory to deal with so it is bound to read over ridden and the information will get morphed so much that it is completely impossible to say what was the original contribution at time step one or time step two once you have reach some time step twenty thirty or so on right so that is the problem with recurrent neural networks and we will tie it back to the problem that we had with the vanishing and exploding gradients right refer slide time twothirtythree so in fact the similar problem occurs when you try to when the information flows backwards during back propagation right it is very hard to assign the responsibility of the error caused at time step t to arbitrary time steps before it right to ver y far away time steps it is very hard and that is the vanishing gradient problem right because we have this multiplicative term and the gradients vanish so that that is very hard to do so both during forward propagation the information vanishes and e ven during backward propagation the information vanishes right and we saw a formal argument of this while doing vanishing gradients so this is just an illustrative diagram but we also saw that formally the guardians do vanish under certain conditions right now let us see an analogy for this and from here on we will build on some ideas which will help us arrive at lstms and gated recurrent units right so the analogy is a whiteboard so you have a whiteboard and it is a lways of a fixed size right i mean the whiteboard is not infinite you just have some size for the whiteboard and you keep writing information on that so at every time steps i keep going and writing something on the board and i am trying to derive somet hing or just try to make a story or anything right i just keep trying to write information on the board now since the whiteboard is fixed size at every time i am essentially morphing the information which was written at the previous time step and afte r many time steps it would be impossible to find out that now whatever my state of the board is how did time step one contribute to that state right because it is written all over the board i do not know where i started what i did and so on and its goin g to be very hard for me to find and this happens right when you do these long derivations on on the whiteboard it becomes very hard to track where did i start where was this variable defined and so on right because it is a fixed size you cannot reall y i will end of deleting some terms and so on right so and we will make this more concrete with the help of an example right suppose i am trying to drive an expression on the board and typically what we do is we use three thing s we use selectively write on the board selectively read the already written content and selectively forget or erase some content so let us see what i mean by these three ideas ok so first we look at selective write so this is my problem this is the derivation that i want to do on the board and i have a very small whiteboard which allows me to write only three steps ok that is the situation in which i am operating ok so i am given the values of a b c d and i want to compute this expression ac bd a ad ok now the first thing that i am going to compute is ac right that make sense because that is the first term that i need so i will write ac equal to five and the second thing i will write is bd thirtythree ok so now why d id not i do the following i could have done this a equal to one c equal to five then ac five then b is equal to something then b into d is equal to something so what am i doing here while writing on the board why did i write only two steps because i kn ow i have a finite size for the whiteboard right so i am only trying to write information which is important right i am not writing everything because i know that i am going to run out of memory right so that is why i did not write these intermedia te steps that a to one c five and then a c five and so on right i juts wrote the results which are important so i am selectively writing to the board because i am dealing with a finite size memory and this is exactly what we do while writing in a note book or a whiteboard or anywhere right we just do not write everything one because you are lazy but second is also because we do not have enough space right so that is about selectively writing now selectively reading i a lready have something stored on the whiteboard so this is the state of the whiteboard at this point now at the next time step i need to read some information but i do not need to read everything that is on the whiteboard i just need to read some infor mation what is the information that i need to read for the next time step bd i do not need to read ac right so i am just doing a selective read of the information or the state which is already stored on the whiteboard now what is happed is i have e xhausted my space where i had just three steps that could be written on the whiteboard and i have done that now what do i do for the next thing now i need to compute ac bd a  i will have to selectively erase so what will i erase bd right refer sli de time sixfortyfour so now as the whiteboard is full and i will have to selectively delete some information and as this obvious in this trivial example that you can get rid of bd because you have already encoded the information in bd a and now the next st ep which is ac into bd plus a can do on the whiteboard and this is how you keep doing at every time step you selectively write at every stage here again note that i am not written ac bd a would be something like i do not know what was it five x thirtyfour one hundred and seventy  right i am not using two steps i am just writing everything in a single step because i do not have enough space right so selectively writing selectively reading and selectively forgetting things which are there in a constant memory is something tha t we do regular right and then other ways of motivating this so you could even think of our brain as something which can store only a finite number of facts right as we keep going or if we will learning more and more things we can only retain a finite number of facts and what happens inadvertently is that you erase some of the steps not consciously of course you do not have a delete button or anything but you erase some of these things that you forget a lot of things which had happened a year back or so and also at various times if i ask you what was this which i have done in last class most of you forget to do the selective read but that is what you do right you always do selective forget but that is what we typically do in our brain also right any time when you are dealing the finite size memory you will always have this three operations either they are explicit or implicit but the intuition is that you end up doing this ok so now since the rnn also has a finite state size can we do something like this selective read write and forget so that one during forward pass even if the information gets morphed it gets morphed in a principle manner right so even in the whiteboard example i was morphing the information i wa s deleting the information written at time step one time step two but i was being a bit smart about that i was retaining some good information and only deleting what was not required so can we do this in analogy so this analogy really sets it up but now the solution is not going to live up to the expectation but it will have some it will be something in this direction ok"}
{"audio_filepath": "Data/Preprocessed/Eigenvalue Decompositon_44.wav", "duration": 541.0, "text": "in this modulewe will studyeigenvaluedecompositionso the answerto that was actuallywhichi was hopingall of youwill give becauseall of you havedonetwo prerequisi te whic h is linear algebra a nd mac hine learning both of the m teac h you principal component analysis s o i was hoping that you will give that answer nowcan you give that answerhe alreadyof coursegavethatansweris that make senseso we relateit to that so but beforegoingto principlecomponentanalysiswe look at eigen value decomposition this is very straightforward so let u one to un be the eigenvectors of a matrix a and let lambda one to lambda n be the corresponding eigenvalues now i am going to construct a matrix u such that the columns of u are these vectors uone to u n is that fine what u looks like and now i am going to do this product i am taking a the product of the matrix a with the product of with the matrix u where u is this right it is the all the eigen vectors tagged one after the other is this fine the next step i am just pushing the matrix inside if you know the four different ways of multiplying a matrix you w ill know that this is correct  or else for now just thing that you ca n just push the matrix inside now what is this i can replace them by the lambda one u one lambda two because a u one is equal to lamb da one u one by definition ok now can you write this again as a product of two matrices one is of course  the matrix u and the other is student diagonal diagonal so the diagonal matrix will come first or the matrix u will come first how many if you say u will come first how many if you say the diagonal matrix will come first the sum is never one ok so it is going to be like this ok and you can write this as u lambda so u is again the vector the matrix containing the eigenvectors of a and lambda is a diagonal matrix where every diagonally element is a corresponding eigen value now this is what we have so far a into u is equal to u into lambda now suppose u inverse exists i will assume that u inverse exists and later on i will tell you under what conditions it exists then i could write it as this any of these two forms in one case i am post multiplying by u inverse in the other case i am pre multiplying with u i n verse ok so this is known as the eigenvalue decomposition of a matrix and the other way of writing it is known as diagonalization of the matrix right you take a matrix apply some operations to it so that the result is a diagonal matrix is this clear to all of you is very straight forward ok and again eigen v ectors play an important role in this now the important question is under what conditions would u inverse exist u inverse would exist if the columns of the matrix u are student linearly independent linearly independent ok do we know the columns of t he matrix are linearly independent student yes yes because it is a student refer time  threeten set of eigenvectors and we already saw the proof that the eigen vectors are linearly independent ok this just follows whatever i say ok now do we need p roof for this i slide nineteen we did this  i did not realize it fine now if a is symmetric the situation is always more convenient why is it student refer time threeforty what would u be student orthogonal matrix what is an or thogonal matrix actually student refer time threefortyseven so the eigenvectors are orthogonal so we have this situation right suppose i want to do u transpose u ok this is how that operation would look like ok now what is the ij th entry of the resultant matrix  student dot product it is the dot product between the student ui and uj ui and uj everyone gets this right the ijth entry of this product is going to be the dot product between ui and uj this dot product would be dash if i is not equal t o zero or j student j j and there is no point in this so each cell of the matrix q ij is given by the dot product and it is going to be zero if i not equal to j and it is going to be one if i is equal to j ok so u transpose u is equal to the identity matri x that means u transpose is the dash of u student refer time fourfortyfive transpose of u and of course inverse also ok so u transpose is the inverse of u and it is very convenient to calculate what is the complexity of inverse so now you appreciate that that is a that has high complexity and in this case if the vector if the matrix is orthogonal that means it is a collection of orthogonal vectors and the inver se just comes for free right so now given this situation and do not read the hint as if this is going to help but yeah what can you now say about the sequence the same sequence that you saw earlier so i have given you that the evd of a is equal to u sigma u transpose where u is the collection of the eigenvect ors and sigma is the eigen values the diagonal matrix containing the eigenvalues now what given this and ignoring the knowledge of the first section of this lecture can you tell me something about this series what would be the nth element of the series student u sigma power n u sigma student power n power n student u transpose u transpose and you arrive at the same conclusion right where i was talking about this operation right so if we can say something about this matrix then we can say some thing about this series what can you say about this matrix if the largest eigenvalue is greater than one as you keep raising it is power that value is going to explode and hence the entire product is going to explode less than one that product is going to vanish and everything else would be less than that right remember is the dominant eigen value so everything would be less than that so that product will vanish ok so the same conclusions you can arrive at right so that is why i want to do these sec tions again so you would have done these in linear algebra but you would have not arrived at these conclusions from a very different interpretation but i want to focus on the interpretations that i care about i do not how many of you have seen this s eries in the course on linear algebra you have ok but i do not see why anyone else would teach this is not required is only required for some things that i want to do in the course right that is why i wanted to do this section so everyone is comforta ble with eigenvalue decomposition it is a very simple stuff right i mean there is no proof or anything involved there we just use some properties of eigenvectors and eigenvalues and do it now there is one more important proper ty of eigenvectors which well use today so let us see what this means right you have a matrix a which is an n cross n matrix ok and your import interested in computing this value x transpose a x where x belongs to rn x belongs to rn so what am i tr ying to do here of all these vectors possible in rn i want that vector which maximizes this quantity what is this quantity scalar vector matrix tensor student scalar scalar ok such that x is equal toone this is the problem that i have been given to solve why it is not clear as of now but suppose this is a problem i am trying to solve or the inverse of this which is minimize the same thing of all the vectors in rn find the vector which minimizes this quantity subject to these constraints then th e solution for this is given by the smallest or largest the solution is the smallest eigen value of a and x is the eigenvector corresponding to that so if you are trying to minimize and the solution is a smallest eigenvalue we need to clarify that if y ou are trying to maximize and the solution is the largest eigenvalue is that clear and the value of x would be the corresponding eigen value so largest eigen vector is the same as something that we have defined to day dominant eigen vector right so let me just repeat so that there is no confusion let us focus on this problem the solution to this problem that is the x which will give me the maximum which will maximize this is the dominant eigen vector of the matrix a right is that clear fine ok and if you want to minimize it is going to be the smallest eigen vector that means the inverse of the dominant so there is a proof for that i will not go over the proof you can take a look at it at your own leisure refer slid e time eightfortysix so what has been the story so far the story has been that the eigenvectors corresponding to different eigen values are linearly independent i f you are dealing with the square symmetric matrix which is something that we will deal with s oon then things are even more convenient because the eigen vectors are actually orthogonal ok and they form a very convenient basis and now we are going to put this to use when we talk about principal component"}
{"audio_filepath": "Data/Preprocessed/Early stopping_66.wav", "duration": 673.0, "text": "i will do will do early stopping where aga in we will ge t into some of these eigenvector analysis so let us see that so the idea been early stopping is actually very simple in principle what needs to be done so we know that this that this trend exists between the training error an d the tester right so in practice what you will do is you will continue to optimize the training error the empirical training error which is the sum of the errors on the m training points you will also continuously keep track of the validation error that means the same quantity you will compute over the n validation or test points everyone get this you can do this and you are actually doing this in your back propagation assignment keeping track of the training error as well as the validation error and you keep plotting them ok i will keep running for various epochs and keep something known as a patients parameter p so if you are at the twentyth epoch and if your patients parameter p is equal to phi and just do a check whether in the phi last phi epoc hs has my validation error e ver gone down or it has been staying the same or has it been increasing ok now i will give you a condition that it was either staying the same or it was actually increasing is this good or bad what does it tell you while your training error was of course decreasing may the more you train your training error will keep going down so what does this tell it is just over fitting you are fitting the training error you are just making it zero or as close to zero as possible but th at is not helping your validation error so the validation error is either worst case increasing or remaining the same right so this is a very commonly used trick which is known as early stopping you keep this passions patients parameter and you make s ure that if you have cross this patients right and the patients here is that i was waiting for the validation error to go down but it is not going down for some p epochs so no point in continuing training anymore i will just stop it does not make sense so and this can also be used in conjunctio n with other regularizers right so in the quiz also we had this question sorry for bringing up the quiz but we also at this question where you have the sparsity regularization and i was asking whether i can add the ltwo regularization along with it so these regulations can be added or used in conjunction it is not that you can only use one of them so early stopping is a way of regularizing but you could also use it in conjunc tio n with ltwo regularization or any other regularization technique that you do not want right so but how does this act as a regularizer from the picture it is probably clear and is the same as the explanation i was trying to give to his question right  t hat you are preventing yourself from entering in these regions and trying to enter into more favorable stop at more favorable regions but can you think of slightly more in terms of what happens in gradient and what would happen if you stopped it early and so on can you try it to connect it to the update rule of gradient descent what happens as you keep doing it for more and more epoch no gradient descent has nothing to do with validation error or backtracking error gradient descent only works on the tra ining data let us think in those terms gradient star diminishing to zero so  what happens how does gradient descent progress where do you start i started a random point at every epoch which is a collection of iterations right or you go or many training p oints what happens to this i start moving i keep moving now if i fix the number of epochs or do not allow it to change any more after a number of epochs what am i doing i am restricting the boundary around the weight right i am not allowing it to gr ow beyond a certain boundary do you get that let us see that so we will first see an intuitive explanation and then go to a more mathematical analysis are update so the update rule for gradient descent is i always make t his mistake t his has to be minus oh the t h have disappeared ok is there so  sorry other to have disappear so now what would actually happen at the t h step is we have w naught three plus or minus does not matter it just tells you that how much it is go ing to change this is what is happening actually at the t h step right you have just subtracted all the previous derivatives that you had so far right from where you started off now you are looking at t steps so at every point you are computing a cer tain gradient but had a certain magnitude now let me say that across all these steps the maximum gradient that you had i will just call it by tau right so that means in this summation there are t terms i am saying the maximum of those was tau that was the maximum rate gradient that i got at any one point now what i am going to do after this i am going to replace this by something this summation is always going to be less than or equal to this right because i am assuming that each of my steps is less than tau there are t such steps so i could have at matched moved t into tau right but i would have moved less than that because tau was th e maximum gradient that i had so this is going to be less than equal to is that do you get the change f rom the equality to less than equal to ok so now what am i restricting actually in early stopping what is being restricted there are only so many symbols there i just speak one t tau is of course not in your hands w naught is not in your hands w so t is the one right so i am only allowing that many updates so that means from w naught you can only moves that much this looks you see that analogy that this is something similar to you not allowing the weights to really grow a lot refer slide time  sixtwentyseven so now but will not end here you will of course do some more stuff on this right ok so we now see a mathematical analysis of this so recall that a taylor series approximation for l w is the following the same thing which i wrote a few sl ides back or many slides back everyone remembers this right and now again i am going to do the same thing that if i know the optimal w star then the gradient at that point is going to be zero so this term disappears  and now if i take the derivative thi s is what will remain this is exactly what we did earlier also right so we will have derivative of this and derivative of this so the derivative of this quantity is just this and the derivative of this is zero because that is exactly what we started of f with right that w star is the optimal solution now sgd up date rule is the following ok which i can write as this i just replaced this by this ok i am just rearranging some terms is that ok how many if you are fine with this how many feels to tire d to even care about this so this is what w t would be this is again some simple steps leading to some conclusion the conclusion is what matters the steps are very easy you can go back and look at them right so again i wi ll use the evd the same trick that i did earlier and it will give me this instead of h ok again i will just do some rearrangements and actually i can show that if i start with w naught equal to zero then w two is actually given by this quantity ok and there is a proof of this in the appendix you can go and look at it now what does this look similar to rotation diagonal rotation exactly similar to the analysis that we did for l two regularization right and in fact if you can you can show that if we compare this expression with the while we had for ltwo regularization and this is the expression that we had for l two regularization right rotation some scaling and then again rotation right then we can show that early stopping is actually equivalent to l two regula rization if the following condition is satisfied this does not mean much because god knows how you will satisfy this condition right but all it is saying is that there is some equivalence at under certain conditions and that is what is the intuition was also telling us that it is somehow preventing the weights from going large and it is doing this in this very convoluted way where this condition holds for it to be equivalent to l two regularization as i said for you and me is going to be very hard t o create this condition right how do i make sure that something like this is true right but that does not matter what matters is that there is some equivalence between them so when you are doing early stopping it is not j ust a heuristic or a blind thing that you are doing you know that it is somehow related to l two regularization hence that you are doing it and hence it also works in practice is it fine we will that work for all of you ok right so the things to rememb er is that early stopping only allows t updates to the parameters t his is the important thing rights so now if a parameter w corresponds to a dimension which is important for the loss then what would this quantity be the partial derivative of the loss with respect to that parameter it is going to be if there is a parameter for example let us take the amir khan an example right that whatever weight you gives to whether the actor was amir khan or not if that is very important because if that featu re is on you are lost completely changes and so on right if you do not learn the weight correctly that feature is very sensitive so for important features the loss would be very sensitive to the changes in the weights of these features is that intuiti on correct right t hat means this gradient would be large ok and if a parameter corresponds to a feature which is not important what would this derivative be small now what is the net effect of this you have some parameter which are important so the d erivatives are large some parameters which are not important so the derivatives are going to be small and you are going to only allow t updates so what is going to happen the parameters which are important we will end up getting effectively more upda tes right because  each of these magnitudes was higher and you did t of those the parameters which are not important we will end up getting ef fectively lesser movement because each of these gradients were small and you did only t of those right so yo u again see this that it is a weird way of ensuring that your important parameters get more updates than your n on important parameters right so it is very important to see these connections between these different regularization methods all of you are fine with this fine"}
{"audio_filepath": "Data/Preprocessed/Faster, higher, stronger_5.wav", "duration": 123.0, "text": "so this is what the progression was right that in two thousand and six people started or the study by hinton and others led to the survival and then people started realizing the deep neural networks and actually we use for lot of practical applications and actually beat a lot of existing systems but there are still some problems and we still need to make the system more robust faster and even scale higher accuracies and so on so in parallelly while there was lot of success happening from two thousand and twelve to two thousand and sixteen or even two thousand and ten to two thousand and sixteen in parallel there will also a lot of research to find better optimization algorithms which could lead to better convergence better accuracies and again some of the older ideas which were proposed way back in one thousand nine hundred and eightythree now this is again something that we will do in the courseso most of the things that i am talking about we are going to cover in the course so we are going to talk about the imagenet challenge we are going to talk about all those networks the winning networks that i had listed there alex net zf net google net and so on we are going to talk about nesterov gradient descent which is listed on the slide and many other better optimization methods which were proposed starting from two thousand and eleven so there was this parallel resource happening while people were getting a lot of success using traditional neural networks they are also interested in making them better and robust and lead for lead to faster convergence and better accuracies and so on so this led to a lot of interest in coming up with better optimization algorithms and there was a series of these proposed starting from two thousand and eleven so adagrad is again something that we will do in the course rms prop adam eve and many more so many new algorithms i have been proposed and in parallel a lot of other regularization techniques orweightinitializationstrategieshavealsobeenproposedforexamplebatch normalization or xavier initialization and so on so these are all things which were aimed at making neural networks perform even better or faster and even reach better solutions or better accuracies and so on this all that we are going to see in the course at some point or the other"}
{"audio_filepath": "Data/Preprocessed/Continuous bag of words model_78.wav", "duration": 2151.0, "text": "so fromhere on so noneof thisthat wecoveredhadanythingto dowith neura l networkssay but itwas importantto understandthe contextand i will tell youwhyit was importantto goover the traditionalwayof learningwordrepresentationsand then we will see how itties to the modernwayor the neuralnetworkway of learning representations so we will star t with the fir st neura l network base d model for learning word representation which is known as the continuous bag of words model so just to set the context the methods that we have seen s o fa r are known as count based models because they rely on these co occurrence counts for learning representations of words and the methods that we are going to see now are called prediction based models and it will become clear shortly why the term predictio n and how they learn the word representations so in a way in the original thing there was no learning involved of course you can say that you were trying to learn these eigenvectors and eigenvalues and so on  but it was not in the same way as it be as we have been learning parameters of a neural network and so on right i t was not in the same spirit but now once i do these second type of models this distinction would become very clear one why is there a learning involved and why they are prediction bas ed models refer slide time onetwentyfour so the story is we are going to look at continuous bag of words model then something known as skip gram  so this is the famous word twov ec model which you guys have already started looking at then we look at g lo v e wor d embeddings which is some kind of a hybrid between the count based models and the prediction based models a nd then we see how to evaluate word embeddings and then end with this depressing note that good old svd is just fine  right so all the progress that has happened in the past five six years you could just use svd and still go by  but if you do that you will probably not get a job right you have to learn these things refer slide time onefiftynine so now let us start with the continuous bag of words mode l  so consider this task and just bear me for a few slides that when why this is connected to our problem and all that so i am going to consider ka task we are here to predict the n th word given the previous n minus one words  right a s this is somethi ng that you do regularly sometimes even in the class when you are w h atsapping or smsing  so this is what you do r ight you start typing he sat on a and you get this prompt that the next word should be chair or something like that  right now you can thin k of this as a classification problem tell me why you can think of this as a classification problem can you tell me what is  so remember that we have always thought of this that there is always a y there is always an x and then we are trying to learn t his relation from x to y so given this example can you tell me what is x here and what is y here student refer time twofiftyone e veryone is clear about that  right so this is x and this is y now i made a statement that i could think of this as a cl assification problem  right so the minute i say classification what is the y that comes to your mind or dash hot by student refer time threeseven y not i anything else would have been inappropriate  but so one hot y is what you would expect there r ight and now what is the size of this one hot vector student size of refer time threenineteen s ize of the vocabulary  so we are trying to predict one of the words in the vocabulary so you see why this is a multi class classification problem you see that t here are many classes and you want to select one of these class n ow the moment i say classification i give you an x and y i will start asking me who will give me the training data for this  so can you think of training data for this any corpus similar t o the one that you are creating right refer slide time threefortysix i n specific w hat you will do is suppose you have framed it as the following problem that you are given for words and you want to predict the fifth word  so in general i have call it as t hat you are given n minus one words and you want to predict the n th word the n that i am considering here is four now what is going to be the training data for this  i f you take any corpus that you have built anything right consider all five word windows fro m there do not get too engrossed in the story  so there are four the first four words you can treat as x and the fifth word would be your y so you can construct many such x comma y pairs from the raw corpus that you are creating any five word window and you can keep sliding this window right what i mean by that  t his is your first training instance x comma y this could be your second training instance  so this could be these overlapping training instances you keep sliding this window and you will get ma ny training instances ok you see that so this task the advantage is that given the size of the web and so on at least for popular languages the training data almost comes for free  right c ompare this to mnist or any other task where you have to actual ly acquire these labels that this is an apple this is a banana and so on here you get the training data for free just need to scrape it from the web know n o window size is something that you will set right whether you want to learn four word windows or what do you mean we do not know the window size no so this again there is a lot of existing literature in the traditional nlp where various and lot of work has been done to figure out what is the right n  so in most nlp task right if you want to predic t the next word a three word window is enough actually i f you know the last three words and you can try this as a mental exercise right if you know three words you do not really need to know the words before that  so this is the mark of assumption with where this is a trigram dependency in the words  right so this n is not really so difficult and in the default tool that you guys would try probably they take the value of n is seven that is an overkill  but that is again it comes from a lot of existing l iterature in nlp right this is not a this task is not deep learning broad right this task is a simple language modeling task which has existed for many years right from probably one thousand nine hundred and fiftys or sixty s or something so this is all n word windows in your corpus as i said training data comes for free and for ease of illustration we will now focus on the case when n is equal to two  that means i am given one word and i want to predict the next word refer slide time sixeighteen a nd we will see how to model this using a n eural network  so these are the two questions which i need to tell you how to model this task and what is the connection between this task and our original task of learning word representations these are the two things that i am going to answer refer slide time sixtwentyeight so we will model this problem using a feed forward neural network what is the input o ne word ok s o say the word is sat i am going to represent it using a one hot vector ok and what is the output i want to predict a distribution over all the words in the vocabulary and i want to predict i want to pick the word which has the maximum probability that is how you did  so for example in the case when you had this classification problem of banana apple orange mango you predicted a distribu tion over these four classes and then picked the one which had the highest probability exact same idea here it just that instead of four classes now you have v classes and your v is very large ok i t is trying to learn a distribution over there and you know th at in this case or the example that you are considering on is the actual next word  so you type sat and the next word is on and probably leading to sat on the chair or something like that  so this is what you would want to maximize ok i have given you the input i have given you the output give me a neural network to model this there are lot of hints in the diagram itself right you see some space between the input and the output so what will you put in the middle layer we will put a middle layer the re  right i s this an ok way of modeling this task i have an input i want to predict an output s o i just use a regular feed forward neural network and let us analyze these parameters a bit more carefully  right so i am something known as w context i have something known as w word i am already using some notations from the svd lecture t here at the end we ended with w word and w context right it is not clear why i am using the same notations  but it will become clear in some time  but let us look at their dimensions so we have this one hot vector i have a parameter w context which i am going to learn right and its size is k cross v  so what does that mean t his matrix is going to multiply by the vector and give me a k dimensional output right is t hat clear  so i have this is of size v because always keep surprising me i do not know why you cannot do this r this is a v dimensional vector you multiply it by a k cross v vector  so you do w into x  so you will get a k dimensional vector  so this i s k dimensional you have a k dimensional hidden representation a nd from there now having captured this hidden representation you are trying to predict which is the next possible class  t his is the same as any other thing right if you had done the ima ge classification or the mnist digit classification you had this seven hundred and eightyfour dimensional in put vector you pass it through a hidden layer and then you predicted one of the ten classes there is nothing magic here it is the same thing that you have done seen before refer slide time nineseventeen h ow many if you get this and what are the parameters w context and w word ok a nd we are going to focus on these parameters and understand what they actually mean refer slide time ninetwentysix so what is the product w context i nto x given that x is a one hot vector  so i will tell you this suppose the i th entry is hot here how many if you say it is the ith column of w context  so it is simply the ith column of w context why  b ecause you have this w context matrix you take a one hot vector which has the second entry as hot if you do this multiplication you basically get the second column of w and you can just see it everyone gets this now how many if you get this now so if you have a one hot vector if its ith entry is on u multipl y it by a matrix you will get the ith column of the matrix ok so if the ith word is present in the input then the ith element of the one hot vector is on and the ith column of w context can be would be selected  so then can what can you tell me about the ith column of w context y ou see there is this one to one correspondence between words in your vocabulary and columns of the w context matrix how many columns has w context have v columns how many words are there in your vocabulary v a ny one word is on only one column will get selected and that is a unique column it is not going to change  right so there is a one to one mapping between the columns of w context and the words in your vocabulary  that means the columns of w context a re the are the vector representations d o you know these vector representations  n o these are parameters of your network  so they will they will be learned how we will see ok so you see the intuition for w context setting it up this way  so now i hav e set up the problem in a way that by parameter matrix directly gives me the word representations ok  b ut any kind of learning has to be driven by some objective  so what is that objective it is already clear to a lot of you  but we will just do that in a bit more detail  so this is exactly what i have just said refer slide time eleveneighteen now how do you obtain p on given sat no no  so for a given training instance s o when you  s o you could s o i will  so for a given training instance you said that your corpus has been divided into those training windows  right so it is possible that engineer sometimes the word does not and is not the next word  but for this training instance what is it  so that is what you have to predict right is that fine re fer slide time elevenfortyeight a nd at test time so you are saying that what you are saying is more practical that when i have typed sat in the w h atsapp message i do not want on as the always the answer  so we get these five options right three to five options  so what could be that you have this probability distribution pick the top five from there and show it as options s o is that fine so we are done with this now how do you compute p on given sat what is the actual operation happening there what is the appropriate output function  t his is a multi class classification problem softmax t his is what softmax looks like  so the property if suppose on is the i th word in your vocabulary then i am saying that the probability of on given sat is going to be this quantity how many of you agree with that i mean those who agree is fine i am asking why the others do not agree what is not clear about this i do not know how to explain this i mean it is just so plain obvious what is the softmax function f irst of all you will do this aggregation s o you will do this w word into h that is fine  right so for you will compute this vector consisting of w word into h fine what is the dimension of that what is the dimension of that mod this is k dimensional this is v cross k or k cross v depending on how you multiply it  so what is the output going to be v  so you have v entries t hese are dash entries the options are normalized unnormalized unnormalized now what does softmax do s tudent n ormalization n ormaliz ation that is ex actly what this formula is doing  right y ou want for the i th word you see what was the end this product right this gave you a v dimensional vector you look at the ith entry there right that is what you are doing here raise it to an exponent and divide d by the summation of all these entries  c ome on guys this is highly disappointing i cannot teach softmax i had in the tenth lecture eleventh lecture of the course right what is wrong  h ow many if you get this now just have to ask of it tangle you so yo u see this right this is what is happening here  so you get this v dimensional vector and you just con converting into a probability distribution using the softmax function so now this value how did he compute this value actually  y ou computed this pro duct which is w word into h and then you took the ith entry of that and then this some transformation on that the softmax transformation you see that refer slide time fourteentwentyone so now i can say that p on given sat is actually proportional to the dot p roduct between the j th column of w context and ith column of w word why am i saying that so remember that this was the ith word in your vocabulary and on was the j th word in your vocabulary  so can you explain the meaning of this sentence to me  f ir st let us look at the first part what is h i t is a j th column of w context oh sorry this should be i this should be j  so this you already saw that h is the jet column of w context because i am multiplying a one hot vector with the matrix is that fine a nd what is the ith column of w word so why what is this product actually equal to if i say w word into h w word into h that is a vector and then i am taking the ith entry of that  so i am saying that is the same as taking the ith column of w word an d multiplying it by h how many if you get this is basically in your algebra  right now these four different ways of multiplying matrices i am just using one of those  right so if i multiply a matrix with a vector and then take the ith entry of that t hat is the same as multiplying the ith column of the matrix with the vector ok j ust go back and verify this just take my word for it for now so now what is happening is that it is proportional to the product between the j th column of w context and t he ith column of w word is that clear now everyone gets this refer slide time fifteenfiftysix so p word equal to i given sat does depends on the ith column of w word ok so now what can you say  so earlier we saw that the ith column of w context corres ponds to a particular word now what can you say about the ith column of w word i t also corresponds to a particular word  so now why these two correspondences i already had a correspondence between w context and every word in my vocabulary now i am sayin g that there is also correspondence between w word and every word in my vocabulary how many of you first of all are comfortable with the sentence  t hat every column of w word has a correspondence with some word in the vocabulary t he second sentences eve ry column of w context has a correspondence with some word in the vocabulary do you all of you agree with both these statements o kay that is what we have try to prove so far  so now for every word  that means i have two columns waiting for it how do i d eal with this situation have you ever dealt with it before  t he same thing happened in svd also  right svd also gave you this u sigma which was w word and then v which was w context  so you can always learn two different representations for the words one is when the word appears as a context word and the other is when the word appears as the target would you get that you see why we have two different representations fine a nd as i said hope you see the analogy with svd right you already saw there that ther e were these two representation  n ow given all this set up and please do not disappoint me can you learn these parameters with some tweaks to the code that you have written for mnist can you use the same code to learn these parameters  h ow many if you say yes so what is the tweaks what are the tweaks  t he input changes instead of the image input you have this v dimensional input what else changes student output t he output changes instead of a ten dimensional output you have a v dimensional output al l of you are absolutely clear about this and what is the training algorithm student refer time seventeenfiftynine b ack propagation what is the loss function c ross entropy good fine refer slide time eighteenfive so for some i will do some more stuff on this be cause there is some in interesting interpretations of the gradient descent update rule here  so i will refer to the word sat by the index c and the word on by the index w o k a nd you already saw that the appropriate loss output function is softmax the appropriate loss function is cross entropy so let me just look at this  right so w was the index of the output word  so my cross entropy formula would just boil down to this everyone is fine with this o k i will just try to maximize the w th entry i n my y hat how many of you are fine with this o kay and that is nothing  but the probability of the word given the context now remember that h is equal to w context into x c i am going to call that as u c  so u c is the dash of the word sat title of t he lecture i t is a vectorial representation of the word sat everyone is fine with that  b ecause that is exactly what this product is going to do and now my y hat w is equal to this because i already said it is the product of the c th column of w context a nd the w th column of w word refer slide time nineteenthirtytwo so now i have a formula for y hat w what is the training algorithm that you will use g radient descent with back propagation n ow let us consider one such input output pair and see the update rule for v w refer slide time nineteenfortyfour so my loss function is this this is actually this quantity ok n ow i can just rewrite it as this i have just expanded the log  so the log of a by b is log a minus b ok now i want this quantity  b ecause this is the p arameter of the network right v w is one of the columns of w context or is it w word w word v w is one of the columns of w word and i want to learn i want to learn it what are the what are these column entries so  that means i am interested in this parti cular gradient so i will start taking this  so what is it going to be so only u c will remain here of all these summation terms only one of them would remain and then you can derive de derivative  right so this is what it is going to look like wha t is this quantity the softmax function  so this is what i get refer slide time twentyfortytwo so now my gradient update rule is going to look like everyone is fine with this i have derived this formula and i have just substituted that here and this nega tive and this negative ok so now let us look at this update rule refer slide time twentyfiftyseven so this update rule has a very nice interpretation which allows us to understand what does the continuous bag of words model actually learn  n ow suppose y ha t w tends to one what would that mean  y our prediction is very correct right you are almost predicting it has probability as one ok what would happen to the update in that case t here will be no updated if it is one there will be no update if it is close to one t here is going to b e very minimalistic update  that means you have already learned the v w well enough ok o n the other hand if i am very bad if y hat w is close to zero what would happen j ust tell me the case when y hat w is actually zero what is the update rule  h ave you seen something similar ever before  h ave you seen something similar before where did you see this update rule  p erceptron what happened when you did this w and x came closer to each other the angle between them actually decreased  so the s ame thing is happening here so what you are trying to do is you are trying to make your word representation closer to the context representation is that clear how many if you get this  i t straight away follows from the update rule right because you are adding a fraction of your context vector to your word vector and we know that when we add two vectors they come close to each other the cosine between them decreases that is what we proved in the word to it lec ture in the perceptron lecture  right refer slide time twentytwothirty so you can go back and refer to this slide on lecture two now s o the training objective is essentially ensuring that the cosine similarity between v w and context word is maximized between the word and the context word is maximized refer slide time twentytwofortyseven now what is the result of this  n ow i want you to think go back with a starting example where we said that we want to learn representations such that cat and dog are close to each other  but cat and truck are not close to ea ch other i want you to think whatever you see on this slide the conclusions that you drew from this slide how do they help you to relate back to that initial goal ok so now let us let me give you the intuition  right so what happens to the repres entations of two words w and w prime which tend to appear in the same context c so say dog eats cat eats  right so dog and cat are two words which appear with the same context eats  so what will happen to the representation of dog i t will come close to eats what will happen to the representation of cat  c ome close to eats  n ot only that dog will also go close to pet animal sleeps right and so on and cat will also go close to these  so transitively what will happen  d og is going close to a certain po int or certain sets of points cat is also coming close to the same set of points  so transitively dog and cat will come close to each other you get this intuition a nyone sees a problem with this  n o  so known objective and i said that dog comes close t o eats is that what he wanted i mean why should dog be close to eats  that means if i find the nearest neighbors of dog i will get words like eats sleeps barks and so on is that what i wanted so that is exactly what is happening a nd based on that i convinced you that dog and cat will come close to each other  but there is a subtle gap here i want you to close that gap how many matrices do we have two that is enough hint we are going to either take columns of this matrix as the representations or t he columns of this matrix as the representation not mixed so now can you tell me so dog here will come close to eats sleeps barks here word will come close to context word right  c at here will come close to eat sleeps and so on right so transi tively dog and cat will come close to here and this is the representation that you care about not representations across these two matrices ah  so what i said is that the training rule ensures that the words representation comes close to the context words representation ok that is what we saw with the training update rule s o  that means dog will come close to any kind of context word that it appears with  so dog i would expect it to appear with context words like eats pet animals dog barks drinks and so on  right so dog is coming closer to these words and i expect cat also to come up here with these words and of course i do not expect truck to appear with these words  right so then cat will also come close to these set of words dog will also come close to these set of words  so transitively dog and cat will come close to each other right all of them are coming close to each other ok which is fine which was my original goal b ut my original goal was not that dog and eats should come close to each other because eats and do g are neither synonyms when they do not have any semantics i mean they have a semantic relation  but that is not what i wanted i wanted similar words to come close to each other  but now i have the side effect that dog is co ming close to eats  but that is bad was how can i live with that so the i mean the key thing that you should notice is that you have one matrix of words the other matrix is of context words  so the representation of dog in the word matrix is coming cl ose to the representation of eats sleeps etc in the context representation on the context matrix  t he representation of cat is also coming close to these words in the context representation and transitively because of this dog and cat in the word matrix a re coming close to each other and this is the matrix that we care about i n this matrix dog and eats dog and sleeps are not close to each other right is that fine everyone gets this now ok so this is only an intuition and this becomes very tricky when i will blow this up what do i mean by blow this up  r ight now what am i trying to do what is the size of n two right i am taking one word and outputting the other word hence you get all these neat interpretations that you are moving close to that vector and so on  t he moment i add more words to n these interpretations become more and more hard right  but this again i mean this is good to understand that this is what happens at least in the best case  so this is only an intuition which is reasonable in m y opinion i have not come across a formal proof which says that this is what actually happens and that is one criticism for wordtwovec  i t works very well  but there is no formal proof which tells you why exactly it works a s opposed to svd right there we know there is a principle behind it here that is not very clear right  but it works very well based on this intuition ok so everyone gets the whole set up how we started with a classification problem of predicting the n th word given the n minus one word s which had nothing to do with word representations that is a simple language modeling problem which has existed forever  w e smartly modeled it or someone smartly modeled it using a neural network such that the parameters of the neural network end up giv ing you the word representations  a nd this network is end to end trainable using an objective function the training data comes for free for popular languages you have like tons of training data the entire wikipedia entire web whatever you can scrape that is why with more and more training data you can learn even better and better representations  so for popular languages the representations are really good a nd then we saw an intuitive explanation for why this works because of this movement of things clo ser to each other and the key thing to notice there are two different representation matrices one for the words one for the context and this is not surprising the same thing happened for svd also u sigma was w word and v was w context right so it is al l in the same spirit  right refer slide time twentyeightthirtyeight now in practice instead of window size of one it is common to use a window size of d either d could be four or seven i have i have even say and seen eleven actually  but not beyond that ok n ow let us see wha t happens if you have two and here itself it should become clear that now those interpretations are not very neat  so what i will use suppose i want to take a context of two words then i have he sat and now i want to predict the next word so what is my in put now  h e and sat  right so i will take the one hot representations of he and sat i will just concatenate them sorry i just concatenate them is that fine and my input now belongs to r raise to two v in general it will belong to r raise to d v ok and n ow what is the next step d o you see something funny here i have just created two copies of this ok i am telling you an inefficient way of doing this later on it will be a very simple thing to do a very efficient way of doing this right  but first just to get the math around i will just do inefficient way of doing it w hy have i staged it twice two words  right so now my h is actually going to be the sum of all the columns of w which correspond to my input words is that fine i have to earlier i had just one word as the input  so my h was just equal to that column of w now my h is going to be equal to the sum of all the columns of w corresponding to the words that i have and i will tell you why  so i have taken w contexts comma w context which is just t he w context matrix stag ed twice back to back  so this was my w matrix this is my two hot vector because i have two inputs now  right so my vocabulary size is three  so the first one hot vector followed by the next one hot vector and now i am going to repeat w w now what is the product of this  i s the sum of the two columns that you see highlighted right and exactly that is what i have written here so if i do it this way then i can just do this very expensive matrix multiplication and to do a something very trivial which is just taking the sum of two columns  right but at least you get the operation and i will just on this next slide or something i will tell you an obvious simple way of doing that  so i just get the sum of the two columns  so that is the inpu t to my network  i f i had k words as input if i had my window size four what would it be i would have these four copies of w context i will have these four one hot vectors and it will just give me the sum of those four columns ok that is going to be the input ok and the rest of the story remains the same  right o nce you have this h the rest of it from there remains the same and this is the formula for h in general i n the special case it was just the ith column in the general case is the sum of all the columns th at are there in your input refer slide time thirtyonethirtyfive now in practice of course this is a very mate expensive matrix multiplication it is stupid to do it that way what you will do is you will just slice of those columns from w context right and then just add them up  so you do not really need to do that stupid mate matrix multiplication because you know that the matrix multiplication is essentially just selecting these columns and adding them  so just select those columns and add them up so you do not do that bad matrix multiplication operation is that fine refer slide time thirtytwoone now what happens during back propagation in this case in the generic case the ordering does not matter is what you have seen yes it does not matter yeah t here is some assumption of the model  so it is that is why the name of bag of words you are not relying on the sequence  so this comes from nlp that if you rely the sequence you call it sequence if you just going to take the words in the sequence you just cal l it a bag of words because once you put them in a bag there is no ordering there right that is why the word name bag of words so and again p on given sat is given by this softmax formula ok now tell me during back propagation and if you give me a righ t answer to this i really feel happy that you have understood everything right f rom the beginning of the course so no pressure  so which are the parameters which are going to get updated during back propagation which are the two large matrices w word and w context s o obviously the answer is not w word and w context otherwise i would not have asked you t he answer is some dash of these two some subset of these two which subset let us start with w context which is the input do we are we going to update the entire w context did it participate the entire w context participate in the computation only those columns corresponding to the words  so only those parameters will get updated so how many columns will get updated d columns  right w word till all the columns of w word participate in the computation how many of you say yes how many if you say no  t he others do not care can you just focus on this circle did all the columns of w word participate in the computation  y ou see the summation at the bott om it is over all the columns of w word all of them participated so the parameters which will get updated are w word and all the columns of the input words and same back propagation will work again is that fine so remember that and this is i cannot em phasize it enough whatever i have explained is only for an intuitive explanation you will never ever do this matrix multiplication right and that is why what you are going to do is you are just going to select those columns add them up and feed them  a nd the network will take care or rather you will take care that you update those parameters only and you do not update the entire w context matrix because anyways there is no gradients flowing to the other components  so remember that in the practical imple mentation of w of word two vec do not search for this matrix multiplication at the input or if you are writing the code on your own which is highly unlikely do not do it that way so if you whatever code that you look at did not have this complex matrix mu ltiplication typically t hey will just pick up the columns and add them and feed them right and i think the tensor flow way of doing is you have this word embeddings matrix and you can slice columns from there and s o so everyone understands this so far n ow what are these problems with this why is this not as simple in some sense as the mnist data set a gain focus on the circle this softmax computation is a very expensive operation right you have a v cross k sized matrix somewhere there and unlike at t he input here you will have to do this matrix multiplication  right so we have a v cross k matrix multiplied by a k cross one vector and there is no simplification of this you have to do this multiplication what are the sizes of v that we saw in practice fifty k one hundred k and if you had g oogled thirteen million or something  right so this is not feasible we cannot do this expensive matrix multiplication refer slide time thirtyfivefiftysix so although all of this works very fine w e need to think of ways to simplify this softmax computation where the denominator requires the summation over all the words in the vocabulary  so you have to do that many matrix multiplications"}
{"audio_filepath": "Data/Preprocessed/How LSTMs avoid the problem of vanishing gradients (Contd.)_111.wav", "duration": 1388.0, "text": "ok so will start from where we left off so in the last class we started with this motivation that recurrent neural networks have this problem of vanishing and grade exploding gradients and we wanted to arrive with some principle way of avoiding this so you have first started wi th this intuition that in many real life situations like for example the human brain or the whiteboard we tend to these to these three operations called selective read selective write and selective forget and they essentially help us in dealing with t hese finite sized memories right or whether it is a whiteboard which is finite sized or your brain or whatever it is right so can we is it possible to kind of improve rnns which also suffer from this problem that they have this finite sized memory and hence if you are trying to capture everything from time step one then by the time you reach time step t where say t is thirty or forty or so on its quite natural that whatever you have learned earlier will get move off to an extent that it just is not recognisab le anymore right so you wanted to deal with this problem and with that we motivated selective read write and forget and then we introduced some equations or converted this into a model and this is the diagram that you see is the model actually that is t he lstm cell yeah and it has these three gates output gate input gate and forget gate and which perform these three functions of selective read write and forget so intuitively all these was fine but we need to be more technical in terms of you tryin g to deal with a problem of vanishing and grade exploding gradients so how does it solve that problem all that makes or the story seems fine but how does this actually relate to the math so we saw some intuition for that and the intuition hinsed on th is observation that during forward pass the gates control how much of information passes from one state to another and in particular if you have the situation that from one time step to another say the forget gate tells you that keep forgetting point five o f the previous state then by the time you reach say the one hundred state you would have forgotten zerofive raise to one hundred of the first state so that means even during forward pass the information from state one vanishes so if it vanishes during backward path that is also fine because state one did not contribute to state one hundred and that was the intuition that all this hinsed on now we are not going to do much different from this intuition we just going to see the corresponding equations for these intuitions and just m ake a more i would not call it rigorous but more mathematical proof on why lstm solve the problem of vanishing gradients and we are also sure that they actually do not solve the problem of exploding gradients and then we will see a simple trick of dealing with exploding gradient that is what we will do in the remainder of this particular lecture and then will move on to the next lecture in this lecture so we will now see an illustrative proof of how the gates control the flow of gradients right so we call that this is the control this is the flow diagram or the dependency diagram that you had for rnns and in particular because you are dealing with an ordered network we add this explicit and impl icit derivatives and finally you came up with this multiplicative form and this term here is actually a matrix because it is a derivative of a vector with respect to a vector and then this same matrix was getting multiple times and then we did this proo f it showed that this term is actually lambda gamma ok it is actually proportional to this term right and as if lambda into gamma is greater than one then this will explode if it is less than one then it will vanish given sufficient times that is refer sl ide time threefiftysix now in particular what is happening here is the following that you have this loss at time step t you have the time step is four now what if this loss or this error occurred because w was not good enough to compute a good value for sone ri ght so w was at a certain configuration based on that you computed sone and that sone was not good enough which eventually led to the error at time step four all of you if you can imagine this situation that you mean you not being not be able to do something w ell at sone now this needs to be told to w so that it can improve right and that information has to come through sone that information is already going from here but this information is about how badly it performed in computing sfour this is not how badly it can perform in computing sone so that information has to travel to w all the way through sone and that was not happening because this path do not look at the bullets this path was actually vanishing and that is what this multiplicative term says that as th e number of times that increased that time that path would vanish ok so that is the actual problem that we are trying to deal so now what is the general situation here right the general principle is that the gradient of l t heta at particular time step say here we are considering lfour so i will just call it lt with respect to any parameter theta i the parameters at w u v b and c with respect to any parameter it would vanish if all the paths leading to that parameter if it vani shes so with respect to this particular path so that is the only path which leads to w through sone if there were multiple paths if there was say one such direct path right if we had you some other kind of connection which gave us this direct path then it would still have been fine but there was only one path leading to w through sone at the gradient vanishes along that path then the gradient will vanish ok if there were multiple paths then only if the gradient varnishes across all the paths then the gradi ent would vanish is it fine what is the corresponding rule for exploding gradients if there are multiple paths the gradient would explode if student refer time sixsix if it vanishes through any one if it explodes though any one of the paths ok so these are the two things that we need to consider ok so to prove that in the case of l st m this does not happen for the first case will have to show that there are at least one path through which it does not vanish and for the second case because we are going to show that it explodes we just have to show that there is at least one path through which it can explode ok so these are the two things that we need to prove and the first thing that we are going to focus on is the vanishing gradient problem so will start with the dependency graph for lstms that means i want to draw something similar for lstms involving all the different elements in lstm so what are these different elements th e two rhyming things one being gates states ok so gates and states that the two things that we care about so let us look at all these so starting with states at time step k one at time step k one you have this two states s k one and h k one ok using h k one you are going to compute the output gate at time step k and it is als o depends on these parameters w o  u o and b o right which is obvious from the equation just to make sure that this diagram remains tractable i am going to get rid of the parameters and i will come back to them later so right now will just focus on the states and the gates ok and then you have these other intermediate states and the other gates right so you had f k you had i k  so add these three gates the temporary state and then what else what are the other two things at time step k so we saw this diagram about all the computations which happen at time step k right how many computations happen three states and three gates right so you seen the three gates and this one temporary state so which are the other two things there is no selective forget with you guys is early everything forget hint look at the grey cells and change the time step what will you get away are you all i mean we did lstms two days right i mean are you al l with that or should i we need to revise something mean i do not need to revise it but we going to is it fine ok so sk and the other thing h k remember that s k also depends on h k just stare at this for thirty seconds and make sure that you are with it righ t all the equations are there these are the see six equations that or the six computations which happen at time step k there are three gates and three states and the dependency graph is obvious from these equations except for the fact that i have ignored th e parameters how many if you are comfortable with the equations and the graph corresponding graph please raise your hands high so i think it should be right we have these six equations and we have this dependency graph now starting so what happened in the graph is we started from s k one and h k one and we reached s k and h k which were the outputs at the next state now what will happen from here we were looking at recurrent neural networks recursion is the answer what will hap pen now the same graph will keep recusing right for the next time step and up to the last time step right does that makes sense ok this looks much more complicated than the dependency graph that we had for rnns right by just because there are so many we in rnn we just had this one state and no gates so here but we have these three states and three gates that is why this so many paths ok now for simplicity what i will do is i will not draw separate nodes for the parameters all the in the case of the r nn dependency graph i had drawn them separately what i am going to do is i am just going to put the parameters on the corresponding edges right so f k actually depends on w f  it also depends on u f  and it also depends on that bias but i am just going t o take a small set of parameter i am only going to focus on the ws not the us and the biases ok there is only for illustration for no other reason right and whatever arguments or proof that we are going to see it holds for all the parameters but we jus t need to prove it with respect to one parameter and the same story repeats for everything ok so this is the dependency graph and these are the parameters now what i am interested in knowing is that there was some loss at time step t and maybe that los s happened because w f was not good enough to compute s k of course w f computes f k and then f k helps in computing s k  but maybe w f was not i am just short it short circuiting it and saying that w f was not good enough to compute s k right and that is why i w ant the gradient to reach to w f through this s k that is what i want do ok and this exactly what i said i am interested in knowing that if this loss can reach w f through s k right so all the three highlighted things that what i am interested in i am interested in the path to w f through s k  of course there are many other paths to w f  but they do not account for the problem in s k  is that fine everyone is clear the setup ok now and we can ask similar questions about all the oth er parameters the ws the us the the input gate parameters the output gate parameters and so on right there is nothing so special about w f the same question holds for all these other parameters also ok now how does lstm ensure that this does not vanish so let us see that as i argued earlier it is sufficient to show that this gradient does not vanish ok if i can show that this gradient does not vanish then i am pity sure there is only there is no recursive connection here be cause it just a single connection so there is no recursive connection here so if i can show that the gradient reaches up to this point then after that i can be sure that it is going to reach wf everyone buys that set up right that is what i need to sho w so to prove that the gradient reaches w f i just need to show that it reaches s k that is the only thing that i need to show and the first thing i am going to observe is that there are multiple paths to reach to sk which are these paths one through s k one  because s k contributes to s k one the other through student h k h k which is visible but now also notice that how many paths are there to reach h k itself four not four actually that is going to be combinatorial because there four outgoing edges from here  but then again there will be four next stage and four next stage and so on right so let us not count the number of paths but let us just convince ourselves that there are many paths to reach to s k from l t   everyone is conv inced about that we are not counting the exact number of paths that is not very hard to do but all we are saying is that we know that there is one path through s k one  one path through h k and h k itself seems to have many incoming path during back propagatio n so there are many paths which are reaching from l t  to sk everyone is convinced about that anyone who has a problem with that now to show that the gradient does not vanish what do i need to show of all the paths the set th ere exist at least one path through which the gradient can flow that is what i need to show ok even if i vanishes across all the other paths i am still fine with it ok so now consider one such path which is this highlighted p ath that is a valid path to reach to sk now let us denote the gradient along this path to be t naught and the total gradient is going to be a sum of many such paths right so i am calling this path as t naught and this is what the gradient look like ok so this is simple just this red path the next red path and then the series of problematic multiplications right you have this recursive multiplications again so everyone agrees that red is good the red path there is no recursion the gradient will flow right we just need to focus on the blue path everyone is convinced about that right ok so that is good the first term is fine as i said because it directly connected to l t there is no recursive or no other intermediate nodes so the gradient will just flow through that there is not a problem there and now we look at the other terms which is first is dh t s t and the other is this ok so let us look at  h t s t what is this going to be tensor vecto r matrix scalar at this point in the course i want a unanimous answer student matrix matrix right and recall that in particular the equation was of this form ok so what is the derivative going to look like even without computing can you tell me someth ing profound about it it will be a dash matrix big matrix how many if you say diagonal matrix how many if you do not think it is a diagonal matrix please raise your hands total sum is never one so remember that ht is equal to h t one  h t two up to h t d  and you have o t equal to o t one o t two o t d and s t equal to s t one s t two s t d  so h t two depends only on o t two and s t two right it does not depend on in particular does not depend on any of the other sts so we have already seen this before in such cases whats the i j th entry o f this matrix of the gradient matrix d erivative of h t i with respect to s t j which of these terms are going to be zero wherever student i not equal to zero i is not equal to zero that means it results in a student diagonal matrix diagonal matrix refer sl ide time fifteenthirtyone so that is exactly what is written here and the diagonal elements are going to be this is that fine everyone with this ok so now this diagonal matrix which contains this on the diagonal i am going to represent it by the following notation is that okfine so this is a diagonal matrix where every element is i mean this is actually a vector right everyone agrees this is a vector so this diagonal is this vector is along the diagonal of this matrix how many if you get this nota tion if you do not get this you will not understand anything else now let us consider  s t  s t one  ok this is what s t is equal to so what is the derivative of  s t  s t one  ft right ft right what else why no why are you rebelling what the i mean s t only right if it is can you treat this as a constant no why because this is a dash network student refer time sixteenthirtyfour so in an ordered network the derivative will have two terms which are those student explicit explicit and implicit in the explicit term what you assume the other terms to be a constant right fine so s t i mean s tilde t also depends on s t one  so w e cannot treated as a constant so once again this derivative is going to contain an explicit term and an implicit term now i am going to make a worst case assumption i making this assumption that actually the implicit term vanishes notice that this not favourable to me i am trying to prove that the gradient does not vanish the gradient is a sum of two terms i am saying it let the worst case be that one of these terms vanishes ok so this is not a favourable assumption this is a unfavourable assumption w hich i am making so let us fine so i making the assumption that the implicit term vanishes so what is the explicit term actually student f t f t and what kind of a matrix is that student diagonal matrix if you agree that it is a matrix first of a ll it is a diagonal matrix again and what is the diagonal student f t f t right so i am going to represent it as d of ft is that fine so remember that the original equation had three terms all of these the last blue onc e for all identical so this is not problematic because this is a directly the last layer this we have already derived a form this is sum diagonal and now for each of these we have a form do you get that these are the three paths that we have done so f ar so let me just substitute them this is what it looks like ok now this is a product of diagonal matrices what will the product look like student diagonal matrix a diagonal matrix and each element would be each element on the diagonal would be a s tudent product a product of all those things right so is it fair if i write it as this right which i can write it as this ok now just stare at this equation and tie it back to the intuition that we developed something about the gates regulating the fl ow of information you have a multiplicative term here right whenever there is a multiplicative term we have a problem because remember these gates are between zero to one so there is a chance of vanishing everyone sees that you are multiplying t terms al l of which are between zero to one so there is a chance of vanishing but i am going to end this proof by saying that the gradient does not vanish so by what am i going to do now ok make the statement the gradient could vanish but this kind of vanishing is fair what do you mean by that now when will the gradient vanish student product at this product of the forget gates vanishes but if the product of the forget gate vanishes that means what would have happened during the forward pass that informati on was not carried all the way back all the way front two times step t right do you see that ok so that is the main reason here right so the red term does not vanish the red term time zone vanish the blue term can vanish but it will vanish only if d uring the forward pass also this multiplicative term at cause the information to vanish by the time you are reach the time step t how many if you get this and this exactly what i meant earlier by saying that if during the for ward pass s t did not contribute much to s t one  because the forget gate was tending to zero then during backward pass there is no need to pass this information back to s t right because during forward pass you did not contribute so during backward pass why sh ould i hold you responsible right and this is absolutely fine to do this and this is exactly what the equation tells us that they gauze the gradient will vanish only if things vanished in the forward pass ok and the gates are doing the same regulation in the forward pass as they will do in the backward pass so everything is fair is that ok and does there exist one path along which the gradients will not vanish when they do not need to vanish so if during forward pass all the gates were on that means the information from state one was actually carried all the way up to state t then during backward pass what will happen the information will go all the way back right is that fine so the gradients flow back only when required as regulated by the forge t gates and this is fair because if you are regulating the same thing in the forward as well as the backward direction then you are not doing anything wrong ok now that is a proof for lstm solve the vanishing gradient problems or in other words the gradients vanish only when required and not unnecessarily or arbitrary as is to happen in the case rnns now we will show there exist one path along which the gradients can explode right so let us show that path so consider this path ok a this path is again also active so i if we consider the path to h k there is going to be active for all the gates and all the states right so in whatever gates or states you are considering this paths would be there and this is what this path looks like you have the derivative with respect to the last layer and then you have these guys ok these pairs h t by o t  o t by h t one and so on everyone fine with this so far w hat is the derivative of h t with respect to o t  we do not remember the equations s o i will just tell you directly so based on whatever we have done so far just trust me that this is what each of the terms in the bracket looks like we can go back and check this this is just comes directly from the equations now what is happening he re does this look very similar to the situation that we had with rnns we had a diagonal matrix and a weight matrix and a repeated multiplication of these right and again the diagonal matrix is bounded the weight matrix is bounded so now the repeated multiplication could explode is that fine so it does not solve the problem of exploding gradients but it solves the problem of vanishing gradients but now still this is bad for us right whether the gradients explode or vanish our training is going to g et messed up so how do we deal with this for exploding gradients what will you do student refer time twentytwotwentyseven clipping we will do student clipping clipping right so in practice the way of dealing with this is gradient c lipping if the norm of the gradient exceeds the certain value then we are going to just clip it to a certain threshold and this is fine because we care about the gradients only for the direction and not for the magnitude anyways when we introduce a lea rning rate we are doing some kind of scale down for the gradient magnitude so this is just being more explicit and being careful that if the gradients are non manageable in terms of their magnitude then we just going to keep them to some manageable val ue while being faithful to the direction and the direction is what matters so that is why exploding gradients is easy but in the case of vanishing gradients you do not have direction also because the entire gradient becomes zero so there is no direction there right so that is why vanishing gradients is more serious than exploding gradients and as long as lstm solve that they are fine with it is that fine ok so that is the end of this lecture"}
{"audio_filepath": "Data/Preprocessed/Learning Paramters of Feedforward Neural Networks (Intuition)_24.wav", "duration": 372.0, "text": "nowwe will move on to the nex t modulewhere we wa nt to lea rn the paramete rs of feed forwa rd neura l network s and we first star t with some intuiti on a nd then mathematical details so we have introduced feed forward neural networks and we are now interested in findi ng an algorithm which can allow us to learn the weights of this network so recall our gradient descent algorithm this is how it looked ok i had initialized those two parameters w naught b naught and then i was iteratively doi ng this in a loop at every step i was moving in a direction opposite to the gradient at that step now can i write this a bit more compac tly we can write using vectors so are you ok if i write it this way so these two was actually no thing but vector at every point so i can just write it this way so theta is the vector containing w and b ok or theta is the vector of all the parameters my network had it just so happened that network had only two param eters  so see wher e am going with this how many of you see where am going with this g ood so where delta theta t right just to remind you it was this the partial collection of all the partial derivatives with respect to all the parameters in this toy example all was equa l to two right we just had two parameters now you see where am going with this ok so now in this feed forward neural network instead of theta equal to w comma b what do we have theta is equal to so many parameters ok so what would grad of theta t now be partial derivatives with respect to student refer time onefiftyeight all the weights but there is a problem here right this is the matrix how do you take the partial derivative with respect to the matrix who asked you to use the matrix how you take the partial derivatives with respect to matrix so what i am interested in this right the question i know there is some loss function which is a function of theta one of the elements of theta has this matrix w one which belongs t o r n cross n right and n ow i want the derivative with respect to w so see what i am trying to do this is scalar and we take the derivative of that with respect to a matrix what is all that  t he derivative with respect to student refer time twothirtyseven every element of the mat rix ok so we can still use the same algorithm except that del this grad of hat of so now i could just say that theta two hat i mean initialized all parameters and theta naught  right compute the gradient with respect to all of them and then do this update right i could just instead of putting them in matrices i could just think of them as a large vector just had initially i had just had w comma b now this vector is even more large in fact i will show you actually how it i s so this is the grad with respect to theta looks very nasty now this is how nasty looks right so you have this weight matrix w one you have the derivatives with respect to first element of w one all the way up to the last e lement last element  so with respect to all the n cross n elements of w one what is the n ext entry going to be w two hundred and eleven to student refer time threethirtytwo wtwonn next after wlone one ok and then after this o k student refer time threefortyone what is remaining biases ri ght so you have bone one to bone n this slight error here but intentionally this actual is k because k is not equal to n right the last layer has only k parameters whereas so that it looks ok i s this clear so is this are all the partial derivative that we need right you do not need to worry about taking a partial derivative with respect to our matrix it just boils down to taking the partial derivative with respect to all elements of the matrix so earlier you just had two parameters now you have these n cross n plus n cross n up to l right so l into n cross n plus l into n that many number of parameters is what you have you get the calcula tion right or rather you have l minus one layers each of which has n cross n parameters right and l minus one layers which also have the biases so these are the ws these are the bs then the output layer one layer which has n cross k parameters and k cross one bias so these are all the number of parameters that you have and this is exactly what this size of this m atrix is right it has all these parameters and you need to compute the partial derivative with respe ct to each of these parameters so this is what grad theta is composed of it is composed of the partial derivatives with re spect to all the parameters of your network ok so now if someone gives you each of these quantities same oracle give you each of these quantities then can you apply gradient descent r ight y ou can use the exactly the same algorithm that you are using e arlier just the sizes of earlier vectors changes how many of you are convinced that now you can use that gradient descent there is not a trick ques tion how many of you convinced  how many of you not convinced assuming that someone has given you these q uantities right i know that it is hard to compute we will see how to compute that but let us assume someone has given you this then you can use gradient descent that is what the case i made in the previous slide right that you could initialize with all the parameters compute the gradients with respect to all the parameters and just do this update fine so now  we need to answer two questions  first is this is the key question because we are taking derivative of what  l oss fun ction s s o we need to know what the loss function s that is the crucial question right and then we are taking derivatives with respect to all these elements so whatever i was told you that assume that oracle gives you now you have to do the har d work and actually find it out r ight so if you can answer these two questions then we are done we have an algorithm for learning the parameters of feed forward neural networks we all agree that if you have these two elements then we have done so here i wil l end this module"}
{"audio_filepath": "Data/Preprocessed/Parameter sharing and tying_63.wav", "duration": 47.0, "text": "the nex t thing that i wo uld like to talk about a nd thi s quickly g o over this parameter sharing and tying refer slide time zerotwentyone so parameter sharing and tying i will just quickly go on this because for the sake of completeness it is there in this lecture  but it should it would really make s ense when i do convolutional neural network s  so for the ti me being just take my word for it that in convolutional neural network s you do a lot of parameter sharing where as the other place that you have seen parameter tying  so that i s again something that i am not going to talk about so this is typically used in auto encoders where the encoder and decoder weights are shared and that effectively reduces the number of parameters in the model which effectively reduces the complex ity on the model  i f th e complexity of the model goes down omega theta goes down because that i s what which wise man told us that tim e steins lemma"}
{"audio_filepath": "Data/Preprocessed/Convolutional Neural Networks_88.wav", "duration": 973.0, "text": "ok s o now we will go to the nex t modulethis is for the camera a nd thi s is on convolutional neural networks so far we have done what we have just talked about a convolution operation you just taken some input boxes and converted them to output boxes what does this anything of this have to do with neural netw orks i keep saying that is a course on neural networks right so everything has to link to that so what is the connection so we will try to understand this by taking the example of image classification and i will use the same trick to get everyones attention so the next few slides are going to tell you the difference between machine learning and deep learning ok so now everyone will pay attention so this is the task you have give given an image and you want to class ify it into one of k categories and i am considering four categories here car bus monument flower ok what is the simplest thing that you can do suppose this is a twenty cross twenty image you know the simplest thing student sir given on the slide you will j ust take this as a four hundred dimensional input feature vector right and you will treat it as a four class classification problem train some multiclass svm or anything on that right so you have a simple input so you are given some one million images all of these are four hundred dimensional and they come from one two three or four these are the four classes which is car bus monument and so on so you can just treat this as an input feature vector and do your classification right that is the simplest thing that you do or else what you could do is you could do some kind of feature engineering right you could say that actually this entire blue sky is not really helping me in deciding anything these entire green lawns and all this is not helping if mo nument car bus and flower are the classes what i care about is the shapes i do not care about the details inside the shapes i am not trying to decide whether the car is of a blue color or what model the car is and so on right all i want to see is tha t this particular shape of a car is present or not now what kind of filter gives you the shape of the image student refer time twosixteen edge detector right so i could use edge detector so now this is something that i have used based on my domain kn owledge that for these four classes actually just detecting the shape is important so i will ignore everything else so there is a lot of details there right so i have actually sparcified my entire input i have just looking at the edges in the input a nd now this is a better refined feature as compared to the earlier feature how many of you agree that this is a more refined feature representation right but this was handcrafted i actually hand coded the edge detector kernel because i knew that it is eight at the center and minus one everywhere else right that is how i thought of it that that is what an edge detector is or at least i read about it somewhere right so that is how you would do it so this is feature engineering so what is this this is how you do machine learning right you take an input you do some feature engineering and then you train a classifier on top of that but now you could become even more creative with the feature engineering and that is what the computer vision community was doing largely before two thousand and twelve come up with different ways of capturing better and better features from images so too popular in features from that era and that is i am just talking about two thousand and twelve not some like five hundred years back but from that era was sift and hog features which actually tell you how do the gradients of these pixels change across the image right so this is again try to capture something like how what is the variation in the image from pixel to pixel right so that is the essence that how is y ou do not care about these entire blue patterns because they are just telling you sky it is redundant right if you have seen some ten pixels or twenty pixels which has sky you know that a large part of it is going to be sky so these try to capture some abs tractions from the image and these are better than the edge detectors and these features were extremely popular so what you would do is you take your original input this is a deterministic algorithm you apply the hog algorithm or the sift algorithm and it gives you a transformed representation for the image and you can use this transform representation to do classification and a lot of work prior to two thousand and twelve two thousand and eleven show that these features work extremely well across a wide variety of across a wide variety of image tasks ok so again what was happening here this was feature engineering because someone realized that what i care about is this gradation in the input images and i can capture this by this nice algorithm called sift or hog of course someone ca me up with that algorithm but it is still kind of feature engineering right so this is how the learning is to happen is you are given some input you do a static feature extraction no learning so feature extraction is deterministic you take the input pass it through one of these algorithms either the edge detector or the blur detector or sift or hog and you get some representations for the input and the only learning that happens is on top of this transformed input so you now have a transformed in put and on top of that you are going to train a classifier and you are going to learn the weights of the classifier so the only thing that you learn is the weights of the classifier so that is equivalent to learning only the soft max layer in case of a feed forward network that is the output layer right now instead of using these so this is the question instead of using these handcrafted kernels or features such as edge detectors or blur detectors or what not can we lear n meaningful kernels in addition to learning the weights of the classifier do you get this question at least whether the answer or not but you get the question so what i am asking is that why should i hand code this edge detector ok refer slide tim e fivefiftyfive why not have after what is the edge detector it is like a three cross three matrix right and i have seen tons of such matrices in my feed forward neural networks i have dealt with very large matrices which were called parameters of the network so why not have a three cross three or a five cross five or whatever dimensional matrix and try to learn what should be the right values in this matrix instead of hand coding the edge detector matrix do you get the idea how to do that as still far but at least do you get the idea that is what i am we are trying to do ok so now instead of just learning the weights of the classifier we also want to learn the weights of the kernels that means instead of using handcrafted features i am now going to student refer t ime sixthirtyseven learn the features so that is the difference between deep learning and machine learning so you had handcrafted features there and now you are going to learn the representations also by treating them as additional parameters in your network how you will do that we will see and it is very simple given that you understand how to train feed forward networks but then why the stop there why just have one feature representation for the input can i learn multiple su ch kernels right i could have one three cross three matrix which learned this first representation another three cross matrix which learned this another representation and yet another three  three or five  five or seven  seven matrix which learns this different representation so instead of learning one static representation from the input i could learn multiple representations from the input in fact why not why just stop t here what is the next thing that i am going to try to do multiple layers of features right so that means at the first layer i learned this representation now i could take this and try to learn an even more abstract representation and then keep doing this to make it deeper and deeper do you get this ok so at every stage now i have these parameters which are helping me learn the representation of the input i am learning multiple representations at every layer and then having multiple layers of the se representations right and everything is learnable end to end ok so you get the difference between deep learning machine learning now there is no handcrafting of features you are learning the feature representation i know i understand there is some confusion about how you would do this but we will get to that just trust me on that that you will be able to figure out how to do this ok and all of this we have some weight matrices here we have some weight matrices here t hese are the layer one weight matrices the other layer two weight matrices and these are the output layer matrices and you see this layer wise arrangement of these weight matrices and they are very comfortable with this because we have done feed forward neu ral networks where we had these multiple layers and we knew how to back propagate from the last layer to the first layer now what i am trying to say is that i would like to adjust these weights of filters in such a way that my classification loss is m inimized and what is the loss function that i am going to use here student refer time eightfiftyone cross entropy because this is a multi class classification problem ok so just hang on with this intuition and we will make it more clear fine so such a network which has these multiple convolution learned convolution operations at every layer and then multiple such layers is known as convolutional neural network ok fine so get this idea that we need to learn kernel filters by just treating them as parameters of the classification model ok but how is this different from a regular feed forward neural network you could have taken a regular feed forward neural network and i will show it to you on the next slide and what is the difference between that and a convolution operation so if you understand that then you would be done for this lecture yeah so we have an input so let us say now i will take back the eminis t case where you are given an input as an image and these are digit inputs and you want to classify them into one of ten inputs and i am going to assume that my input is four cross four that means i have sixteen pixels ok so the simplest thing that i could have done or the feed forward neural network way of doing this is that i would just flatten out this image i will get sixteen inputs i need ten outputs at the output layer so i have an output layer which will have one of these ten classes and then i add as many l ayers that i want in between ok this is what a feed forward neural network would look like and if i consider any one neuron in the first layer it takes inputs from all the sixteen inputs right that is how a feed forward neural network is you have these extr emely dense connections where every output depends on every input at every layer ok now so this is the same story which i have said now let us look at what a convolutional neural network looks like so again you have th ese sixteen inputs i am using a two cross two convolution ok now if i use a two cross two convolution if i place it here then i am using pixels one two five and six and computing one value so you see the difference between this and a feed forward neural network in a fe ed forward neural network h eleven would have depended on student refer time eleventhree sixteen values sixteen inputs and a convolutional neural network it is depending on student refer time elevennine only four only four neighbors ok and similarly h twelve  i am using a stri de of two by the way right so i am not placing the filter here i am just skipping one step h twelve would depend on pixels three four seven eightok and so on right so one thing is clear that as opposed to a feed forward neural network you have sparser connections here  is that clear and why do we have sparse connections because we are exploiting our knowledge about images that in an image you do not really care about the interactions between on between a pixel at the leftmost left top most corner and the right botto m corner right so there is sky here there is ocean here or there are trees here you would want to capture the neighborhood around that pixel not really capture it with the entire image that is why you do not want all of these sixteen inputs to contribute you only want a small neighborhood to contribute do you get that intuition ok so this is the first property of a convolutional neural network that it has sparse connectivity ok but its sparse connectivity really good i ju st made a case for that now i am going to counter argue right is it really good that you have these sparse connections because you are losing out information right you are using out interactions between certain pixels so why can why is that good st udent refer time twelvetwentynine i am hearing a lot of interesting answers but remember that you are always going to have multiple layers ok so consider these two pixels in the first layer these two pixels did not interact because h two only dependent on these three and hfour only dependent on these three there is no a there is no unit here which depends on both xone and xfive is that obvious because i am just using a window of size three but now once i go to the next layer once i go to g three  g three depends on h two  h three  h four here which in turn depends on x one x two x three x four x five right so even though at this layer x one and x five are not interacting with each other as you go deeper these interactions become obvious do you get that right so that is why you will always use a deep convolutional neural network where all the pixels get to interact at a deeper layer but at the more image it layers you just want to capture the interactions between a neighborhood so it is like you take this neighborhood find out something then take neighborhoods of n eighborhoods and then try to find out something at the next layer and keep continuing in this layer how many of you get this right ok so this is what sparse connectivity looks like another chara cteristic of cnns is something known as weight sharing so let us see what it is so remember i had considered this two cross two kernel and i was placing it at these four pixels which is pixels one two five and six and i was pacing another kernel at these four pixels which is pixels eleven twelve fifteen and sixteen right these four pixels and i have used different colors for them indicating that these filters are different so they are both two cross two filters but i am assuming at the values inside them are different does this have t o be the case just think what a filter is trying to do student refer time fourteentwentyseven it is striding across the entire image at every locatio n i want to do the same operation remember when we are doing blurring or edge detection or sharpening i had the same filter which i was applying at every location so i want to see what is the effect of this filter throughout the image so i do not really want to change this filter that means these four weights would be the same as student pink weights the pin k weights how many of you get this so this is a question do we want the kernel weights to be different for different portions of the image so imagine that we are trying to learn a kernel that detects edges so the same kernel configuration is requir ed throughout the image because that is the kernel configuration which will detect an edge so you want the same kernel to be there everywhere so we are going to share these weights they should not be these pink and orange weights we will just have the same weights everywhere ok so this is known as weight sharing so now this is something ridiculous if you think about it now how many weights do i have in layer one student refer time fifteenthirtythree four weights that is all tha t looks too less right it would lead to student refer time fifteenthirtynine dash fitting student refer time fifteenfortytwo under fitting because we have very few parameters so how do i deal with the situation student refer time fifteenfortyfive you will have multiple kernels right so you will have another kernel which takes something else you will have one more kernel which takes something else and you can have as many such kernels right so the more the number of kernels will have you will have that many into four as the number of parameters and that many outputs at layer one how many if you get this ok good so these are the two important characteristics of convolutional neural networks one is sparse connections and the other is weight sharing ok refer slide tim e sixteenseventeen so so far we have focused only on the convolution operation now let us see what does a full convolutional neural network look like or maybe i will do this next time i think this is"}
{"audio_filepath": "Data/Preprocessed/Guided Backpropagation_97.wav", "duration": 247.0, "text": "so we will see wha t guided backpropagati on is so idea here is a bit hacky a bist heuristically but it still works very well so let us see what it is right so suppose you feed an input image to a convolutional neural network that image will go through all the convolution layers and say it one convolution layer thi s is what your feature map looks like i am operating at a very small scale i am just considering a two   two feature map ok now we consider one neuron in some feature map at some layer ok so we will consider this particular neuro n and we are finding interested in finding the influence of the input on this neutron so this is what i will do is i will set all the other neurons in this layer to zero because i do not care about them i only care about this particular neurons i just focus on that and we now back propagate all the way back to the image right that means i will compute if i call this as h two then i will compute  h two   i one i two i three and so on ok now recall that during forward pass what happens is because you have relu neurons any output which was negative that was clamped to zero in the forward pass any output which was negative was clamped to zero so what would happen to th e gradients when they flow back through those neurons you already did this if an relu neuron is dead the gradients do not flow back right so the gradients will not flow back through these neurons that means that only the so only these gradients wil l actually flow back which correspond to non negative entries in the image before it or in the matrix above it right is that fine so now these guys use this interesting idea that in the forward pass you dont allow negative things to go forward so th e backward pass also do something similar dont allow the negative influences to go back that means any gradient which is negative just clamp it to zero ok so what i am going to do is all these negative elements in the gradient i am going to set them to zero you see that so this is just taking the same idea which you apply that forward propagation that relu clamps the output to zero if the influence was negative and the backward pass also do the same any gradients which are negative just clammed them to zero so the intuition here was that maybe there was a pixel which is really influencing the particular neuron and it stands out but because there are some positive and negative gradients flowing back they seem to cancel each other and all these influence s tend to be zero because thats what we observe that image was largely gray with very few non gray pixels so this is very heuristically because the reason i call it a heuristic is because you are messing with the math right the math tells you that the correct gradient has to go back irrespective of whether its positive or negative but they give this justification that on based on two things and the forward pass you are not passing the negative gradients a negative outputs so in the backward pass also kill them and this should avoid this canceling of positive and negative output so this is known as guided back propagation because you are meddling with the actual back propagation you are doing something different and s o the idea was to neglect all the negative influences and when they apply this guided back propagation this is what the influence looks like so you see that it is much sharper now it is actually very nice its focusing completely on the eyes and you can see the layout of the cat much more clearly as in the earlier picture earlier image right so this is a popular technique to use to for various things it is also among other things for in for understanding what your convolutional neural network is doin g right so this lecture is entirely about understanding what are the neurons learning what are the weight matrices learning what are the kernels learning and so on so these are all again tricks that you need to have in your repository to be able to d o something more than just reporting accuracy ok i will get seventy percent accuracy on this status refer slide time fourfifteen right so this guided back propagation is one algorithm that you will implement as a part of the assignment so"}
{"audio_filepath": "Data/Preprocessed/Contrastive estimation_81.wav", "duration": 380.0, "text": "so we will move on t o the next way of dealing with the expe nsive softmax so remember that so thi s is known a s contrastive estimation so remember that this is where we are in the story that we saw the bag of words model we saw the skip gram model and we saw that bo th of them have this expensive s oftmax computation at the end and that is the problem we are trying to deal with so we saw one way of dealing with which was negative sampling so you i hope you saw that there was no expensive computation there the only computation there was the dot product between the two words which appear together or which do not appear together now let us see what happens in contrastive estimation so here again you use a same idea so you have a positive sentence or a positive example he sa t on a chair you create a negative sentence which you replace the word by some random word now you construct a feed forward network like this which takes these two one hot representations basically uses your word context matrix to give you the summation of these two representations right that is exactly what we have done in the skip gram model now you have this hidden representation which is the sum of the two word representations now from here on instead of doing this s oftmax computation which we had ea rlier we just predict a single score ok we just predict the score for this word pair being of correct word pair we do the same thing with the random pair so we take sat we take abracadabra and the add up there word representations you get this hidden representations and you get a score sr fine so what is the output computation right now what is the is it a matrix operation is it a scalar operation is it a vector operation what is this h is equal to  we need to change this to k on the slide plea se note  so what is this product w into h just a dot product between two vector w is just k cross one that means it is a vector so as compared to k cross v earlier we just have k cross one you get that how many of you get this we have a very simple com putation at the end ok but now how we set up by loss function earlier i could set up the loss function as maximizing the log like it of the correct word but now i just predicting two scores so what is the loss function what should i try to intuitively do and today we are going to see a new loss function which we have not seen earlier so try to think about this what would you do forget about the math forget about the machine learning all that what would you actually want what is your wish list t hat should be easy to characterize score s score sr do you want this or this first one right you want s to be greater than sr can you think of making an objective function out of this you want to maximize student refer time threeeighteen s minus sr fin e that is a good starting point so would you be happy with this what would you want this or this both cases s is greater than sr right what would you want student a big margin a big margin fine so we would like sr to b e greater than s and not just so we could try to maximize s minus sr ok but we would also like this difference to be a certain margin that means i would want s to be greater than sr by at least a margin of m and that m is something i will decide so i could say that it should be at least ten points greater than sr or one point greater than sr depending on the scores that i have so all my scores are between zero to one then probably a margin of zerothree or zerofour is ok sounds reasonable right that means s could be zerosix and sr could be zerotwo does that make sense so what i am saying is what i am trying to say is that this is my sr i want s to be greater than sr i am not just happy with that i am saying that even if i add a margin to sr even then t his condition s hould hold right and that is the same as saying that s sr and there should be at least a margin of m between that that is the difference that i accept i am not if you tell me that s is zeroninetynine and sr is zeroninetyeight where then you are not really distinguishing muc h i want at least s to be zeronine and sr to be at least less than zerofive or somewhere so there should at least some gap between that and that gap is m so instead of maximizing s minus sr i am going to maximize s minus sr plus m is that fine o k now suppose you are at some point of training i will have some need some parameter configuration that means you have learned some values for vc and vw and you do this forward propagation compute s and sr and we actually find that thi s condition holds right so right now my loss function is this at some point you are doing this and you observe that this condition holds that means s is actually greater than sr plus m  in that case what do you want a loss to be how many of you get the question i want that s and sr should be separated from a margin of m in the favor of s i am doing my training i am at certain configuration for uc s and vw s and so on i pass it through the feed forward network and i get s and sr and i observe tha t this condition already holds is my network doing anything wrong at this point it is doing it is job properly what should be the loss that i back propagate zero a gain gets that there is nothing to correct here i do not need to back propagate any loss so then can you give me the full objective function maximize this but at this condition already holds then do not do anything is that fine so that is about this so and again observe that we have gotten rid of the expensive s oftmax computation"}
{"audio_filepath": "Data/Preprocessed/Skip-gram model_79.wav", "duration": 614.0, "text": "so with that we will move on to the next model i am still not telling you how to solve this problem we will come to that later  i am just going to the next model which is the skip gram model ok and this is the famous wordtwovec which you are trying to implement the model that we just saw was known as a continuous bag of words it predicts the output given the context skip gram model does the reverse of that you are given a word you want to predict all the context words so now i am given the word on i am trying to predict the words which appear on the left and right side of it is that fine so how many prediction problems am i solving how many predictions am i giving you four in this case right so you see that this is a case where your y actually belongs to r four right of course it is not r four it is four into v and because you are predicting the entire distribution but what i meant is that you want these four different outputs you just do not want one single output apart from that does everything else remain same you have an input word you compute a hidden representation from that hidden representation you try to predict the outputs you get a probability distribution what is your loss function it is a dash of cross entropies sum of cross entropies how many cross entropies do you have four in this case and also notice that i have i hope i have yeah i have changed w word and w context they are flipped now is that fine the role of context and the word has changed in the simple case when you are trying to take one word as the input and only predict one word around it it just becomes the same as the first case that we saw in the continuous bag of words there is no difference there because there also you take one word and predicts the other word so the entire math s remains the same how many of you get that and even when we have multiple context words our loss function is just going to be the sum of the cross entropies for all those d predictions that i need to make or d minus one predictions that i need to make and then once i see a loss function which is a sum of some things i am not worried because i know how to deal with each of these components and gradients are additive so if you have the gradient of some it is just the sum of the gradients so as i long as i know how to deal with one of these i can deal with the sum so that is why i do not really worry all of you are at that level where you do not worry with the sum as a loss function what are the problems with this already written there same as the bag of words right now  we are doing these four expensive computations at the end so the softmax computation is expensive there are three different solutions and there are three different ways that we can deal with it one is something known as use negative sampling the other is to use contrast of estimation and the third is to use a hierarchical softmax so we are going to see all of these and i will shamelessly continue for a few more minutes so first we will see use negative sampling because that is very easy  so let d be the sat set of all correct w comma c pairs in the corpus what do we mean by that all words which actually appeared in the word comma context pair so you can look at the vector  which i have constructed so sat on sat or sat chair you can imagine that all of these appeared in the context of each other  so this is my correct corpus as from what i got from my data now  let d prime with a the set of all incorrect w comma r pairs in the corpus and r here stands for random some how am i going to construct this corpus so i take a word i know all the words which appeared with it and i know all these other words which have not appeared with it so i will randomly sample a word from there and put it as r is that fine so i can compute d prime again d prime comes for free d was always for free now d prime is also obviouslyfor free so i have w comma c and w comma r and i have these corpora d and d prime and as before let v w be the representation of the word and u c be the representation of the context word ok so v w will try to these two and you see will try to this and u r something else that we will use for this hopefullyis that fine okay  now  for a given w comma c which belongs to d which is the true corpus what are we interested in maximizing so let us think of z is a random variable weather which tells us whether this is a true pair or not so given w comma c i want to maximize that p of z is equal to one ok now  this is what i want to maximize now it depends on me how do i model this probability  so can you guess how am i going to model this the answer is there in the figure can you tell me what is the model that i have chosen can you tell me what is the formula for z equal to one given w comma c that i have chosen this stands for dot product this stands for the sigmoid function i know this is some uc representation this is some vw representation note that these representations are not learned yet i need to learn them using the training objective that i set but at the beginning they are initialized to some random values and the way i am going to model probability of z equal to one given wc is that i am going to say that it is just the sigmoid function of the dot product between them how many of you get this are you comfortable with this this is the modeling choice which i have made or rather the authors of skip gram right now how am i going to now what do i want to do for all w comma c belonging to d i want to maximize this probability is that fine for all the w comma c pairs which belong to my true corpus which is the d corpus i want this probability to be high how many such pairs do i have many many right so let us call them as i have n such w comma c pairs so can you tell me what my loss function is going to look like maximize this for the first pair and for the second pair and for the third pair all the way up to the end pairs so what is it going to look like for every w comma c which belongs to my correct corpus i want to maximize that probability of z equal to one given that w comma c pair right and since it is an and i will have this product how many of you are comfortable with this so this as such and this is something that you do regularly you should have done this in machine learning or pattern recognition or somewhere right that you want to basically maximize the log likelihood of the data which is saying that you want to maximize the probability of every training instance which is saying that you want to maximize the and of all these probabilities right be you take the and of all of them is that fine now  for the other case wr belonging to d prime what is it that i want to maximize this probability right because i know this is an incorrect pair so i want my random variable to output zero ok now  what is this going to be the probability one minus the probability  that it was correct and that actually if you just simplify a bit it turns out to be this now for all the elements which belong to d prime what is the objective function that i have i want to maximize this for the first w comma r random pair for the second w comma are random pair and so on for all the random pairs in my corpus so it is just going to be a product of all these probabilities is that fine so nowwhat is my total objective function for every pair in d maximize that for every pair in d and for every pair in d prime maximize the zero probabilityso what is the total going to be is this fine how many of you agree with this so for everything belonging to d i had this and rule for everything belonging d prime i had another and rule and i am interested in both the acts right maximize for d and maximize for d prime of course different quantities for d and d prime ok fine so you get this once you basically take the log and so on so this is a simple set of math operations that i do you will end up with this neat formula ok that for all the w comma c pairs belonging to d you want to maximize this quantity which means you want to maximize what you want to maximize the when will this quantity be maximized when the dot product between the two is high that means again what are you doing we are trying to bring the context vectors close to the word vectors again transitively what will happen the words which appear in the same context will go close to each other  what is the additional thing that you are ensuring here the words which do not appear in the same context you are trying to push them apart why because of second loss function you see the difference between the two now in the first case you are only opt i mean you are obsessed with bringing things close together  here you are also focusing on the case that where you do not want certain things to be close together because they never appear in the same context is that fine so you see that this is a more powerful loss function in the earlier one so that is what the skip gram model does and in the original paper mikolov et al sample k random pairs for every positive pair right so that means if your size of d was n what was the size of d prime b k into n so that they had that many positive examples and k times that the number of negative examples and this was a hyper parameter which was tuned and they used a value of k such that it gave them the best results also remember that we have this problem of constructing w comma r now i said that consider all the words which do not appear with your word and sample from there and put something there so they used a slightly that means how do i sample that one is i assign all the words a uniform distribution that every word is equally likely  what is a better way of doing that okay i think i just finished this next time"}
{"audio_filepath": "Data/Preprocessed/One-hot representations of words_74.wav", "duration": 521.0, "text": "so today we are going to talk about learning vectorial representations for wo rds so these are the acknowledgement slash references for where are the things that i have referred to by preparing for this lecture so you can just go this some of these are also available as video lectures on youtube can t ake a look at them also i n the first module we are going to look at one hot representation s of words so as usual we will start with this motivation or motivation motivating question why do we need to learn representations f or words v ectorial representations for words when words are there right you can write them using alphabets and characters and so on so why do we need vectorial representations mention whatever you have seen so far in the course i have seen anything like this let us see so suppose you are given an input stream of words and it could be a sentence or documented if i say documented pretty much covers almost all the text that you see right you can always abstract everything as a document and email is also a document a manuals are also documents and so on and we are interested in learning some function of it and so i am given a document and i am interested in the function y hat say y hat is equal to sentiments of the words in the document or the s entiment of the document itself this is imaginable this is not something that i am cooking up this is something that you would want to do you would log on to for example if you are a movie maker you would want to know once the movie is released people h ave written reviews about it what is a sentiment is positive or negative similarly if apple has released a new product or a new feature you would want to know what are the reviews written about this product and what is the feature what is the sentiment c oming out of it is positive or negative now sentiment is a binary thing or it could be rated also right it could be on a scale of one to ten also but let us consider it is binary that either people liked it or did not like it so now i am trying to lear n this function which gives me which takes as input words but as output gives me real numbers either zero to one or on a scale or one to ten or whatever right and this is not something that we have dealt within the course so far let how do we take as input wo rd so all inputs have already always been numbers right they were either coming from rn or they are coming from zero to one raise to n or something of that sort we never had the situation when we have words as written so right so now how do we deal with the situation and also i have made a case for that learning this function is a valid thing to do you have several news cases where you will need this so now if we employ a machine learning algorithm that some mathematical mo del  so we saw that we could have several such models logistic regression svm and neural network and feed forward neural networks and so on right and at the end we are trying to learn this function y hat is equal to f of x but in our case the x instea d of have be instead of x being numbers it turns out that x is actua lly a collection of words so now how do we reconcile with the situation where we have suddenly have words instead of numbers so the way to do that would be we need a way of converting t hese words or documents into some number into some vectorial representation and once we have this vectorial representation right so now we have r r raise d to n and we know how to deal with r raise to n given r raise to n how to predict r or even r sq uare or rm in general that we know right we can design neural networks or any other machine learning algorithm should do that but how do we go from here to here that is the question right and that is why we need to learn vectorial representations of w ords this is a motivation clear to everyone okay n ow let us start getting with a refer time threefiftytwo how to do that right so now we will start hearing this word corpus have you heard this word before that is exactly what you are collecting for the word to like assignment right you are collecting a corpus in specific languages and you have taken a very toyish corpus for the purpose of illustration so here is a corp us it just contains four sentences right so think of it t hat i have a very restricted domain i have very small set of documents and i just have these four sentences with me this is the valid corpus the corpus that you have constructing is probably much larger scale you are trying to collect one hundred thousand sentence s or fifty thousand sentences or something of that order right ah but we will take this toy example now consider set v of all unique words across all these input streams so i just call them input streams by input streams i mean sentences or documents o r whatever right you could take it as any sequence of words and v is set of all unique words across all this input sentences that you have so can you tell me what v is here what would can you tell me some elements of v of the set v human machine inte rface and so on so that is why in fact this is the entire set v which is written on the left hand side right and v is called the vocabulary of the corpus so that means everything in the corpus comes from this vocabulary all sentences are constructed by arranging words from this vocabulary some of you might always i mean find this very trivial but i am just going over the basics so that at least the terminology is clear to everyone and what we want is a representation for every word in v so tha t is the title of the lecture learning vectorial representations of words so for every word in our vocabulary whatever corpus we are dealing with the vocabulary would change and for every word there you want to learn a representation for that word so that is what our quest is today ok and now one very simple way of doing this is right you tell me you want a vector and that is all you care about here is a vector i will give you one hot representations so if a total number of words in my vocabulary is v i just construct a vector of size v ok and i have assigned a number to ev ery word in my vocabulary right so i will say human is equal to zero machine is equal to one interface is equal to two and so on and if you ask me for a vectorial representation of that word i will just say take this the or vector of size v and switch on the corresponding bit and anything else would be zero hence one hot right at any given point of time only one of the elements in the vector would be on so that is a simple one ho t representation as this is a very simple recipe to get a vectorial representation of words and for every word in your vocabulary now what is the drawback o f this v tends to be very larger right so for example there is a standard corpus known as the penn treebank corpus which is used in various nlp applications for various reasons and that corpus has a vocabulary of fifty k google of course operates at it is own scale so they have a word one t corpus which has thirteen million words so this is like all the most of the web pages that they have dr awn constructed a vocabulary from that so now i am talking about for every word representing it by a vector of size thirteen million theory does not good work right there is too much of s torage required for that and if you lo ok at that information in it is so redundant that is all zero except for that one bit which is on and the other important problem is that these representations do not capture any notion of similarity other three words that i have shown you which do you want to have similar representations cat and dog why because both of them are domestic animals right both of them are mammals so there are some things that you would want at least at the minimum that the similarity betw een a cat and dog is more than the similarly between cat and truck or alternately the distance between cat and dog is less than the distance between cat and truck so now once i start talking about vectors i can talk about similarities like cosine simil arities or i can start talking about euclidian distance so once anything i convert it to a vector i can start asking these questions other two questions which i am asking are valid right what would you expect to be the euclidian distance between cat and d og as compared to cat and truck now what happens with the one hot representations take any two words in your corpus any two what will be the euclidian distance what wil l it be square root of two right for all the words take any two words in your corpus wh at will that cosine similarity mean zero because all these vectors are orthogonal  right so the cosine similarity is going to be zero but this is that means these vectors are not really capturing any information abou t the essence of the word right so re member always we are interested even like that has been our philosophy right from auto encodes and so right or even principle component analysis  they are always interested in learning meaningful representations which capture something fundamental about the entity that w e are trying to represent right b ut here something like that is clearly not happened ok so that is not acceptable"}
{"audio_filepath": "Data/Preprocessed/Sparse Autoencoders_55.wav", "duration": 507.0, "text": "so in this module we will talk about sparse auto encoders refer slide time zeroseventeen j ust some concepts before we jump into the actual way of doing this so hidden neuron with sigmoid activation will have values between zero to one and you say that the neuron is activated when th is output is close to one and it i s not activated when its output is close to zero ok n ow a spare encoder tries to ensure the neuron is inactive most of the times what is that mean s tudent c lose i t i s close to zero for s tudent m ost of the m ost of the s tudent refer time zerofortyseven i nputs right so i am passing a lot of inputs to it it will try to ensure that i t i s close to zero for most of the inputs  so in other words what does it trying you ensure i am looking for the word average t he average activation of a neuron is close to zero does that make sense is that fine refer slide time onesix so this is on what you see on the left hand side this is how you would compute the average activation of a given neuron you have all the m examples you see what the activation was for each of these and take the average righ t now if the neuron is sparse then the average activation would be close to zero is that fine this is all just different ways of saying the same thing n ow a sparse encoder uses a sparsity parameter say rho and it is very close to zero say zerofive a nd it tr ies to enforce the constraint that on average the activation of any neuron in the hidden layer should be equal to rho which is again close to zero n ow can you think of a this is all fine in plain english right you understand what we are trying to do f ir st of all tell me why does this makes sense  w hat is it that you are trying to ensure  o ver fitting happens because there is lot of dash s tudent p arameters p arameters slightly abstract it out s tudent m emorization l ot of s tudent m emorization m emor ization ok lot of freedom right i mean the weights have a lot of freedom to move where ever they want to do whatever they want to do such that they can just drive the training error to zero w hat have we done to that freedom now s tudent w e are restrict refer time twotwentytwo w e are restricting them  so any kind of regularization always tries to restrict this freedom that the parameters or the network have in general right and there are different ways of restricting this freedom y ou see that this is one of those ways right you are trying to ensure that on average the neuron should not fire  so it i s clear that this some kind of regularization any one has a doubt with that  n o now the second question is taking slightly more on this right it i s i can just move ahead and i have convince you that this is regularization b ut can you think of bit more and see what is actually being tried to achieve here what are we trying to do h ow many of you get that or at least could here that first of all only the second r o w ok  so yeah how many of you can think about this like what is it trying to achieve s tudent refer time threethirteen r ight so on average neuron is going to be inactive that means where ever it is active it i s really going to capture some rel evant information right so i t i s going to be active whenever it is active it i s going to adhere to certain patterns  so we are ensuring that each of these neurons are just a very few patterns and it has discriminative power in that sense do you get th at so now if  that means if i show it a three if i show it a two if i show it a one every time if the neuron fires when there is no discriminative power in that b ut now  if i ensure that the neuron fires only a few times it will try to fire for meaning ful l patterns  so it will try to fire for a curve or a curve in the between as you have it in the case of three right you have this cusp in the between in the middle so it will fire for some kinds of pattern so that is what the hope is it i s not just like adding some math and adding some regularization  but at least there is some intuition behind that how many of you get that intuition o k good a nd now can tell me a way of putting this everything english is fine intuition is fine  but how do convert th is to a mathematically equation y ou want to ensure that rho hat l is equal to rho there will of course be different ways of doing this the way these guys do it by adding this term to the loss function so remember your loss function is always going t o be l dash theta plus omega theta right w here omega theta does the regularization and l dash theta is your regular loss which would be the squared error loss or the cross entropy loss or whatever loss you are dealing with right so remember this term is always there but the reason i do n o t bring it up so often is because we have already dealt with it w e know how to compute the gradients we know how to do the back propagation and all that a nd now since your final loss is just a sum of these two term s i know how to deal with this and i know that gradients are additive so i just need to deal with the second term  t hat is why i am only focusing on omega theta l theta has been dealt with is that fine now this is what omega theta is why does this m ake sense  w hen would this take its minimum value when rho is equal to s tudent w att w att everyone sees that how many of you sees that p lease raise your hands ok fine let us plot it and check actually right refer slide time fivetwentythree refer slide t ime fivetwentyseven so this is how that function looks like  so i h ave plotted the function which i have written here for a of course a single k right and my rho that i have taken is zerotwo and if i plot that function for different values of rho hat l it will r each the value zero only when rho hat l is equal to point so again go back and plot this and check and it i s actually clear from the equations itself that it will be minimized only when rho hat is equal to rho l right s o  that means this is a genuine  i mean this is a reasonable thing to do we would think of other ways of doing that and i am sure you can but this is also a reasonable way of doing this refer slide time sixeight so now our last function is as i said it is going to be a combination of two values l theta is a normal squared error loss that we have been dealing with and omega theta is this sparsity constraint that you have added now you already how to calculate the first term what are we interest ed in now so you see that this patte rn will keep repeating right so now you can do whatever you want your loss function y ou have this generic frame of called the back propagation algorithm and you know that a last part of that back propagation algorithm is going to remain the same right o nly thing you are changing is the output layer or the loss function j ust need to compute something there and the rest of it will remain the same how many of you get this general idea a nd also appreciate it right t hat i s why this is a very powerful fra me right you can just make minor tweaks at the top and you are rest of the code has to remain the same so you can actually go back and try out these regularization terms in mnist assignments right if you really want to see what happens ok so now th is is what omega theta is and now what i am going to do i t can be rewritten as this that i s obvious just expanding out the law of function a nd by chain rule this is what i get now unfortunately the rest of the slide there is an error the tas please n ot e this i can kind of overlooked this ah  but i will just convey the idea right so you would want to do something of this sort everyone agrees with that remember what is rho hat i t depends on sorry rho hat l it depends on s tudent refer time seven thirtysix h of l right it i s the average activation of the l th neuron and this depends on some of the weights so that i s why this chain rule makes sense and now how to compute this t here is an error on this slide but you have done enough gradients in the c lass for me to have confidence that you can do it on your own everyone is confident that they can work it out on your own refer slide time sevenfiftyeight so i will skip this  we will fix these errors there are some summation and other terms missing here an d the second part is actually correct w hich has been derived on the next slide refer slide time eightfive but i wo uld n o t go over this this is there are the slides again go back and look at it  h ow many of you are confident that you can do this on your own p lease raise your hands yeah because we have done enough of this in class right so you can you should be able to it no if you are not able to do it then i am not doing a good job at teaching you right so you should be able to do it now fine and we will fix these errors  so ta s just remind me after the class so everyone gets the general idea you find a loss you find a constraint you define it with omega theta find out the derivative of that with respect to your parameters and just change y our gradient descent upgrade tool accordingly"}
{"audio_filepath": "Data/Preprocessed/Link between PCA and Autoencoders_52.wav", "duration": 1014.0, "text": "so we will move to the nex t module where i w ould like to show y ou a link between pca a nd auto encoders so this is what i am trying to show you that under certain conditions pca is or rather an auto encoder is equivalent to a pca and the conditions are if you use a linear encoder if you use a linear decoder if you use a squared error loss function and if you normalize the inputs to this so for the time being just ignore the last bullet let us look at the other three bullets using squared error loss functions so remember i gave you differe nt choices right you could have used the cross entropy or the squared error loss but i am going to prove this equivalence only under the condition when we have the squared error loss what do i mean the u encoder is a linear encoder g is a linear functi on we are not using a sigmoid or any logistic or anything like that and linear decoder again the same thing we are not using the sigmoid or soft max or anything at the output it is a linear function under these conditions i will show that or i will t ry to show you that pcas equal auto encoders equal to pca what does this mean actually n ow what do i mean by it is equivalent what do i have to show you actually how many of you understand what i am trying to prove how many of you can mathematically define it ok so we will try to make this clear over the next fifteen minutes first let us look at the last condition right which i ignored ok i always anticipate all this right so i have full faith in you guys ok what is thi s mean now what i am doing centering the data and i am also doing one by square root of m why mean  as the standard deviation so the operation in the bracket ensures that your data now has become zero centered right it is a zero mean and now let x dash be this matrix this one right such that all it is elements are zero mean is this still a flicker again alright so let i am calling x dash as this matrix ok so this matrix where i also have one by square root of m i can write it as everyone gets this is si mple now do you see where this is headed what would x transpose x be covariance matrix so i needed that one by m right at the out so now this is the co variance matrix so if i do this normalization to the original data and then if i take let x dash be that quantity and then if i take x transpose x then i will get the co variance matrix everyone gets this that i did this to get the co variance matrix so that i mean i did this so that when i take x transpose x i get the co variance matrix after thi s normalization only it will be the covariance matrix so first we will show that if we use the linear encoder decoder and a squared error loss function then the optimal solution to the following objective function what doe s this objective function student squared error squared error loss is obtained when we use a linear encoder do you understand the implication of this what does being stated here ok so i have fixed the decoder i have said that the decoder is going to be a decoder i have fixed the encoder or i have fixed the loss function this is going to be a squared error loss function this is given to me now under these conditions i am trying to minimize this loss function ok then i am telling you that the o nly solution to this is that the function dash should be a linear function which function the function g should be a linear function you cannot choose sigmoid or logistic or anything else right the optimal solution will occur when g is a linear function everyone gets what is being stated here so this summation that i have written right or in fact this the entire objective that i have written is actually equivalent to this objective is this fine with everyone even though i have not defined what h is just fine with everyone so we had x which was x one to x m ok i had picked one of these x i s what is the dimension of this student one cross one cross m and then i had multiplied it by a weight matrix w not w star  remember that what do the dimension of w student n n n cross k and what will i get as the output student refer time fourfiftyeight i got an h which was one cross k what did i do this student m ultiply it by multiply it by student w star w star which was k cross m and what did i get as the output student x hat x hat which was one cross n right so what i am telling you is that i could do this together for all these x i s i could do this operation at one go and i can call this as x matrix and what will i get he re h one to h two to h m and i can call it as the h matrix and i multiply it by w star and what do i get x cap ok is that fine ok but without defining these things also it was fine so it does not matter so now how many of you get that this quantity is the same as this quantity n ow how do i explain this was the frobenius norm of a matrix some of the squares of the elements now what is the matrix x it is the x one one up to x one n and x m one up to x m n and all elements in bet ween right what is the matrix h w star we just did that the sam e thing expect that it is x hat student refer time sixfour i take the difference between these two what do i get every element of that matrix is equal to this quantity that i have underli ned right so i get a new matrix such that every element of that matrix is equal to this quantity is that fine now i am taking the square of every element of that matrix and adding them up what is that equal to student refer time sixfifteen a frobeniu s norm how many of you get that now almost everyone ok so this is equivalent to the frobenius norm ok now where have you seen the frobenius norm before what did we show in the svd theorem let us try to connect things right if you do not learn how to connect things it is going to be very difficult what is this x hat it is a dash of x student reconstruction reconstruction it is a dash of x approximation what is the solution to this optimization problem what is the solution to this optimization p roblem i shall started off with the answer that we saw this in the svd theorem and then i asked you a question what thirty hours thirtytwo hours not even thirtytwo hours are passed since we did this come on what is the solution to this no no that is fine but what is the solution x hat is equal to what the best approximation to x is given by what is it fine yeah so some k yeah but it is going to come from the svd theorem right is that fine it depends on what rank approximation you want but it the best approxim ation to this is going to be given by the svd of x is it ok everyone gets that yes forgot about it but now do you remember it all those extra lectures eighto clock in the morning so that means h w star should be equivalent to this that we know from the svd theorem that the optimal solution is going to be given by svd so if i just compare terms ok then i could write that one solution is this that h h is equal to u into sigma and w star is equal to v transpose i could have chosen the other solution also where h is equal to v or sorry u and w star is equal to sigma v  ok but i will work with this particular solution you see this i am just matching variables right it is said that a b is equal to c d e so i am saying th at a is equal to c d and b is equal to e now we will work with this so and we will try to show something  so  let us see what we are trying to show now first thing that we will show is that h is actually a linear encoding so what does this mean you first always understand what has been tried to prove right i am saying that i am going to show that h is a linear encoding of x then what is it that i am trying to show i am trying to show that h is equal to a linear enco ding of x when h is of the form w x and not something of the form w sigmoid of w x or something like that or any other non linearity for that matter is the statement clear that is what i am trying to show when i say h is a linear encoding i mean that h is obtained by a linear transformation of x now h as we defined on the previous slide is equal to this now if i already had an x here then i was done but i do not have any x there yet so i want to a get to a form where i can show that h is equal to w in to x so i will just do some simple trickery and arrive try to do arrive at that form so the first thing i am going to do is pre multiplying pre multiply by this quantity and this is fair be cause this is just equal to i what next i will write these three x s as u sigma v transpose and i will leave one x as it is that now just can you just try to see what the next step would be this v transpose v will disappear because it is equal to i now what happened here i actually just expanded this inverse so i will think of this as a b c so a b c inverse is equal to c inverse b inverse a inverse so i have just applied that it just that my inverse is a very straight forward matrices here they are j ust the transform of the original matrices everyone gets this step well you can stare at for a for a few more seconds if you want how many of you do not get this how many of you get this ok now what is next this u transpose u disappears student r efer time tenfiftynine this also disappears student refer time eleventhree no student refer time elevensix it is this u is only the first k columns of u right this is not the entire u this is just the first k columns of u fine now what next a into b inv erse is student b inverse b inverse a inverse what will happen now that quantity will disappear so what do you have left now ok so this is something ok so now let us look at this is let us say this is n cross n and this is n cross k what is the output going to be student n cross k n cross k and what is the output going to look like is the first k columns of student identity the identity matrix everyone gets that if you do not you can just work it out with the small matrix after going home a nd you will get it right if so if i done the full multiplication i would have got the identity matrix but i am just talking the first k columns so i will get the first k columns of the identity matrix do not fed too much if you are not getting this you can just work it out on paper and you will get it so i get the first k columns of the identity matrix and this inverse disappears this sigma transp ose into sigma transpose refer time twelvenineteen now what next what is this product going to be the first k elements of student sigma inverse sigma inverse and that is going to get multiplied by sigma k cross k so that will give me the first k elements of student identity identity m atrix there is some very simple matrix operations where you are just ta king some columns right so if you do not understand this right now do not worry you can work it out everyone is confident they can do this please raise your hands if you are confident and now what do i finally get this multiplication will give me student the first k columns the first k columns of v ok  so  have we come to the desired form what i have shown now h is a dash of x or linear transformation of x that means my optimal encoder was a linear encoder and what was the optimal weight mat rix w the first k columns of v yeah i someone pointed it last time also i could not i ignored it i will just pretend i understood but i get it i know that there is a simpler solution i do not know why do it this way but there is a simpler solution i just like making life miserable for you guys but but the point is you can figure it out that it is a it is a linear transformation of x now we have that the encoder is equal to the first k columns of v ok what is v eigenve ctors of x transpose x ok student a what is the other thing that you know about the eigenvectors of x transpose x they are the solution for the student eigen if you have given an matrix x then the pca is the eigenvectors of the co variance matrix was the co variance matrix x transpose x what is are it is eigenvectors capital v right so what have we arrived at are we done with the proof yes how many of you think that done with the proof how many of you think that we are done now so it is done ri ght so we have proved what we wanted to prove right so what did we want to prove that you are doing auto encoders you are trying to train an auto encoders and you are loss function is the squared error loss function we saw a neat way of writing that squared error loss function as a matrix operation where x minus capital h into w and then we saw that these squared error loss function is nothing but the frobenius norm of this and we knew that the minima of this objective function the frobenius norm o f x minus h w would occur when s w is equal to svd of x right we started from there and showed that h is actually a linear transformation of x and what was that linear transformation which matrix was used for the linear transformation v capital v what is capital v it is the eigenvectors of student x transpose x transpose x so what is happened in effect is that if i was trying to train my auto encoder with this objective function the weights in my initial layer w would actually converge to v which a re the eigenvectors of x transpose x that means the transformation that i have learnt this transformation which i have learnt is the same as a transformation that i have had learned using pca because pca would also have given me v into x where v was the eigenvectors of the co variance matrix and we just arrived at the same solution everyone gets it now we are done with the proof so what we have proved is under these specific conditions that the encoder of a linear auto enc oder is linear auto encoder is equal to pca if we use a linear decoder if we use a squared error loss function and if we normalize the inputs to this and you understand why each of these steps was important why was the last step important student refer time sixteenthirtytwo only then we would have got the co variance matrix why was a step before that important because only if it was the squared error loss we would have got that frobenius norm objective function right and why was the linear decoder important again the same thing because x minus h w we wanted it to be linear right is it fine so you see why all these assumptions were important and under these conditions we have proved that auto encoders e equivalent to pca"}
{"audio_filepath": "Data/Preprocessed/Fooling Deep Convolutional Neural Networks_102.wav", "duration": 358.0, "text": "with that we g o on to the last module whic h is fooling dee p convolut ional neural networks so turns out that using this idea of optimization where we are able to actually change the image to suit our needs right and these needs were one was we wanted to change the image so that it fires for a particu lar class the other was deep dream where we wanted to change the image so that it is starts seeing patterns which were otherwise not observed in the image and the other was d part where we trained the image or we optimized over the image so that we c ould produce some artistic images and these are the different optimization problems that we have seen but the same idea can actually also be used to full convolutional neural networks and i have already hinted at this earlier so let us see how to do tha t so now suppose we feed an image to a convnet and i know this is the bus image right but now what i do is this is a trained convolutional neural network and what i do is instead of setting the cross entropy loss to maximiz e bus i will set up the cross entropy loss to maximize ostrich and then i will back propagate through the network i will not modify any of these weights or parameters and i am only change the image right so what i am trying to do is i know that this is the bus image but now i am setting the objective that it should fire for the ostrich class so now i am going to back propagate and change this image so that the blog the likelihood of the ostrich class increases you get this set up its very stra ight forward ok and turns out that if you do this with very minimal changes to the image you can actually fool the convolutional neural network ok so this is the change right you have the original image the second image i s actually the amount of change you made and the third image is the original image plus this change now to the human eye there is no distinction here right you would all of first would still think this is a bus and in fact i do not even see that there is a noise in the third bus that you see same for some other class they have taken some bird or something like that and added some noise to it and a temple and in all of these cases the network actually predicts that the modified image is an ostrich ri ght or some very random class from the original class so why is this happening and before asking that question let me just finish and it need not be that you start with an original image and then try to modify it actually yo u can start with a blank image and do the same experiment where you modify the image minimally so that p of robin becomes one or close to one and you will get some very arbitrary noisy looking images which no in which to at least you and me do not look lik e a cheetah or robin or armadillo but the network thinks that these are the classes that these images belong to now this is definitely a risky how many of you appreciate that it is bad ok now and a network is not just predicting it is predicting it with a very high confidence right ninetyninesix percent confidence so why is this happening can even think of a reason for that no but ok in that case i would have been fine if there are one thousand classes it should have given one one thousand probability to all the classes right but this is like worse than random classifier right it is saying with ninetynine percent confidence that this is a ostrich or whatever class that is so why is this happening and the interesting thing is that this in some sense ties back to the universal approximation theory or at least some ideas with that can you think of why this is happening ok so let us try to see a very intuitive explanation for this so on so this explanation is due to andree karpathi we need to put the acknowledgments this slide does not have any acknowledgments actually so remember that images are extremely high dimensional objects right they are two hundred and twentyseven  two hundred and twentyseven which is a very high dimens ional object high dimensional space and no matter how much training data you have you see a only a small sample of this high dimensional space right because its real numbers two hundred and twentyseven  two hundred and twentyseven just imagine the number of possibilities ou t there no matter you have one million samples ten million sa mples for training this is much smaller than the actual number of samples which exist in this space of these only a few are images right so now think of all two hundred and twentyseven  two hundred and twentyseven matrices that you can make and how many of them are actually going to be natural images the probability of natural images is very small most of these are random things right they are just matrices which do not make any sense which actually look like th ese images that you see here right now using the training images we fit some decision boundaries and this is the decision boundaries that we fit right that this is class one the rest of the green part is class two and so on and in fact we are doing the se decision boundaries for some one thousand classes while doing so we actually end up taking decisions for a large number of points that we have not seen we have not seen any points in this space but i have made a decision for them that all of them belong to the green class i have not seen any point in this space but i have ended up taking a decision for them that all of them belong to the red class right so in particular what i have done is i saw a cheetah class image from a cheetah class i saw a few i mages from the cheetah class and i drew some boundary around it to say that this is the cheetah class but my boundary also contains images like this because this is a very high dimensional space and in that boundary a lot of points actually fall in and some of these points are these random points which have no relation to cheetah right but i have been so aggressive in fitting to the training data that i have drawn these boundaries which also include a lot of these points and now all i need to do star ting with these rend random images is that go somewhere inside this boundary and then i am all set right it will start detecting it as cheetah because the boundaries have been drawn by the classifier how many if you get this explanation good so that is the intuitive explanation for why this happens so this is where we will end the discussion on convolutional neural networks"}
{"audio_filepath": "Data/Preprocessed/Visualizing patches which maximally activate a neuron_93.wav", "duration": 351.0, "text": "so in thi s lecture we wil l look a t various ways of visualizing convolut ional neural network s and althoug h it is not very obvious a t this point a s we g o along we will see what i mean by that s o let us start this lecture so i forgot to add the acknowledgments slide so a lot of the material that i am going to cover today is based on some content by a ndrey k arpaty in his online course s tanford course we will add the appropriate acknowledgments and a link to the cour se ok so with that i will start module one which is visualizing patches which maximally activate a neuron ok so what are we trying to do here is we are trying to the quest today largely is going to be able to understand what a cnn has actually learned right and what i mean by that is we said that there are these filters which try to detect edges which try to detect blurs and so on and then there are these neurons which fire for certain things and so on so we want to see different ways of finding ou t what a convolution neural network has actually learned or what have the filters actually learned or what are the different neurons in the convolutional neural network actually capturing what do they fire for what are the kind of images that make them tr igger and so on right so that is the first thing that we are going to look at how do you visualize patches which are causing a neuron to fire so this is again our vgg network just put it vertically say have passed an image to that and then at every layer you are applying convolutions and then match pooling and so on right up to the last layer right n ow we consider some neurons in one of these layers so i am considering this neuron and i want to find out what exactly is this neuron trying to do right and which is the same as asking what kind of images does this neuron fire for so i have thousand different classes i have cats dogs cars trucks and so on i am interested in figuring out what are the different kinds of classes that this neuron fires a nd this is more from say i am already getting some output accuracy and i am either happy with it or not happy with it in either case i just want to see what is it that my network is learning is there any scope for improving  is that that there are no neurons in the network which actually fire for the dog class did not should i do something differently was it that most of the neurons fire for all classes that means they do not have any discriminative power so what exactl y is going on right so that is why we are that is why this study is interesting and you will do something of this sort in your cnn assignment ok so and by now we are clear that if i am focusing on any neuron and any layer i can always go back and tra ce the patch to which it corresponds in the input image everyone is fine with that right so we saw that if i am somewhere here then every neuron here corresponds to some sixteen cross sixteen patch in the original image and the same is true for every layer righ t i can always this is a deterministic process i can just find out which are the original image pixels which contributed to the computation of this particular neuron in any layer ok so i can do that so now what i am going to do is i will send as m any images as possible whatever images are there in my training data test data whatever images i have i will pass these images through the convolutional neural network ok and for the neuron of interest i will note down which when does it fire and wher e ever it fires and by fire i mean it is a output is close to one or it is a output is high because these are relu neurons i look for high output they do not saturate at one right so this i look which images for which this neuron had an high output and for those cases i will go back and trace the image and see which patch of the image actually caused this to fire so i want to see whether my neurons are actually learning things like noise detector or eye detector or something right refer slide time zero fourone so let us look at the results of one such experiment done by a group of researchers so they considered some neurons in the pool five layer and they did this experiment that they pass a lot of images and whenever this neuron fired they went back and saw what was the patch in the image which was causing this neuron to fire so that they found that one set of neurons is actually fires for people places so if you go back and trace which is the image which caused is to fire or which is the patch t hen it is largely centered around a persons face or which is something which is very clearly a person ok a nother set of neurons fires for dogs another set of neurons fires for flowers all sorts of flowers and different orientations different maybe colors are same here but they are all different thing right somewhere inside a bouquet somewhere inside a flower pot some somewhere on a table and so on but expected of that these neurons are firing for any flowers that appear in your input image and the fire only for that patch nothing around it so it is very is actually able to localize and fire there are some images which fire for this images the digits and alphabets written in the image so these are some addresses or dates or billboard signs or someth ing like that and whenever there are these characters or numerals there and this neurons fire and some neurons fire for houses and then some neurons fire for shiny surfaces so there is this different sets of neurons which fire for different sets of thin gs right so also that means your convolutional neural network is trying to learn specific characters of the input characteristics of the input and this is one way of visualizing so this is not like anything tricky here it is just that its good you ca n think of this as debugging tools for your convolutional neural network right because in your you i guys are used to programming where you give different inputs and see what is the output and then try to debug it so this is one way of trying to figur e out whether your network has learned does it really need more training is there a certain class of images for which it is not firing at all or is it confusing between two classes and so right so that is one way of visualizing"}
{"audio_filepath": "Data/Preprocessed/Eigenvalues and Eigenvectors_42.wav", "duration": 1024.0, "text": "so this lectureactuallyis a bit of a digressionand it is supposed to coversome ofthe basicsthat weneedfor varioussectionsof the courseso it is veryimportantthat you understand some concepts for linear algebra specifically eigenvalues eigenvectors and in particulartodaywe willdo principalcomponentanalysisand the reasonthat ido itis there is an very neat relati on of pc a and to autoencode rs an autoencoder is something that well cover in the course it is a part of any deep neural network course and singularvaluedecompositionis somethingthat weusingwhenwe learnword vectorsthe wordvectoris againsomethingvery importanti can just i can do the non svd version of it where i just talk about what word to wick is but that will not give you the sameprobablynot thesameinterpretationas if youstart fromsvdand then reach word vec tors right so that is why i am covering these basics so how many of you know eigenva lues a nd eigenvectors very embarrassing question how many o f you absolutely hate eigenva lues an d eigenvec tors s o let us see if we can change that todayi mean on the positive side so what happens when a matrix hits a vector so most of you a lot of people that i talk to right actually think that eigenvectors are the villains of linear algebra it is very hard to understand them and so on but today i am going to make a case for they are not the villains they are actually the superheroes of linear algebra so that is what the l ecture is about so what happens when a matrix hits a vector student transforms it transforms it right so  actually what happens is that it strays from it is path  so this is the original refer time onefiftyeight this is the original vector x ok and now o nce i multiply it by a that means  if i do the transformation a x then i get a new vector and two things happen right one is the direction changes which is obvious and in many cases the scale also changes that means the vector might get elongated it is magnitude would increase or it would decrease so if you really think about it actually right so matrices are the real villains of linear algebra right and we just look at this vector was minding it is own business going along it is own direction a metric comes and hits it and completely changes it is world right i mean it just throws it off path increases a dimension or slows it down or whatever it so that is they are the bad guys now for every villain what do you have a super hero right so w hat is a super hero corresponding to orbit what does a super hero do know that is a very linear algebra i am talking about comic books that this is very linear algebraic answer he s tands up to the villain right and that is e xactly what eigen vectors do it right they refused to change th eir part they tell the matrix  you can hit me as many times as you want probably you can increase my you could probably slow me down a bit or push me ahead or something but i am not going to stray off from your path right so that is what eigen value eigen vectors do so here is a matrix which is a villain and here is an eigenvector which is our hero and now when this matrix hits this eigenvector it refuses to stray from it is part right it s ays i will move forward i will move back whatever but i will not change my direction ok i will just stay honest to what i am and these vectors are called the eigenvectors i am more formally you can write it as ax is equal to lambda x right so that mean s the direction remains the same only the scale changes it will either get slowed down or it will get boosted up right so the magnitude would change but the direction remains the same now what is so special about eigenvect ors like why are why is it that they are always in the lime light i know the any course that you do invariably touch eigen vectors or eigen values at some point in that course right where be it machine learning image processing whatever you do you alway s speech everything that you do you will always have eigenv ectors and eigenvalues why is it so  well it is turns out that several properties of matrices can actually be explained away by looking at their eigenvalues so if i look at a matrix i would pro bably not be able to comment much on it but if you tell me something about the eigenvalues i can see a lot of things about of it and there is an entire field on this way this entire spectral graph theory which looks at properties of laplacian matrices an d come in something on the properties of the graph and so on right and that is just an example which we do not care about but what we care about in this course there are a few things that we care about with respect to eigenvalues and eigenvector and tha t is what i am going to focus on right so that is what this lecture is going to be out and i will take two specific cases which are very important for us to understand certain concepts later on so i will start with the first one refer slide time fourfive zero and i will start with a very simple example to motivate this problem and eventually will lead to a result which will help us understand a very important concept in deep neural network training which is exploding and vanishing vanishing gradient  we will not touch that concept today but we will use these ideas when we are looking at that later on so let us take this example of two restaurants so there is a chinese restaurant and a mexican restaurant and on day one k one students eat in the chinese r estaurant and k two students eat in the mexican restaurant so this is what my situation is on day zero k one for chinese and k two for mexican now what happens as is obvious people get bored or they have different want to try out different things so on day tw o or other each subsequent day what happens is that a fraction p of the students who ate chinese today will opt for max mexican on day on the next day and a fraction q of the students who ate ma mexican today are going to opt for chinese so you get thi s situation right so i started with k one k two so what i am saying is on day one that is the next day only a fraction p of the k one students will remain for chinese and a fraction one minus q would be transferred from mexican to chinese ok and similarly on ly a fraction q of the students would again stick to the mexican food and a fraction one minus p into k one would shift from chinese to mexican is this setup clear ok can you write this as a matrix operation it would be a matrix multiplied by a vector right c an you tell me the vector student refer time sixtwentynine k one k two k one k two and the matrix is in all this ok this is what it is and i am saying that this happens on each subsequent day it is every day now this keeps happening so on day one i started with s ay one hundred and eighty and now day two it change to something again day three it will change something by the same fraction now let me call this as matrix m and this is of course v zero right by definition as we decided now what would happen on day two what would v two be m applied to v one right and which would be m square applied to v zero i am just substituting the value of v one which is m into v zero in general on the nth day what would happen m raised to n into v zero ok so you see that the number of customers in the two restaurants is gi ven by this series you had v zero then m into v zero then m square v zero and so on up to m raised to n vn ok you see how the number of customer is changing now and this is how i represent it as a state transition diagram right so i had certain numbers on day one and it changed with the trans with the probability p they will stay back with a probability one minus p they will move to the next or the different restaurant and so on right and now this though a very toyish example can you re late it to many things in real life or many things that you will take in decision making right that you are so even if you are playing a game for example and even if you are playing atari games or something you are in a certain state based on some acti on that will take will move to a different state and so on right so these things happen in various real world applications right there is a certain state for example even in stock market prediction you are at a certain value of fish stock it might chan ge to a different value right and these values you could just say them as high low or neutral that i am not going into the actual numbers today the stock value is high it does it possibility that it will transition to something low and so on right so th ese kind of state transition diagrams occur in various real world examples now this is a problem for the two restaurant owners right why is this a problem for the two restaurant owners they do not know how much food to make but every day the number of customers is changing right but is the number of customers actually changing will the system eventually reach a steady state will it is it obvious that it will reach a steady state or maybe it will not even reaches steady but the way i describe it i do not see why it should reach a steady state right you have some people here they go there come back go there and so on the only thing which i have assumed is that the transition matrix which was the matrix m is constant across all the time steps right so  every day it is at the same priorities by which things are changed right so what is your guess if i were to ask you to take a guess ok let us see how many of you think and it is there is no correct answer here at this point so just tell me how many of you think it will reach a steady state how many of you think it will keep changing and why is the sum never equal to one ok so fine so it turns out that they will right and let us see how so we will define some things and some of these are just definitions some of them have accompanying proofs which i am not going to do here you can the proofs have been linked from the slides so you can take a look at them if you are interested so suppose there is a matrix a n cross n matrix which has eigen values are lambda one lambda two up to lambda n now what this definition is saying is that assume that there is one eigenvalue which is greater there is no assumption actually the eigenvalue which is greater than all the other eigenv alues is called the dominant eigen value and when i am looking at a dominant eigen value i am only concerned with the magnitude not the sign so it could be that an eigenvalue is minus ten and all the other eigen values are one two three four five so the dominant eigen v alue would be minus ten right and i will just take it as step is that clear the definition of a dominant eigen value now how many of you know what is the stochastic matrix so matrix m is called a stochastic matrix if all the entries are positive and the sum of the elements in each column is equal to one so now this definition is again slightly misstated so there is a row stochastic matrix the column stochastic matrix and also doubly stochastic matrix right so what i am talking about here is a column stochastic matrix like our matrix have you seen such a stochastic matrix any time in your life in the last five minutes the m matrix right so the m matrix is a stochastic matrix because the sum of the columns was one right you had p one minus p q one minus q ok or was it some of the rows was one rows was one is it the columns so this is a stochastic matrix just a definition now i combine these two definitions which is dominant eigen value and stochastic matrix and give you a theorem right so the largest dominant or the dominant eigen value of a stochastic matrix is equal to one ok so to prove this what do i have to prove so i need to prove two things one that one is an eigen value of this matrix of any stochastic matrix and second all the other eigen values are les s than one so that is exactly what this proof does here you can take a look at it and just to give you a heads up so last year i use to do this that please see the proof go back and look at the proof people never look at the proofs so i used to ask the m in the quiz where i should be sure that people not going to answer right so please when i say go back and look at the proof do that ok so and lastly if a is an n cross n square matrix and you have this series a v zero a square v zero up to an vn then this series will converge to the dominant eigen vector of a what does a statement mean let us not get into the proof right what does it actually mean ok so let us start with very basic stuff right what is the series actually what is each element in this se ries it is a vector it is a vector everyone gets that every element in the series is a vector now what do i mean that a series of vectors converges to the dominant eigen vector what is convergence mean if i keep finding the next element next element ne xt element of this series and i keep doing this as long as i can i will reach a value n right where n is the nth element in the series which will just be a multiple of the dominant eigen vector is that clear you not seem to be clear everyone gets that s o what do you mean by if you take a series of numbers and if i say that the series converges to zero what does that mean if you keep finding the next element in the series you will hit a point n where you find the nth element of the series and it will be zero refer time thirteentwenty that ok so we will just i will leave it at that for now now so stochastic matrix dominant eigen values the connection between two and the convergence theorem for a series of vectors which is a v zero a square v zero and so on refer slide time thirtythirtysix now let ed be the dominant eigen vector of m where m is a dash matrix in our case it is a stochastic matrix so what with the corresponding dominant eigen value be student one one ok so given the previous definitions and theorems what can y ou say about the sequence it converges to a dash of ed student refer time thirteenfiftynine a multiple of ed right so there exists an n such that the a length nth element of the series which is given by this is going to be equal to some multiple of the domina nt eigen vector no no k is some multiple no this is not related to eigen values yet just wait for the next statement then you will see the difference that this is not the do eigen value yet now my question is what happens from here onwards what would be the next element in the series how many of you say some k dash into ed what is the other pause i do not have the other option what is the other option student k into ed k into ed how many of you say k into ed a large number of ok so  you see that no w just notice the eigen value will come up right so at step n plus one you would have m into vn which is m into k into ed and this quantity is actually one so the theorem says it will converge to some multiple of k and now if it is a stochastic matrix what will happen after that time step it will just remain the same vector so what would happen to the number of customers in the two restaurants it will remain the same right you get that ok fine now this was all for what kind of matrices stochastic mat rice s square stochastic matrices but we generally care about any square matrix in fact we should care about any matrix not discriminate but any square matrix will do for now so for a square matrix let p be the time step at which this series approaches a multiple of the dominant eigen vector the theorem was for any square matrix remember it was not for stochastic square matrices we just use this value that for a stochastic square matrix the dominant eigen value is one which i t need which leads to that neat result that the num then the number of customer s just becomes constant but for any square matrix i could write it as this that there exist some step p at which the element of the p th element of the series would just be a multiple of the dominant eigen vector now what would happen a t step p plus one is this fine what about step p plus two and in general at p plus k or p plus n everyone gets this so now can you tell me what does this knowing this dominant eigen value tell us about this series when will it stabilize actually student refer time sixteentwentyfive when lambda is equal to one that is the case we already saw if the dominant eigen value is greater than one what would happen student refer time sixteenthirtythree series will explo de the series will explode and if it is less than one what would happen the series will vanish ok so this is an important result that we will use when we are discussing exploding and vanishing gradients so we will see that in the case of something one as a recurrent neural networks you end up with something of this sort and then i will make some comments on that right so that is why we will be using this will come probably six seven or maybe more lectures down the line ok but we will be using it at this point so the main result from here is that if the dominant eigen value this should be lambda d is greater than one then it will explode less than one it will vanish and equal to one it will stabilize so that is one result one important property of eigen valu es and eigen vectors that well be needing at a later point in the course"}
{"audio_filepath": "Data/Preprocessed/Backpropagation: Computing Gradients w.r.t. Hidden Units_28.wav", "duration": 1217.0, "text": "now we will go to the gradient w ith r espect t o the hidden units so this portion so you already see there is a repetition here and i do not need to treat each hidden unit separately i can just have a formula for the hidden unit and then i could compute it for all the hidden units  so that is what our aim is so let us do some simple stuff first and then you will come back to it so suppose you have a variable x you compute two functions from that one is x square the other is x cube  i will call this as y one and i will call this as y two and i take y one and y two and compute a z which is say a log of y one by y two  now what i am interested in is this what is the answer for this how do you get this this is a fair question to ask y one y two are funct ions of x z is a function of y one y two hence z is a function of x so i can compute this derivative and i can ask for this derivative how would you compute it if i cannot really do this right so if this path did not exist then it is trivial it is just the chain rule along one path but now you have two paths so what will happen add them right so can you tell me a formula for that so let me know if this makes sense to you ok does this make sense now let me complicate this a bit just let me just do i t as y three now student refer time twofifteen what will happen student refer time twosixteen that is all right so you see that if there are multiple paths you can just add up the chain rule across all these paths right that is what chain will ac ross mult iple paths does so with this we will go back to this figure so now i am interested in i am interested in going to the hidden layers again i will do this to bit calculation where i first asked for this guy and then i will a sk for the light blue guy right and am going to look at one unit at a time now what is the what am i interested in the derivative of the loss function with respect to say d h two two right the second unit of the second hidden layer now what i am going to say here is exactly what i had written on the previous slide this was our final function right w hich was z so z was sorry again i have not chosen my variables well ok but if so we had exactly the same situation right whic h is which you see here ok so we will just have to sum up the derivatives partial derivatives across all the paths which lead from this guy to this guy and there could be as many paths as there can be but i do not care i will just sum across all those paths in fact actually here there are not just two paths because we have always assumed there are k classes so there are actually k of these paths right so this form this is exactly the formula which i wrote on the next slide right this one but just w ritten in terms of the net work that we are dealing with  so you can just go back and look at this but as long as you understand this figure you from my point of view we can go ahead  so everyone understands this figure that we just need to compute the d erivatives across all the paths and add them up so now let us start we again the same recipe we will compute it with respect to one guy and then go towards the gradient  so what is this now l et me explain right so dl theta there ar e k of these guys between right so there are k paths so this summation has to happen over k paths just as you told me when there were two paths the summation was two three paths to three that is k paths of the summation over k guys the derivative with res pect to each of these guys and the k th the m th unit rate that is the in dex that i am iterating over a nd then the derivative of this guy with respect t o whatever you are interested that is just that there are only two nodes in the path in the chain but th ere are k such chains how many of you exactly get this ok how many of you have a problem want me to repeat this you have problem oh many of you ok good please do this so i am interested in this quantity that means i am interested in the partial der ivative of this loss function with respect to this guy and this guy is nothing but h ij that much is clear is the j th unit of the i th hidden layer in fact this is actually h two two so my i is e qual to two and j is equal to two now i just made a case on the previous slide that if you have such a function which first computes some intermediate values and then your final function is computed based on all these intermediate values right and now you are trying to find the gradient the partial derivative of this with respect to th e original input that you had so then what you will do is you will sum across all the paths that lea d from this guy to the output  how many such paths are there you already see two such paths here right b ut i am saying there are k such paths because there are some other nodes here which i have not drawn we have already said that in the output layer we have k nodes right so there are k paths so that takes care of the first bit that the summation is goi ng to be over the k paths now what is each of these paths composed of this intermediate value and this quant ity that we are interested in  first we will take the derivative of the out of the loss with respect to this intermediate value what is that tha t is the unit in the that is the unit in the previous layer or the next layer rather so i am interested in i so i am looking at the unit in the next layer hence i plus one right because that is what comes in my path the next layer is what comes in my p ath we ha ve always the special case  t hat this guy feeds into k guys but all the other hidden units before that feed into n guys right so that is let us just keep that complication aside for the minute and we just look at this case ok is that fine  so  we have agreed there are k paths and each path is composed of these two nodes from the last loss function to this intermediate value and then from this intermediate value to the quantity of interest and why is this i plus one because the next node in the p ath when i am at the i th layer so i will be feeding to the i plus one th layer right and in fact i will be feeding to all the nodes in the i plus one th layer that is why i am taking or all the k paths right and then that node which is this node with re spect to the quantity that i am interested in  is this clear now right this is very similar to the toy example which i did i just have k paths now instead of two paths there so let us move ahead now what is ok which of these quantities do we already kno w is there any quantity that we know this one why because in this special case i plus one is actually equal to l right because we are feeding into the last layer and they have already seen how to compute the partial derivatives with respect to the last layer so this quantity is known we do not know this for the generic case yet but we will get that but for this special case when we are feeding into the last layer we know this does everyone get this ok now do we know this quantity so what you have told me is that we know this quantity because that is what we have computed in the previous module do we know this quantity we have to compute it can you compute it ok let us just do it right so let us assume that this hij that am dealing with is act ually h two two ok fine now what is a i plus one m actually which are the elements there a three one and a three two i am assuming that i only have two units in the output layer ok so my m is equal to two now is this fine this is how the next layer is related to the curr ent hidden layer plus biases ok now what am i interested in one of these guys ok let me take one of these guys so can you tell me what is a three one first row multiplied by the first column there is only one right plus b two one student refer time teneleven sorry student refer time tentwelve b three one  now let me just clarify something what is this in terms of variables i j k m what is this this is i this is j this is k this is m this is i plus one right ok this is one of the ms that i am dealing with no w i want the derivativ e of this with respect to hij  in fact i want it with respect to h two two where this is i and this is j is this clear what is this derivative w three one two everyth ing everyone fine with this now help me find this what is this ijk m and i p lus one what is this this is coming from the m how many of you see this because that is the unit that you are connecting to and this is j so what is the formula how many as many as the number of neurons in the next layer a bias will be connected to a ll the neurons in that layer right everyone gets that right there are only two units so there will be only two guys ok so what is the formula for this w i plus one mj everyone comfortable with that ok fine you can just go back and look at this and it shou ld be cleared right so whenever you are dealing with vectors and matrices right if you are really good at it you can imagine the entries and figure out what is happening if you are not good at it do not be lazy just work it out right you just need to write down this product and at the end remember everything is always element wise and you are never dealing with a vector or matrix now just dealing with the individual components of them so you should always be able to compute these derivatives or parti al derivatives with respect to the individual components and that is exactly what i did here right if you j ust work it out if you just write it out then you will always get it if you cannot but eventually try to get to a point where you can just visual ize it but if you canno t at least try to work it out so this is what it will look like ok now consider these two vectors one is this vector what does this vector look like this is a collection of all the partial derivatives so this is just a collection of all the partial derivatives nothing new we have already seen this now what is this vector actually in fact i have started with the matrix and i am saying look at this vector what does this mean this i plus one is just th e lay er in which the matrix is right so that index we do not really care about for a matrix what we care about is the i comma j index ok now what does this dot comma j mean all the i s belonging to j that means the dash column j th column everyone g ets this this is all the i s or all the entries belonging to the j th column s o it is effectively just the j th column so it is one comma j two comma j up to k comma j right so these are two valid vectors now tell me what is this quantity going to be th is is the dash between two vectors dot product dot product between two vectors is a student refer time thirteenfortythree is a summation over element wise thing ok i have said enough now try to connect this is a very simple maths the column that you will ever get in your life try to connect this to something which is already there in the slide how many of you think the answer is this this into this plus this into this plus this into this and just write it as a formula you will get this everyone sees that ok so no w i have a compact way of writing one of these entries one of these guys i have a compact way of writing this it happens to be the dot product between two vectors one of them is the gradient but do i know this already do i kn ow this quantity already in this special case yes because i plus one is equal to l and that i have already computed this of course i know right because these are the weights that i am dealing with where do i go from here this dot yeah it means anything from that column so that means the entire column student refer time fourteenfortyeight ah no these are weights right so this is a weight mat rix it has columns and rows i am talking about the j th column so i fixed the value of j i am talking about the j t h column but i am not telling your given i th entry there am just telling you all the entries there that just means the j th column you can take this offline ok this is very simple i will take it offline ah now where do i go from here student refer t ime fifteensixteen i plus one student refer time fifteentwenty ok n o in this specific case are we done student refer time fifteentwentyseven where are we right now with respect to one unit where do we want to go the entire thing so what is the quantity that i am inter ested in gradient with respect to always say with respect to h i right student refer time fifteenfortyfour where i is two in this case this special case ok what is that going to be collection of all these guys that you have already computed ok now simplify this what is this first column of the matrix multiplied by the same vector the second column of the matrix multiplied by this vector t he nth column of the matrix multiplied by this vector this reminds you of something very very difficult this is a very v ery complicated matrix multiplication right first row of the matrix multiplied by a column the second row of the matrix multiplied by column how many if you get this right so this is can you tell me what this is wi plus one transpose student refer ti me sixteenfiftytwo perfect right so now you see that this entire quantity we can compute in one go by using a matrix vector multiplication right so that is what i meant when i was saying that we should not be doing these unusual computations but we able to compute that at one row right so now we can just do this matrix vector multiplication and get this entire quantity ok now what is still missing in this module so what is the special case that i have assumed i told you that i already know these quant ities bu t only if i plus one is equal to l  i need to tell you this in the generic case ok so we are almost there except that i do not know this when i is not equal to l or i is less than equal to l minus one ok that is the case that i am looking for ref er slide time seventeenthirtyeight so that is again very simple again what will i do i will compute it with respect to ok what is this this is the guy that i am interested in the generic i not the l th one right the generic i this is what the vector looks like the gradient vector looks like i want each of these guys ok now i will take one of those and i will write it as this ok what am i doing am saying that i already have the entries up to here ok a t a very general level even here i could have said the sa me thing remember that i had said that the output layer you can always write as hl right so even at the output layer i could say this chain rule always holds how many of you agree with that i want to go from the loss function to one of the lighter bl ue guys so am saying that i can go through the intermediary dark blue guys that is all i am saying i have just compressed this entire path into up to the dark blue guy remember i had said earlier that i will be compressing this chains now how many of these quantities do you know the first one is what we computed on the previous refer time eighteenfiftytwo the second one looks very difficult sorry so h ij is nothing but sigmoid of a ij or any non linearity of the a ij so i can just write this derivative as i will just write it as sigma prime ok or g prime is this fine now i have it with respect to one unit what will i do go to the gradient fit it all these values now simplify this what is this a vector right what is this another vector there is a one to one correspondence between them so you have two vectors and you are doing a one to one multiplication what is this student refer time nineteenfortythree how many of you say dot product dot product is always a what is the output h ere student vector can it be a dot product can it be a dot product no please empathic no ok so what is it going to be an element wise multiplication and this is how you denote that ok so what is this called you had a mult i product right so thi s is every element of one vector multiplied by the corresponding element of the other vector ok so now again the entire vector we can compute at one row right i am not i am when i am teaching this i am telling you how to compute one element and then go to the gradient but when you are going to implement this we are just going to compute the gradient at one go"}
{"audio_filepath": "Data/Preprocessed/Deep Learning(CS7015): A typical Supervised Machine Learning Setup_19.wav", "duration": 917.0, "text": "we will start module two which brings us to a typical supervised machine learning setup this is a very important module please pay attention so now we have a sigmoid neuron we have taken care of the fact that the perceptron was a very harsh function so we have a smooth function so things are fine now what next where do we go from here what is my next topic going to be yes a lot of you are giving the right answers we need to learn these weights it does not help just to define the function this function depends on certain weights and now i need to give you an algorithm which will help you to learn these weights now remember when i talked about perceptrons before giving you an algorithm what did i revisit what did i talk about the error surfaces and then i had motivated from there that our goal is to find a set of weights which give us close to give us zero error in that case or in generals speaking generally they should give us a minimum error they should help us to minimize the error rate so i need to set up that similar story here so we will again revisit the concept of error so now in the case of perceptron i had shown you this figure which they were this data is not linearly separable which is obvious and i told you that perceptron cannot handle this data but what do i mean by it cannot handle this data it cannot give zero error but what would happen if i run the perceptron algorithm on this take a guess what does the perceptron algorithm do fine and i could convergences my condition i could make that condition a bit loose what is a valid convergence condition that you would lose here use here till almost all my points are separated properly so instead of aiming for one hundred percent separation i could have a threshold which says as long as ninety percent of the points are separated i am fine with it that looks like a reasonable thing to do so now if i run the perceptron algorithm with that condition what do you think will i get as a decision boundary everyone has a picture in mind ok let us see does this match whatyou had in mindroughly of course theremanythingspossiblebutit will basically pass through this now what is happening here what is the problem there are three blue points which are wrongly classified and three red points which are wrongly classified but in most real world applications we will find that this line is not too bad you could live with this error this is probably three out of thirty on both sides which is roughly ten percent error unless you are using it when some mission critical applicationsor in health care where it is a life and death situation or something in most cases you could live with this right so if you are trying to predict whether people will vote for a particular party if you make this kind of error it would be largely ok unless it is a very close election but largely it would be ok so we could live with this kind of errors in most cases so from now on we are not going to be too optimistic and if you are going to say that there would be some error but my job is to find the weights such that my error is minimizedi want the minimum possible error that i could get is that fine so again whatever weights we want to learn we are going to be driven by some error function and we would want to minimize that error function sothisbringsustoatypicalmachinelearningsetupwhichhasthefollowing components so this perhaps is the most important slide in the course and i will say this at least for one hundred other slides in the course but at least for now this is the most important so you are given some data xi yi and you are given n such elements right so let me just elaborate on this and give me i will give you some instances of this let me give you some instances of this right so one thing we say i already told you was this so this is my x and this is my y so one example which i gave was about movies so this was genre actor and critics rating and so on this is one instantiation of this problem i could also give you another instantiation which was i just told you oil right so this is how much oil can i get and here my factors were salinity density and so on there were many other factors so this was my x again x belongs to rn where n is some number integer and another example could be say fraud detection so i have a customer i am a bank i have a customer who has bought some credit card and i want to predict whether he or she would commit a fraud and i would look at factors like what is his occupation maybe salary maybe family size and so on there could be various factors which i could look at so here again this becomes an x ok and you could think of various such examples right where you are given an x and you are given a y ok so this is the data that you have now what is machine learning where does machine learning fit into this so we know that there is some relation which exists between y and x in each of these cases all of us are convinced that there is some relation so whether a person would commit a fraud would depend on these factors it is reasonable to assume that it is not a very wild assumption whether you would find oil at a location would depend on some of these factors and it is related and similarly for the movie case so there exists some true relation between x and y such that if i plug this value of x into the relation it would give me the value of y there exist a true relation this true relation couldbe governedby variousthingsrightit couldbe governedby physicallaws example in the oil mining case it could be even governed by biological laws again the marine life in that location and so on it could be governed by economic laws it could be covered by psychology right we do not know why a person cheats what is his function that he is using when he cheats and so on right so these could depend on various factors but we all agree that some function exists hence we get these values for this particular input for every input we get a certain value so there is some function which takes us from the input to the output we do not know what this function is we never know in practice it is a very very complex function is all that we know we do not know this exact function if you knew this exact function then there is no problem to solve we just use that function and you can predict how much oil and all of us can become billionaires so that is not the case we do not know what this function is so then what do we do in machine learn we make an assumption ok we make an assumption that there is some function which takes x to y and this function is governed by some parameters and this is our approximation of how the real world works and now under this assumption we want to predict the parameters of this model given the data now let us take a very simple case where we could assume that y is equal to wx plus b i am taking this in the scalar single dimensional case now how would you estimate the values of w and b oh come on if i give you two data points you can estimate the value or should i write it that would jog your memory right this is how we all learn right so m and c you can estimate if i give you two data points so that is the simplest case now we willmakesimilarassumptionsbutmorecomplexfunctionsandjustaswecould estimate m and c from the data we would expect to estimate ws also from the data so that is what the machine learning setup is so let us see so the model when we talk about a machine learning model it is our approximation of the relation between x and y and we are free to make any such approximation so i could say that this is what i think is the relation between y and x and which is governed by some parameters w do you know what is this function have you seen this before no not sigmoid which model is this logistic regression ok but i could also have made a different assumption i could have made this assumption what do i get linear regression ok please note that this error on the slide ok and i could make some other assumption i couldassumethatyisactuallya quadraticfunctionof xi amfreetomakeany assumptions the only thing i need to ensure is there is some parameter involved what is wrong with making this assumption if this is valid is this also valid if not why there are no parameters so no not for any x we will get the we will it will depend still depend on the value of x if i plug in different values of x i will still get a different output there is nothing to learn what do i do with all the data that i have there is absolutely nothing i can use it to learn i have just said that y is equal to one over one plus e raised to minus x i can ignore all the data that you had given me whenever you give me a new x i will just plug it into this formula and tell you the answer and that is bound to be wrong because i have not adjusted this formula now once i put in the ws it gives me this degree of freedom where i can now adjust the formula i can learn the ws in such a way that my predicted ys are very close to the actual ys so that is why we need always need a parametric form of course there is nonparametriclearningalsobutiamjustsayinginthissupervisedsetupweare thinking of models whether you have parameters so you have the data you have the model the model always has some parameters in all of the above cases w is a parameter right either the small w which is a vector or the capital w which is a matrix right so notice that this is a matrix this is one cross n n cross n and n cross one now how do we learn these parameters that is the question that we need to answer how do we learn these parameters we are convinced about two things that we never know the true function so we come up with an approximate function and we have to insert some parameters in that function so far good now i have to be able to learn these parameters now for learning these parameters we have something known as an learning algorithm so did you see any learning algorithm so far perceptron learning algorithm right so you already saw the perceptron learning algorithm and it was able to learn the weights for a perceptron there are various such algorithms today we are going to learn one such algorithm which is gradient descent now any kind of learning what is it driven by learning is driven by errors objective function and so the analogy which i like to give is suppose you are trying to learn trigonometry you have a chapter that is your training data the chapter has a lot of formulae that is your training data now what is your objective there are two objectives actually i will tell you the easy objective the training time objective is that once you read to the chapter a few times at least whatever formulae are given in the chapter you should be able to produce the correct output for that so if i ask you what is sine square theta plus cos square theta you should be able to answers them and you should be able to give me the correct answer so in other words you are trying to minimize the error on the training data whatever training data you have which is the chapter content you want to make zero errors in anything which is given in the chapter that is your training error of course there is also something as known as a test error because after you have learned the chapter i will give you an independent set of exercises which might contain questions which are not seen in the textbook earlier so you would have seen sin square theta plus cos square theta but now i could ask you some other formula which you should be able to give me answers if you have learned properly right so now right now we are just talking about the training error that means getting all the formula in the chapter correctly and our chapter is actually the training data which is given to you this is what we are reading so this always going to be driven by an objective function and our goal is just as we wanted to minimize the errors that we make on the formula given in the chapter we want to minimize the errors on the training data is this set up absolutely clear to everyone anyone who does not understand this has any doubts so this is something this is the same framework that we will use again in lecture eighteen nineteen twenty and so on to explain some more complex models so you have to absolutely make sure that you understand this it is not very difficult but just make sure you understand this fine so let us concrete at this a bit more and we will consider our movie example and try to fit that into this framework so what is our training data there they are given movie comma like dislike and when i say movie i am just using a shortcut it is actually all the details of the movie genre actor director critics rating and so on that is our input and our output is the like dislike value what is a model that i chose what is a model that i chose i do not know what is my true relation between when i like a movie or not but i made this approximation that this is how y depends on x and i made sure i introduced some parameters there i could have chosen some other functions also but i chose this now the parameter is w this should be bold w the learning algorithm that we are going to use is gradient descent which we will be seeing soon and what is an objective function here can you tell me a formula so we have been talking about it in terms of english that i should be able to get predictions which are as close to the true prediction can you put it into a formula y i minus y i hat where hat is the prediction this is the prediction so that is y i minus y i hat that should be minimized is that fine whole square of that so why do you square it so that is correct so for all the training points n training points i want to minimize the square difference between y i the true prediction and the prediction sorry the true value and the predicted value is that fine and why do i use squares differentiable is one the other thing the positive errors and the negative errors should not cancel so it would be happen that on some movies i make a positive error of zerofive right that means the actual label should have been zerofive and i gave one on some movies i make a negative error of zerofive and these would cancel each other and i will get the false impression that i am making zero errors but once i square the values the negative values also become positiveso this cannot happen right so that is why we always use the squared error function and also this is differentiable which is more important now the learning algorithm should try to minimize this particular quantity ok so this is a typical machine learning setup almost any supervised learning problem that you see you could cast it in this framework change the y hat function appropriately change the parameters appropriately maybe use a different learning algorithm depending on the problem that you are trying to tackle and you should be able to fit it into the same thing is that fine ok at least for this course everything that we do we will largely be able to fit it into this framework"}
{"audio_filepath": "Data/Preprocessed/Introduction to Encoder Decoder Models_112.wav", "duration": 1270.0, "text": "and in thislecturewe are goingto talkaboutencoderdecodermodelsand attention mechanism so this is a very interesting lecture at least interesting to me because this is very put allthesepiecesthat wehavelearntso farrightwe havelearntthree typesof networksfeedforwardnetworksrecurrentneuralnetworksandconvolutionalneural networksand we haveseen independentapplicationsof eachof thesewordto vecand imageclassificationand so on nowtodaywhatwe are goingto seeis howdo wedo different combinations of these networks and come up with a wide range of applications like apply them to a wide range of applications ok so let me start by an introduction to encoder decoder models and then we do various applications of encoder decoder models so what we are going to do is we will start by revisiting the problem of language modeling so the problem of language modeling was that you are given some t one words or characters and you want to predict the t th word or character right this is like auto complete in short right whenev er we are typing something you have type four words you want to predict the fifth word or you have typed four characters and you want to predict the fifth character ok so more formerly this is what we are interested in how many of you get this equation th is expression so we are given a sequence of t one words and you want to find out what the value of y would be at time step t and we want to find out that value which maximizes this property that is what this argmax equation means and now we will try to see how to model this using a rnn so let us see we are going to start with go that is that we want to start generating a sentence and then we will produce the first word which is i ok and what is it that we are predicting at this point what is th e network supposed to predict what is the output supposed to predict actually it is supposed to predict a dash over the vocabulary a broadly distribution over the vocabulary right so this is what is happening we will of course come back to this on th e next few slides but you have say words wone wtwo up to w v in your vocabulary at every time step you want to find a distribution over these words and then pick the word which had the maximum probability at that time step right that is exactly what thi s quantity is that is what we want the rnn in to model and then we want to keep doing this till we reach the end of the sentence ok so that is the language modeling problem and as we had made a case for it earlier the word produce the time step t depen ds on a few previous words how does a recurrent neural network ensure that at any time step i am going to give it only one word as the input so how does that ensures that it depends on all the previous words also through the recurrent connections and the gate and sorry it is not the gate the state st ok fine so we will see this of course in more detail and we will write down the model equations and what is happening so we are interested in this quantity which is the probability of the word at the time at the t th time step where this j belongs to vocabulary v and see a vocabulary of tenk words or twentyk words for english it is actually much higher but say you are considering only tenk to twentyk words then we want to predic t a distribution over this vocabulary so using an rnn what are you going to do at the output layer is the following is this correct how many if you understand this equation not many why what does this equation compute first of all softmax softmax m eans student probability distribution probability distribution ok what does it take as input at every time step the state right what does it do with the state a linear transformation right and then a bias ok so what is this quantity scalar vecto r matrix vector of size students refer time fourzero the re fer time fourone what is the g th element of the that students probability of the g th word the probability of the g th word right so i just have to explain it in that many words everyone gets it now everyone gets it if you do not get it you will not understand the rest of the lecture  i am very serious everyone gets it so in other words what we do this entire y one to y t one  which we were conditioning on we are just using s t as a surro gate for that and that is fair because st has actually captured all the previous information that we had now just using st as a state which captures everything that happened so far so that is actually how we are modeling this and the recurrent connecti ons ensures that st captures everything which has happened so far so now let us look at the five things that we have in a typical supervised machine learning set up which are those data model students parameters parameters students objective function objective function student cross refer time fourfiftyfour very good no someone said objective function and then loss function learning algorithm right ok so whats the model here students refer time fiveone you know wha t you are trying to model which is a property distribution what is the actual so here y is the probability distribution and your x is the input given to you can you tell me whats and we have already set always said in this course that whatever be the y whatever the be the x we are interested in this function x sorry function f and we should be actually expressively we able to write this function so what is the function here can you actually write down the set of equation just think of what the o utput is how you are going to reach the output given this network what is yt going to be whats the equation for yt and then try to go back all the way back to xt so yt depends on something that something might depend on xt so how do you go all th e way back right that is the thing which i expect you to do ok how many of you get it now please raise your hands ok so let us see at every time step what am i interested in predicting students probability distribution a probability distributi on that means i will have to compute which function students softmax softmax so the green vector is what i am going to focus on so whats the equation for the green vector is this fine now what does this contain apart from the parameters st how do i get st is it fine you can write now you have written this output y as a function of x because x appears here or other yt as a function of xt is not it is straight forward right once i show you the answer is should be how many of you get it now please raise your hands high up above okay what are the parameters b and c right so these are the parameters whats the objective function cross entropy or dash of cross entropies students sum of cross entropies sum of cross entropies right so the loss is going to be over all the time steps at every time step is the cross entropy loss right everyone gets this ok whats the learning algorithm back propagation students true time true time fine so that is what it is going to be right eve ryone is clear so you can see that we have written the final output as a function of the input right and this is end to end trainable that means the gradients can flow modulo this vanishing exploding radiant problem and we have a way of handling that we can replace rns by lstms that is all rightso that is what it is now this just make sure you understand this properly so that we are going to do various instantiations of this for different problems ok now here is one q uestion we all smartly wrote this xt but why is the input at every time step when i am predicting home the input was at but how did i get at that is what i dash at the previous time step predicted at the previous time step right so this is what th e input looks like so at time step one i predicted i as the output at the next time step i am going to feed that has the input does it make sense so just see if you are doing auto complete you would select that i am fine with the word i at this time s tep so it is going to take that as the input and then try to predict the next word that is what exactly is happening here and now you are predicted am at the next time step you are going to feed am as the input and continue this chain throughout ok so the input at every time step is going to be the word that you have predicted the previous time step and i am just going to represent it by a one hot vector right it is the index of the j th word only that could be hot everything else would be zero and all of yo u are fine with this no so at training time this is the inference time at training time we will have the real inputs no that is at inference time at training time we will just use that through because training time you know what the inputs are right you know the true sentence you have the wikipedia sentence right and you know what the true sentence is going to be what i am talking about how will you generated at test time at training time you know all these things right no about training time h ow will you do that you will know what the next input is right so now ok so i said that the input is going to be a one hot vector is everyone fine with that one hot vectors are ok what else could you use students word representation the word represe ntation for that right so assume that you have already done the word to vec assignment and you have completed all the word representations and you have them with you now but instead of feeding the one hot representation of the input you can just feed t he word representation of it does that make sense one hot representation is just one of the many representations possible for the world so why just do that you could do s v d you could do one word vec or whatever you want right so that is in practic e what we will feed is the word to vec representation so everyone gets this what is happening at every time step now one more thing that you need to notice that s zero  which is the input at time step one the previous so s one one  so that we do not know what it is so we just keep it as a parameter we say that s zero is also weight vector and you are going to learn it along with all the other parameters in the network does not make sense because you do not know what s zero means i s a semantics of it is not clear like what was generated at the zero th time step we do not really know right so will just make it a learnable parameter and that would be trained along with all the other parameters of the network so before we move on what we are going to do is we are going to see a very compact representations for rnns grus and lstms so remember rnn is the following equation rnn is defined by the following equation the s t is a recursive function of s t one an d x t right so i am just going to write it as that s t is equal to rnn of s t one x t  instead of writing all these parameters and sigmas and all that i am just going to write it compactly as this now this is what what is this gru so how may going to w rite it as students gru gru of students s t one x t s t one x t what is this students lstm lstm how may going to write it lstm of when the output of the lstm is both h t one and s t one right fine so in some sometimes i will just say st sometimes i will s ay both ht minus ht and st as per whatever i needed right so this is i am not going to write these equations and parameters again i will just say that lstm of this assume that is a function which does this calculation and gives you back ok ok refer slide time eleventwentytwo so far what you have done is we have seen how to model the conditional probability distribution given the previous t minus one words now let me give you a different application right what if we want to generate a sentence given an i mage so this is what i am interested in doing i am giving an image and i want to generate a sentence can we just think of it formally what is it that you want to do so we saw that in this case formally we were interested in this conditional distrib ution in this case what is it that we are formally interest in if i were to write it as something formal what would i write it as ok i will give you a hint what kind of a distribution is this a conditional distribution right given the previous sequ ences previous s equence of words generate the t th word now in this situation can you stated an similar words given the students image image generate the students sentence sentence or given the image and the description that are generated so far be cause i am going to write the description one word at a time given the image and the description that have written so far generate the next word in the mission so what kind of conditi onal distribution is that p y t given students y one to t minus one yone to t minus one students comma comma students image image does that make sense everyone gets that ok so what so this is what we want right so here now we are interested in this quantity as a post to this quantity does that make sense ok and t his is again a conditional distribution so earlier how did we model this we just modeled it as the following we said that the whole context of yone to yt minus one is just contained in that blue vector which is st right so re move this variable and replace it by a vector does that make sense ok now you have the image also so how are you going to model this so what are you going to write on the right hand side ok let me give you a hint we all agreed that this is the quan tity that we are interested in right we also agreed that the following is fine replacing yone to t one by st is fine now what about the image what do you mean by objection in the image you will supply the words which are there the object names that man fine that is fair enough well if want to make it more abstract more neural so what you are saying is that whatever information is contained in the image should be passed here whatever information is contained in the image should be passed here how do you whats the way that you have learnt of computing the information in the image a dash neural network students refer time fourteenfifteen a students convolutional neural network feedforward neural network students convolutional neural network convolut ional neural network ok so but what from a convolutional neural network how many representations that is a convolutional neural network learn how many does v g g network learn v g g sixteen the last layer is a softmax layer fifteen right so which one will yo u give now one before the last one that is called the students refer time fourteenthirtysix dash layerdash dash layer fully dash layer students fully connected layer fully connected layer ok at least your language moral works fine ok so that is the f ully connected layer remember that all the layers in the convolutional neural network learn an abstract representation of the image and as she was trying to say that this abstract representation contains or at least you believe it contains all the infor mation that is there in the original image just as st contains all the information that was there in the sequence yone to t minus one this abstract representation that we will get from a cnn contains all the representation all the information that is there in the image we all believe that ok and we also believe that any of these representation is fine in practice the convention is to use the fully connected layer that is called as f c seven the seventh fully connected layer right and it is seven because you al so start the numbering from the convolution layer one two three four five and then the seventh layer ok so that is what you will take ok so does this make sense and it is a very simple extension from what we were doing earlier this is what i have circles is wha t we were doing earlier right where we only had st now i am saying is just as you believe that st and codes all the information in the previous sequence i am just asking you to stretch that a bit more and say that f c seven of the image contains all the i nformation that was there in the image is it fine ok but still there are some issues and there are other ways of making this condition on f c seven in particular what you could have done as she was trying to suggest initial is t hat maybe you have a vocabulary of all the objects that are possible in your image right so maybe in your image there is man woman there is flying desk frisbee or there is dog cat and all these things right so you do an object detection first get o ut all the object which are there and then make the distribution conditional on these objects right so you can say that i will allow for a ten words to describe the image so there word one is equal to man because i have detected the object man in the ima ge word two is equal to frisbee because i have detected the object frisbee in the image that is all that is another way of doing it ok so i just want to make it clear that there are different ways of making the conditional distribution conditional on t he image itself we are choosing to make it conditional on f c seven of i right that is the neural way of doing it ok so let us see two such options the first thing that we coul d do is we could set s zero to f c seven of the image what is s zero the first thing that was passed to the language model ok so remember we had this go symbol and we had this s zero which was mysterious we did not know how it comes but now we know it that s zero could just be the image that is what my starting point is so take this image and now start generating the representation generating a description does that make sense ok so this is what the network looks like so what do you saying is that these things are of dimension d the cns output was say of dimension four thousand and ninetysix so this has to be converted to size d right that means what will you how will you do that we have a four thousand and ninetysix dimensional vector and you want to convert it to a d dimensional vector w belonging to students refer time seventeenfiftyone four thousand and ninetysix was d fine in g eneral any two vectors if you want to make them compatible this is what you will do you will project so that they are of the same dimensions x zero will be the go symbol go is the special word in your vocabulary which says star generating the sentence ri ght so whatever vocabularies you will add two special words right one is go and other is stop so whenever you generate stop you stop generating after that fine what is the other way of so here now what happens is so this is what is happening techni cally and that is why that is what i wanted you to understand this now s one depends on s zero ok what we are interested in is the following that yt should be conditional on yone to t minus one comma image ok they have make sure that i is s zero and this quantity is st you have to find now since the first time step depended on the image all subsequent time steps will depend on the image is that ok what is the other way of doing this what now in this looks slightly inefficient whats the other option that you cou ld have used just feed the image at every time right so that is the one constant thing that this is the image now whatever you have generated so far considered that but in the addition to that also consider the image so what would the diagram look like just passing the input to every stage of the decoder ok i have already started using terminology which have not introduced but i will just introduce it shortly so let us look at what the fu ll architecture looks like there is something known as the encoder which takes your input encodes it and gives you a representation right then you have something known as a decoder because given this input you want to decode what the output is right s o remember general terminology would be whatever input is given to you you want to encode it and whatever is the output that needs to be decoded right it is you could think of it that this is the image now i am trying to decode the description for the re is that fine ok and then you have an rnn which is used to decode the sentence from this input so such architectures are known as encoder decoder architecture and these are become extremely popular and we will see why they are so popular and why the y have led to the popularity of deep learning in general ok so everyone understands this diagram anyone who does not see a problem with this diagram there is actually no problem but i want you i want you to see beyond the diagram and to look at the equations what do you mean by that what do i have here as the input what is my x so this this looks fine i have taken one box and connected it to another box and everything is fine right but that is not what i am interested in what am i interested in  can you write the input as a the output as the function of the input in this case is it possible to do that so that is what we need to make sure that we are able to do right so we look at various applications suggest lepted criptical here but i am going to come back to it so i just the emphasis that look beyond the diagram the diagram looks very nice i hope it does thanks to the tas but it does and but we need to understand what is the what is the set of equations being conveyed through thi s diagram right what is the function that we are trying to learn we are going to write y as a function of x are we able to write that function because now we are suddenly thrown in a convolution neural network at some place we have an recurrent neura l network then we have the feed forward layer at the output which is the green vectors so does all this combined together right"}
{"audio_filepath": "Data/Preprocessed/Better initialization strategies_72.wav", "duration": 1546.0, "text": "so in this module we will talk about b etter i nitialization s trategies refe r slide time zeroseventeen so this is where we are in the story right we saw that deep learning has evolved and at least these are the four things which have happened so we have by the way this slide is incomplete what are the other things w hich have happene d actually w hich you already said in the beginning two more things which are not technical but which happened more data right and more compute but these are not really technical in the sense that i mean this just happened we have large amounts of data t hat means more data means what if you are more data for training you would have complex networks but not over fit right because you have so many so much of data right and more compute of course it speeds up some of these matrix computations which h appen so remember in a deep neural network most of the things which are doing are matrix matrix operations right you are taking are that is what exactly you did in your back propagation assignment you did a lot of matrix vector computations and so on and the advent of gpus this b ecame very very fast rate orders of magnitude fast so this two which are here as nothing much to talk about that is just something we all understand what has happened they and so now we will talk about better weight initi alization strategies so let us start with this question right we will take this network and we will ask this question what happens if we initialize all the weights to zero i like it when you all try to visualize it and ok so you have to see what happens right so let us start with a one one which is w one one x one plus w one two x two so i always start small l i do not try to see what will happen everywhere just start with one neuron and see what happens take a one two what would a one mo nths value b if all the weights are initialized to zero zero and a one two again zero right a nd same for a one three it is all the way all the neurons in this layer are going to be zero till is it so that means they will all get the same activation so if the as are sam e the h s are also going to be same and that is obvious irrespective of what non linearity you use now what will happen during back propagation what will delta w one one b this again i do not know why you do this ok anyway that will be erase it into x one so  remember that the gradient is always proportional to the input and you have somewhere along the lines along the chain rule you have this h one one and a one one just remember that and what would delta at gradient of w two one b is that fine now can you see some things on the left hand side and make some comments on the gradients we have seen that a one one is same as a one two and h one one is same as h one two that means these gradients are going to be equal right that means the weight started off at the same value they are going to get the same updates and again remain at the same or different value but this same right then of course move from where you started will not be zero anymore but they will all be at the same value both the weights will get updated with the same value and they will remain equal so but fine as i keep training they will move away from each other right this is what i told you when you feed in the first example take a both the weights remain the same but now if you f eed another example and you keep feeding batches there there is no dearth of data that you have and eventually these weights will move away from each other the update is the same again the weights are the same again the same situation will hold right again your w one one x one plus w one two x two is going to be the same as w two and x one plus w two two x two and the same argument repeats how many if you get this  ok so once you initialize the weights to zero in all subsequent iterations the weights are going to remain th e same i mean they will move away from zero but they will all be equal ok and this symmetry will never break during training so what actually is happening in terms of the capacity of the network this is same as student s ingle line tying the weights th e same as tying the weights s o this symmetry will never break during training so asking what is the net effect w hich is happening so you have so many weights in your layer but all of them are moving together will so in essence you do not have the sa me freedom as you have with n different weights right here in some sense unintentionally tied them because it started off with the same value now you are all moving at the same values rate you are all going to the same value so you do not really have the amount of freedom that you would actually expect with n different parameters all of you get this and the same is true for w one two and w two two also which are the weights connected to the second neuron and this is in fact true for all the weights in laye r two you can actually mathematically verify it that means whatever this small analysis that i did here just go back and do it for all the weights in the network and you will see that all of them if you are going to initialize them to zero all of them are g o ing to remain equal this is known as this symmetry breaking problem this is are known problem this is existed much before two thousand and six and so on if we initialize all the weights to zero you will have the symmetry breaking problem is there a nything sacrosanct abo ut zero or would this happen even if you initializer to same but non zero values and that should have been cleared from the iteration right because after the first iteration we were at non zero weights and after that the story repeated right so even if you in itialize it to non zero weights the same story is going to report repeat so that means as long as you initialize all the weights to the same value you are going to end up with this symmetry breaking problem ok which is not good so what is it that we have learnt about initializing weights student refer time sixthirtytwo definitely do not initialize all weights to zero definitely do not realize them to the same value ok this is the first thing that you have learned so we are seeing different ways of not making the light bulb and then we will come to a way of making it so zero and equalist no bad yes some weights will not get updates in that case right so then that that should be fine s o that is the other thing i wanted to make at some point right the se four things right initialization optimization regularization and activation function these are not independent things they are all tied to each other so as you said now if you use regularization then probably you could be a bit careless with the init ialization even if you had initialize the weights together drop out would have ensured that some of these weights are not active at a particular training instance that means they will not get weight updates that means they will move away from the oth er weights so that is this is not that only one of these things can be done right you are going to use a combination of these things but while analyzing them we will just look at one of these things assuming that the others are not being right  s o will assume that we are not using drop order anything is that fine so this at least this in practice you are not supposed to initialize the weights to zeros and equal values that is what we have learned so far now for the rest of the to convince you about some other weight initialization methods w hat i am going to do is i am going to take a feed forward network where you have as input some thousand points each of this point is five hundred dimensional and the input data is drawn fro m a unit gaussian what i mean by that is you have this x one two x one five hundred rate for the data instance one so all of these five hundred dimensions come from a unit gaussian is that fine so this comes from a unit gaussian this comes from a unit gaussian and so on ok  that is what i am going to assume and the network has five layers each layer has five hundred neurons the input is five hundred neurons each of the five layers is also five hundred neurons and now we will run forward propagation no backward propagation n o loss nothing and i am not even giving you an objective this is just some input and i just want to see what happens up to the last layer i am not even bothered about the actual last layer that means i am not trying to minimize any cost entropy square d error loss anything so let us try a few initialization strategies so we realize zero is not good realize equal is not good so let us try some random initializations but small weights ok and this is my way of randomly initi alizing with small weights so my w is a matrix of size fan in into fan out rate which is n cross n ok t he number of weights coming in and out rate so n cross n and i am drawing from a uniform distribution a nd then multiplying it by point zero one which en sures that all the weights are very small y ou get the setup now with this i am going to start with the input and then keep doing these transformation s so i will do w transpose x plus b pass it through a sigmoid and do this five times because i have five de ep layers now this is what happens to the activations across the five layers so the first layer remember that we had drawn from a unit gaussian right so that is what the data input data looks like so this is the first layer which is the input data basi cally and then this is what happens across the different layers so what is actually happening and this is for the tan h activation function there is no variance in the output of so this tells me so this basically tells me that for all the neurons wha t is the average value that i am getting right and i should ideally get some histogram that for some neurons i am getting the value minus one for some neurons minus zeronine zeroeight and so on but what this is telling me is as i keep progressing across the layers all the neurons have very similar values and they are all close to zero this is what actually happens in practice i have just actually run it and computed the histogram and if i use sigmoid activation functions again something similar all the values tend to be close to the center which is zerofive s o this is zerofive and although i had started with a nice gaussian distribution now what will happen during back propagation so do not try to think for now tha t why this happens i am just telling you have actually run the code and this is what happens now given that this has happened what will happen during back propagation so all the activations in a layer are very close to zero all the gradients are going to be close to zero that means no gradients are going to flow back that means which problem are we dealing with vanishing gradient problem so if you initialize your weights to very small values and this is easy to see in the case of tan h so for tan h this is my function right and this is zero now remember that this is wi summation wi xi if all my weights are close to zero or very small v alues what is summation w i x i going to be it is going to lie somewhere here right so all these inputs are actuall y going to be very close to zero now if my inputs are going to be close to zero i know that during back propagation at some point my gradien t is proportional to the input t hat i have given and when i say input here i mean layer one layer two layer three and so on so that means all my inputs are very close to zero now my gradients are actually proportional to the input so all my gradients are also going to be close to zero that means my gradients are vanishing right because remember that across five layers you will have these products of gradients right all of them are very close to zero so you will end up with something very close to zero raise to five how many if you get this right so our gradients are going to vanish if you do this very s mall initialization of the weights and that is exactly what is happening so this is the histogram for the gradients and i see that all my gradients are actually very close to zero that means no effective training is happening my weights are not receivin g any updates this is what happens in practice if you initialize your weights to very small values now let us try to do the opposite of this very small values did not work so let me try large values and for large values i j ust sample from the uniform distribution i will get some numbers between zero to one now can you guess what wil l happen remember summation wi xi all your weights are large so why am i saying that number between zero to one is actually large it is not by all pr actical because this is going to give me this function is actually going to give me numbers between zero to one why am i calling them large weights student refer time twelvefiftyfour no i will i just talked about the weights assume there is no biases how many of you get that answer remember there are five hundred neurons so if you have five hundred small values that summation is going to be still large right if you all of these are zerofour or zerofive which still looks small but if you have two hundred and fifty of these or if you have five hundred of thes e the resultant sum could be somewhere of the order of two hundred and fifty right and that is very large because if you pass that to a sigma and neuron what will happen saturation right  so you get this why i am calling these weights as large so and this is actually what happens so when i have these tan h activations across all the five layers i observe that my neurons saturate i either get minus one as the output or plus one as the output and same thing happens if i use sigmoid activations i either get zero as the output or i get one as the output right neurons saturated means what will happen gradients will vanish right so even if you initialize the weights to very large values all your gradients are going to be close to zero because  they are going to vanish and again you have a problem so what have we seen so far zero is not good equal is not good small weights is not good large weight is not good then what do we do so let us see what to do so let us try t o arrive at a more principled way of initializing weights and this again do not should the messenger i am going to give you a proof under certain assumptions ok so just bear with me i just tell you what those assumptions are going to be as we go along so as i said right so i mean you would argue that in practice these assumptions do not hold true but at least they give us some insights into what is happening right  what is the overall idea behind what is being proposed so let us start with tha t so now consider this deep neural network and i am just considering the first layer of it where i have this neuron s one one and i am talking about things before the activation so i know tha tsone one is equal to this quantity right so all the incoming we ights to the first neuron which is w one i into x i now for some reason i am not telling you why i am interested in the variance of this can you tell me why i am interested in the variance what did you see in the previous examples there was no variance right there was hardly any variance so let us see what happens if you compute the variance of this so i am just taking the variance formula a variance of a sum is equal to the sum of the variances right this is of the form variance of a into b where a is w one i and b is x i what is the formula for this or if you know it or do not know it do not care so this is the formula so this is the generic formula for variance of a into b where you have to assume that a is wone i and b is x i  so this is just a formula there is no trick here no math i mean no nothing fancy here just apply the formula for variance of a b and substitute a is equal to w one i and b is equal to x i now i will assume that all my inputs are zero mean fin e we have been assuming that forever and all my weights are also from zero mean ok what is the effect of that which quantities will disappear this will disappear because mean as zero means the expected value of the weight is zero so the square of that is zero an expected value of the input is zero the square of that is zero so what am i left with summation where i variance of xi into variance of wonei  ok now i am going to assume that the var iance of xi is equal to the variance of x that means it is the same for all the is so i had this remember i ha d these five hundred inputs so i am assuming that for all the inputs the variance is the same they all come from a similar variance distribution and i am also going to make the same assumption for the weights fine and then i end up with this nea t formula that the variance of sone one is equal to n times the varianc e of w into variance of x right because i assumed that all these terms are equal and there are n such terms everyone is fine with the m aths so far with the assumptio ns that we have so in general for any of these neurons right i nstead of just seleven i could take any sone i and this is what the variance is going to be variance would turn out to be because i have assumed that all the weights an d all the inputs come out from the same variance distribution ok f rom a distribution having the same variance now let us what would happen if this quantity is very gr eater than one the variance of sone i would be v ery large right and what would happen if this variance tends to zero variance would be very low so i am just giving you two extremes to build the intuition and let us see what we are going to do with that intuition fine now let me add one more layer and see so i have added one more layer and using the same procedure as above he will arrive at variance of stwo one is actually given by this formula and actually what has happened here is that this is s i had x i earlier but now instead of x i  i have sone i because those are the inputs to this layer right so this is exactly the formula that we had arrived at earlier assuming zero mean and the same variance for all the weights and the i nputs and i am arriving in the same formula for the next layer where instead of x i have sonei so this will result in this quantity ok but i already had a formula for a variance of sonei what was that n into this quantity so i will just substituted it there so i can say that variance of stwoi is actually equal to this i just substituted this val ue so that turns out to be i have a square here when i have two here so you see where i am headed with this what will be the variance of s k i  this raised to k and is on everyone gets this ok i can just continue the same analysis and i have assumed that these weights and alw ays are the same variance right so in general i can say this ok now can you tell me something about when would this variance vanish when n variance of w is student l ess than one less than one ok a nd which is the thing that we should aim for you would want this quantity to be equal to one in which case it will neither blow up nor shrink fine so so it to ensure that the variance is the output of any layer does not blow up or shrink we should ensu re that n into variance of w is equal to one right so what is this this just take a minute to understand this i am saying that i am going to initialize my weights so i should initialize them in such a way that the weights are coming from some distri bution like we saw that the distribution was a uniform distribution from where i was drawing the weights so they are coming from some distribution i should try to draw them from a distribution such that this condition holds if this condition holds the n across layers my activations will not blow up or shrink though that is exactly what was happening in the earlier case w hen i was doing those bad initializations with small values and large values so let us see how to do that so what i am going to d o is i am going to consider a r andom variable z ok where is z comes from a normal distribution ok and i am going to scale it is value i will draw from there and then i am going to scale it by one by square root of n what is n number neurons in each laye r r ight h ere it is the same across all layers but it could also be different so i am considering a particular layer and n is the number of neurons in that layer and now if w is actually equal to z by square root of n then i can write that n into varia nce of w is actually equal to this quantity everyone is fine with this there is no trickery here i am just saying that why i am doing this is not clear that will become clear but at least what i am doing is clear i am drawing the weights i am taking a random variable z which comes from a normal distribution and then i am setting my weights to whatever values i have drawn i just divide them by the square root of n now let us see what is variance of a z a square into var iance of z hey that is a basic formula all of us know this so now what is variance of z by one in z into one by square root of n one by n into variance of z right so the n and n cancel and what is variance of z what did i assume about z it came from a no rmal distribution zero mean and unit variance so variance of z is one that means this quantity n variance of w is going to be one if i have initialized my weights such that they are equal to this  r ight and now do you see whether the weights are very small very large or what are they some now they made the weights dependent on the number of neurons so if you have very large number of neurons you are drawing drawing weights such that or you are initializing weights such that it is some normal variable di vided by the square root of n right so now when you do this summation w i x i your summation cannot blow up because you have already divided it by n how many if you get this so this is a standard way used for initializing weights how many if you tri ed this for your back propagation assignment why did you try this ah student refer time twentythreetwelve because you are having some problem s with saturation i guess right so this is how you should initialize your weights this is more or less the standard tec hnique and some variant of this right because instead of n you would have this fan in and fan in out it how many weights are coming in and how many weights are going out so you make it proportional to the square root of n into k or something like that right so but in general this idea right of course this proof we arrived at it with lot of assumptions but we at least got to some principle way of initializing weights and this is a largely used standard thi s and some variants of it refer slide time twentythreefortythree so now let us see if i actually take the same network that means five layers five hundred neurons at every layer and then initialize it using this so this exactly what i had told you right that take it from a unit distribution sorry a normal distri bution and then divided by the square root of the number of neurons in that layer and now let us see what happens across the five layers you see what happens we get this good variance in the activation functions they are not all going to zero or one or point five  right so this solves the purpose for tan h activation and also for the sigmoid activations y ou see a good spread in the weights an d remember actually for sigmoid although these values look close to each other but this i s the zero to one range this is actually minus one to zero which will not happen for sigmoid so within the zero to one range you get a good spread i f you initialize the weights this way but it turns out that this initialization does not w ork for the relu function in the relu function you still see this effect that you started off with a good spread but as you keep going across dep th s this spread disappears why would that happen to someone gave an intuition for this and is again one of those heuristic things that in the case of relu you need to account for this divided by half because half of the relu is not active right half of the relu is zero so you need to account for that fact and do this simple trick t hat instead of taking the square root of the fan in you take the square root of fan in by two because you know that half the times it is not go ing to produce any output right so that is a very simple heuristic that someone tried and that leads to bett er activation functions better activations across all these layers right so as you see across all the layers the spread is good now s o the same idea ok so now you have a good way of initializing neurons so this should help you in your future assign ments fine so this is how what you have learned about how to initialize your weights and it makes a lot of difference to how your network will behave right  and that is what the i was trying to show that by computing these activations across differe nt layers and i showed that as you change these initialization s strategies you get better activations"}
{"audio_filepath": "Data/Preprocessed/CNNs (success stories on ImageNet)_90.wav", "duration": 1204.0, "text": "so now we will g o to the nex t modulewe will talk about so me succe ss stories on imagene t rig ht s o thi s is the challenge whic h actually made convolutiona l neural networks very famous back in two thousand and twelve so they are going to look at some algorithms in fact two more hopefully today s o this is the story right so there is this challenge or competition called imagenet large scale visual recognition competition right that is what ilsvrc stands for and this was a data set created which had one thousand categories it actually has ten thousand categor ies but in the competition we use only thousand of those categories yes so one thousand plus one thousand i think the roughly the data set size is one million and so that is what was used for training a classifier and i am talking about two thousand and ten the pre deep era right i mean so of course deep era networks existed at that time but the participants and these challenges and that time were relying on the classical machine learning approaches so what was that approach take the image student refer time onesixteen featu re extract features which features student refer time onetwenty predominantly student sift and hawk sift and hawk features were the predominant features at that time and then you train a classifier on top of that and then you use things like on symbols or better handcrafted features and things like that certain more tricks on top of that so that with that on this data in two thousand and ten the error was twentyeighttwo percent that means if i give you a test set of one thousand images you will make two hundred and eightytwo errors on that right that is what this means then in eleven there was still some progress this was again pre deep era and there was this error came down to twentyfiveeight and then in two thousand and twelve there was this alexnet which was a deep convolutional neural network applied to the task of imag e classification and it gave a dramatic reduction right from twentyfiveeight to sixteenfour and was i think absolute in absolute terms eight to nine eight to nine percentage better than the best system in that competition that year ok so that was in two thousand and twelve in two thousand and thirteen there was further improvement on a different architecture for doing this and that give a further error reduction of elevenseven then in two thousand and fourteen there was vgg net so these are all three that we are going to see today which give a further error reduction of seventhree then google decided t o join the party and they make it sixseven and as i have said before then afterwards microsoft got crazy and they brought it out in threefiftyseven and this is when we started making claims that a convolutional neural network has become better at this task than humans rig ht because if you show these one thousand images to a human even a human is bound to make a threefive percent more than threefive percent error that means because some of these images would be blurred so i would not be very sure whether this is a bulldog or a differe nt type of dog or something like that right so i even a human cannot really recognize it correctly and that is the whole hype around how convolutional neural networks have beating human level performance on this particular task right and let us see so  this was all the shallow pre deep era the first architecture was eight layers and i think this was called a varied no this probably not this yeah the second architecture was also eight layers then we had nineteen then twentytwo and then one hundred and fiftytwo right thats how the progress has happened so these are all the architectures that we are going to look at today or at least some of them today and the rest maybe tomorrow ok so we will start with alexnet and i am going to tell you the exact architecture of alexnet what was ref used what did it actually use so the input was an rgb image so it had a depth of three and it was two hundred and twentyseven  two hundred and twentyseven that is what the data set input was all the images in the data set were to two hundred and twentyseven  two hundred and twentyseven cross three so the first thing that they did was they decided to use ninetysix filters can you read that anyways i will say it outright so they resided to use ninetysix filters with a spatial extent of eleven cross eleven a size of four and padding of zero ok so the moment you see size a stride of four what do you know is going to happen there is going to be some shrinkage roughly by how much one fourth right so now can you compute these three things what was wtwo what was htwo and what was the number of p arameters in this layer we will do it for a few of these layers and then i will just rush through that so whats wtwo going to be you have already done this computation right the exercise that we did was exactly this computation so there was fiftyfive cross five five and what is the depth going to be i want everyone to say that student refer time fivefifteen ok and what is the number of parameters student refer time fivetwenty ninetysix into student eleven eleven into student eleven eleven into three dont forget the depth the depth is three here so that is the number of parameters that they had in this layer ok eleven into eleven into three into ninetysix now what is the next layer going to be a max pooling layer ok so they had a max pooli ng layer where they used a three cross three max pooling that means you are going to pick up max from a three cross three grid and the stride was two that means we are going to get half the output and now can you tell me what wtwo htwo would be roughly half of fiftyfive fiftyfive rig ht so twentyseven twentyseven and what is the number of parameter is going to be dont be lazy everyone be say it student zero zero right so that is the max pooling layer now what is the size of your input volume at this point student twentyseven twentyseven cross twentyseven cross student ninetysix ninetysix as opposed to the original input which was two hundred and twentyseven cross two hundred and twentyseven cross three so as you keep progressing your width and height is decreasing but your depth is increasing because you are using more and more filters to capture mor e and more patterns in the images now so you have twentyseven  twentyseven  ninetysix then they decided to use two hundred and fiftysix filters each of size five  five with a stride of one and pad ding of zero ok is it right so how many parameters do you have now student refer time sixfiftythree two hundred and fiftysix into student five into five five into five into student ninetysix ninetysix so thats the number of parameters that you will have and the size s ince would decrease only by one right because you have a stride of it will decrease by two because a filter size is five and you have a stride of five ok so these are the number of parameters we had zerosix million parameters in this l ayer what is the next layer going to be pooling so you do a max pooling again you do a three  three you do a stride of two so your width in height is going to decrease the depth does not change remembe r in max pooling the depth does not change because the max pooling operation is per feature map it is not across the depth fine then use a three  three filter and three hundred and eightyfour of those so how many parameters woul d you have student refer time sevenfortythree three hundred and eightyfour into three cross three into two hundred and fiftysix the depth so now you guys get it so i will not bore you anymore and then you have a convolution operation again which is a three hundred and eightyfour convolutions each of size three  three and so many parameters followed by a convolution operation again followed by a max pooling operation then followed by a fully connected layer so what would i do to this two hundred and fiftysix  two  two i will fatten it so i will get what dimensional output student one thousand and twentyfour one thousand and twentyfour two hundred and fiftysix into two into two so this one thousand and twentyfour dimensional vector i am going to fully connect it to a four thousand and ninetysix dimensional vector how many paramet ers four million right four into ten raise to six right so roughly four million then you have another four million another four thousand and ninetysix vector fully connected how many parameters student sixteen million sixteen million then you have the one thousand classe s that you are interested in right so again fully connected so you get the full architecture anyone has any questions no one wants to know why this particular configuration among all the possible configurations why not ten layers why not first eight cro ss eight filters why not nine cross nine filters unfortunately no one knows laughing student refer time eightfiftynine so this i mean see this what this is what would happen right now we get into something known as hyper parameter tuning right so what are the h yper parameters in this network the kernel size is and the number of filters right so you would have tried a lot of these things evaluated on the validation set seen which one gives the best accuracy and then chosen right so that is probably what wo uld have happened but there is not enough insight into how this particular architecture came up apart from some things right that three curves three neighborhood sounds reasonable initially when you have the full image you use larger filter sizes because yo u want to capture a lot of things there but once the image has shrunken you use smaller filter sizes so those are some rational decisions which look reasonable but why these three convolutional filter layers back to back instead of convolution max pooling convolution max pooling and so on right so the some of those things are not clear so just in case you are wondering do not wonder this is just the architecture this is known as modestly named as alexnet so that is laughing yeah and so i said that this has eight layers but you clearly see more than eight layers here so why did i say that has eight layers which are the layers we are not counting student refer time tenten why student refer time tentwelve because they have no parameter right so when you count the number of layers you only count those layers which have parameters so you have five convolutions and three fully connected layers then so the total number of parameters in this network is twentysevenfiftyfive million parameters an d ok at this point i will and obviously you notice that most of these parameters were there in the fully connected layer so you had four million here then sixteen million here and then again four million here right so roughly twentyfour million of the twentyseven million par ameters were there in the fully connected layer you see that skew in the number of parameters ok and i will just look at the fully connected layer again so the last max pooling layer actually gave you a two hundred and fiftysix cross two cross two output you just flatten it to get a one thousand and twentyfour dimensional vector and then you connected fully to the four thousand and ninetysix vector right so that is what i mean by a fully connected layer why do you move max fully so the reason for that is basically to shrink the size of the image right because after that if if you keep working with this size right then the number of parameters is going to really blow up a by using a larger stripe yeah both of them are feasible right so now see from here remember that we had the ori ginal image sizes two hundred and twentyseven cross two hundred and twentyseven and by the end we were just left with two cross two and then adding a fully connected layer on that makes sense right if i had not done this shrinkage throughout either by increasing the stride of the convolution layer or by d oing max pooling right then you would have left with something of the order of two hundred cross two hundred here and then you have to do a fully connected on top of that is just infeasible right it just throws away all the hard work that you have done by doing weight sharing and sparse connectivity right so that is not feasible there are also papers with say which i think it is titled fully convolutional neural network which does not have any max pooling layers and they show that that also works fine in fact whe n we see vgg net we will see that it has back to back convolution layers and very few max fully layer right so these are all things which people have trained not so many years two years the challenge came out in two thousand and ten and in two thousand and twelve this was used right so  it is like not really a large gap right and if you read the original paper they had to do a lot of tricks to actually make this work it was not as simple as i am showing it of course now with all the stability which comes from these platforms tenso r flow pytorch you can probably just go and implement this as it is and you should be able to reproduce the results but six years back that was not the case there was a lot of hard work involved in getting this too work and they this was also the paper  which introduced the relu non linearity in the context of convolutional neural networks right so they had to change from sigmoid or tan edge to relu a lot of these small small things which they had done and at that time it is also not possible with t he existing hardware to train this on the given gpus that you had at that time so they had to do some splitting across gpus and so on so it was not as simple as it is today with all the hardware as well as api developments or platform developments aro und this right so probably that is why it took two years to yeah sure so each of these things so after you do the convolution operation you pass it through the relu non linearity ok so what does that mean is that the convolution operation gave you a feature map every entry here was just a weighted average of the neighbors right you take this entry or rather you take this feature map and create a new feature map where every entry here is the sigmoid of every entry here do you get that or not sorry sigmoid some non linearity and they use the relu has the non linearity so you do get everyone gets this so all the convolution layers are followed by a relu non linearity layer so you get this volume pass it through the relu and get a new volume b ut i have just shown that as a single operation it is before pulling so this was the fully connected layer so now we look at the next architecture which is zfnet now i am going to compare zfnet with alexnet so on the to p you will see alexnet on the bottom you will see zfnet ok so again the input was the same two hundred and twentyseven  two hundred and twentyseven  three now instead of eleven cross eleven filters zfnet decided to use seven  seven filte rs and their rationale was that you do not need such large neighborhoods you do not need as small as three  three but probably you need at least as much as seven  seven you do not need eleven  eleven so that is the first change that they did and that would also result in some parameter pruning right because the number of parameters now would be seven cross seven into three so the difference in the number of parameters at this layer for zfnet which is at the bottom and alexnet which is at the top would be this how many of you get this ok so thats in the difference in the number of parameters so now the output volume still remains the same its fiftyfive  fiftyfive  ninetysix then again they had the same max pooling operation this layer there was no difference between zfnet and alexnet and then after that you had again layer three which was exactly the same as alexnet refer slide time fifteen twenty then layer four again the same as zfnet afterwards layer five instead of three hundred and eightyfour filters they decided to use five hundred and twelve filters the rest of the thing remains the same that means the size or the spatial extent of the filter remains the same that again results i n some difference in the parameters so thats the number of parameters that got added in zfnet as opposed to alex net and of course theoh sorry sorry oh sorry the bottom one is a zfnet yeah that is correct sorry so in zfnet you had five hundred and twelve filters as opposed to three hundred and eightyfour filters in alex net ok is it fine and then the next layer again instead of three hundred and eightyfour filters they had one thousand and twentyfour filters then again instead of two hundred and fiftysix they had five hundred and twelve filters and then a max poo ling layer then the same dense fully connected layers ok so everyone gets this this is the difference between the two architectures and this led to that difference in the error of around three to four percent is that we have seen e arlier so difference the total number of parameters was onefortyfive million and of course zfnet had more parameters because is that it has these more filters in the deeper layers ok so we go to the last point which is may be mor e in that vgg net so again in the case of vgg net the input was ok so i just want to i will not see it refer time sixteenfortyone in so the input was again the same it was rgb cross two hundred and twentyseven cross two hundred and twentyseven so this is what the vgg arc hitecture looks like they have so in vgg network throughout ok wait so how many layers this zfnet have eight so you only count the pink boxes because the those has ones which have two parameters now vgg net has slightly more number of layers but in all the convolution layers they use three cross three filters right from the beginning they use three cross three filters ok so you have the first convolution layer then another convolution layer another convolution another max pooling layer followed by two convolut ion layers then a max pooling layer followed by three convolution layers max pooling just keep adding box is writing just because you can and then you have the fully connected layers so again there is not much intuition for why sixteen in fact later on so meone came if this is the vgg sixteen architecture because it says sixteen layers later on some of someone came up with the vgg nineteen architecture which has nineteen layers right so a lot of this is data center even right so you try your best to get the best possible a ccuracy on the imagenet data and that is the architecture you came up with right but as long as how many of few feel comfortable with what is happening right and i mean when i say comfortable i mean that you really understand the gory details of what is happening at each layer in terms of input volumes output volumes number of parameters how are you going to train this network end to end can you see how are you going to train this so you will get some loss here that is going to propagate all the way back to the first layer right and this propagation is going to happen over some sparse connections that fine now this is one very important point that i have skipped and which none of you is questioning is everything that is happening here differe ntiable student refer time eighteenthirtyeight what happens to max pooling is max pooling or differentiable operation so i am going to ask you this how are you just note this down how are you going to back propagate to the max pooling layer because you need to see whether the max pooling layer is actually a differentiable layer or notso here i just some statistics about vgg net everyone is writing that down laughing this perhaps means i will not ask it the kernel size is three cross three throughout the tot al number of parameters in non fully connected layers is sixteen million the total number of parameters in fully connected layers is one hundred and twentytwo million so you see that this fully connected layer is really a problem it like really hogs all the lime that it has the m aximum number of parameters there right and so and the most number of parameters are there in the first fully connected layer because you have this five hundred and twelve  seven  seven you remember then alex net and zfnet the last layer was two hundred and fiftysix  two  two which has definitely more manageable than this layer which has grown almost eight times in size but not even eight actually four into four into two right sixteen thirtytwo times in size right so that is really blown up the number of parameters in the first fully connected layer so you just imagine the i mean you have such a deep layer and then you realize that all the main number of parameters are there in this one particular lay er everything else is much fewer parameters or orders of parameters less number of parameter is less then this one fully connected layer"}
{"audio_filepath": "Data/Preprocessed/Feedforward Neural Networks (a.k.a multilayered network of neurons)_23.wav", "duration": 1081.0, "text": "so welcometo lecturefour of csseven thousand and fifteenthe courseon deeplearningtodaywe willtalk aboutfeed forwardneuralnetworksand backpropagationso quickrecapof the story  so far it so we started with mp neurons we saw there were some problems with the mp neuronstheycouldhandleonlybooleaninputsand booleanoutputsandthreshold needed to be hard coded so from there we moved on to perceptrons which allowed for real inputs real outputs and sorry real inputs and binary outputs and we also learned an algorithmfor learningtheseweightsand parametersrightso we needtherewasno need to hand code these parameters anymore but then wefoundthat fora singleperceptronthereis a limitationit cannotit can only deal with functionswhichare linearlyseparableso thenwe wenton to amulti layernetworkof perceptronsand we provedby illustrationthat itcan handleany arbitrarybooleanfunctionwhetherlinearlyseparableor not thecatchis that you will needa largenumberof neuronsin the hidden layerrightthenwe alsoobservedthat perceptronshavethis harshthresholdinglogicso whichmakesthe decisionsvery unnaturalit is zerofortynine itis negativezerofiftyone is positiveso you wanted somethingmore smooth so the smoothestapproximationto thisstep functionwhichis the perceptronfunction was a sigmoid function sigmoid is afamilyof functions and we saw one such function whichwas logisticfunctionand then wesaw that itis verysmoothnowit is continuous and differentiable nowfor the sigmoidneuronon a singlesigmoidneuronwe sawa learningalgorithm whichwasgradientdescent andwe provedprincipallythatit willalwaysgo in the direction where the loss decreases right s o that iswhat is the basis for gradient descent and then wegraduatedfroma singleneuronto a networkof neuronsand madea case that such a network of neurons with enough neurons in the hidden layer can approximate any arbitrary function right ok so i have told you that it can approximate any arbitrary function what does that mean and what is the thi ng in the network that does all this all the tower functions and the tower functions depend on weights and biases so there in that illustrative proof again we were adjusting the weights and biases by hand right we knew that we wanted these very tiny tower functions and we were doing it now from there where should we go student refer time twothirtynine we need an algorithm to learn these weights and biases right so that is what back propagation is so today i am going to formalize these feed forwa rd neural networks we just did it by illustration the other day i will introduce you to the terminology and see what the input outputs are and so on and then we will look at an algorithm for learning the weights in this feed forward neural network re fer slide time threethree let us begin so this a lot of this material is inspired by the video lectures by hugo larochelle on back propagation he has a course on neural networks it is available on youtube you can check it ok so let us first begin by introducing fee d forward neural network right so what is a feed forward neural network the input to the network is an n dimensional vector so ok that means my input belongs to r n that fine the network contains l minus one hidden layers where do you already know what hidden layers are right we have been defining that terminology since multi layered perceptron so you have these hidden layers and there are l minus one of these and then it has one output layer containing k n eurons ok those are the feed forward neural network looks like what is missing here student refer time threefiftyseven the weights right so each neuron in the hidden layer ok before that each neuron in the hidden layer and the output layer can be s pli t into two parts right so i will call the first part as the pre activation and the second part as the activation have you seen this plate before right what does the pre activation do student aggregation aggregation and what does the activation do s tudent non linearity non linearity right so we have this pre activation and activation at every layer and a i and h i are vectors is that correct because this entire thing or rather this part is h one and this part is a one both of these are vector s right and for this discussion am going to assume that everything till here belongs to r n so the input was r n and all the hidden layers also have n neutrons is that fine so please pay a lot of attention to this couple of slides because this is go ing to stay with us for the rest of the lecture and perhaps two more lectures and even for the course alright so this is very important that you understand this the way we are defining a feed forward neuron network the input l ayer can be called as zeroth layer what i mean by that is t hat i could refer to this as h zero ok there is no a zero h zero here because there is no pre activation activation you are just given the input so i just call it as h zero ok and the last layer can be called as h of l  right whatever you get from this green part you will call it as h of l  ok what is the dimension of h of l  r raised to k it belongs to r k because i have said here that you have k neurons each corresponding to k classes ok now we have weights between the input layer and the first hidden layer now can you tell me this belongs to r n this also belongs to r n so what is the dimension of w one n cross n right because it contains weights for connecting each of these inputs to e ach of these hidden layers there are n here n there right so it is n cross n and what a re the dimensions of the bias n  one corresponding to each of the hidden inputs fine and this is only for up to this layer because till here i have assumed every thing is n now what about the output layer n cross k and the biases k k dimensional ok so this is what the network looks like but now i have to give you some function so i have just i have shown you a diagram but what does it mean mathematically because remember that we are always interested in writing something of the form y is equal to function of x right and that is not well defined yet so let us start defining that ignore the red portion for now ok i will go over it so each of these activations right or rather the pre activations is given by b i plus w i into h i minus one so what it means is that these activations take inputs from the previous layer multiply by them by weig hts and also add the bias is that clear so let us see it right for example if i look at a one which is this vector so that is three dimensional and assuming it is three dimensional for simplicity so it is a one one a one one a one two a one three right and that is equal to how do you get rid off this b one one b one two b one three plus this matrix multiplication is this clear to everyone i know it is trivial but am still going over it right so let us not ok and then how do you do this matrix multiplication row was mu ltiplied by the column  so this is what you get right and in the end i can write it as this right and this looks very similar to what we have been seeing throughout it from a mp neuron to perceptron to sigmoid neuron and now this case right so it is just an aggregation of all your inputs or weighted aggregation of all your inputs that is the case which i want it to know  and that is obvious now so you understand what these are right so this is r n in our case we have assumed n equal to three what is this  i will keep asking till this is completely fine with everyone r n and this is student refer time eightthirty n cross n and this is student refer time eightthirtyfour n cross one n cross n i mean r n sorry is it fine so everyone understands the operation h appening here it is a weighted aggregation of your inputs so every guy here is a weighted aggregation of all the inputs ok now after that i do h i of x is some function of a i of x ok what does this mean so this is aga in a vector right i have assumed that it is three dimensional so these are the three elements of h i  so  these are the three guys now these are some function of these light blue guys ok now how does that function operate on the vector it operates element w ise not all functions on vectors are element wise but this particular function we are going to do element wise that means that h one one is equal to g of a one one h one two is equal to g of a two and h one three is equal to g of a one three right where if i take g of a one three one of the functions that i could choose is the sigmoid function so it would just be one over one plus e raise d to minus here so what is happening is i am taking this value and passing it to the sigmoid function to get oh sorry am taking this value and pa ssing it to the sigmoid function to get h one one taking this value passing it to the sigmoid function to get h one two right so the key thing to understand here that this is a element wise operation right it is not operating on the vector that does not make sense it is operating on every element of the vector right ok and g is called the activation function it could be logistic tan h linear anything right so we will see some of these functions later on ok now  the activati on at layer i sorry they are supposed to be activation at the output layer the activation at the output layer is given by the final function which is f of x is equal to o of a of so let us see  so  this is a three in our case l was equal to three because we had l minus one hidden layers and the lth layer was the output layer right  so  this is a l  so this is what i have computed here that light green part of the figure that you see right now based on that i want to produce an output so that is someone h ad asked me a question that why do we always choose sigmoid because sigmoid will clamp the output to zero to one what if i want to predict the amount of oil which will not be between zero to one right that is why for the output we will use a special function that will call the output function and later on i will show you that i t depends on the task at hand  so it is going to change with the task that we are going to do right so we are just going to say that the final output which is h of l is equal to so me function of the pre activation at that layer is this terminology clear to everyone how is each function operating  is that clear to everyone and we will see some examples of the output activation function right n ow just f or simplicity am going to remove the xs from the brackets right  s o instead of calling everything ai of x h i minus of x and so on i will just call them ai h i and so and so that just simplifies things but we know that everything is a function of x be cause x is the input and that passes through some functions and we get the final output right so this is the notations that we are going to use is the dimension of everything that you see every variable that you see here completely clear to everyone dimension of a i b i w  h i x everything is clear ok and the output layer has a slightly different dimension than the other layers because there we have k classes as opposed to n neurons everywhere else ok fine now i need to put this in the paradig m that we saw for supervised machine learning what were the five components there data student model m odel student parameters p arameters student learning l earning algorithm student refer time twelvetwentynine objective f unction right o k everyone remembers that ok so i said that we will do deep neural networks and we are trying to write this y hat as a function of x but then what i gave you is just a diagram from which this is not clear whether y hat is actually a function of x how many of yo u think y hat is actually a function of x very few ok so let us see what exactly is o ur model assumption here right so the question let me repeat the question just to be clear so i said that they are given some data we do not know the true relation between y and x we make an assumption that y is related to x using some function f right and it is has some parameters and then we like to try to learn the parameters of that function so what is the function here so what is your model what have you assumed as the model can you write y as a function of x if yes what is that function how many of you have the answer i think you have your answer ok i think i cannot wait more so i will give you the answer then it wil l become very obvious ok so this is how y is a function of x right so let us see what is happening i took the original x which was this i transformed it added b one that was the dash at layer one student refer time thirteenfiftysix no this thing student refer time fourteenzero pr e activation at layer one i passed it through the activation function right ok now again let us be clear about the dimensions what is the dimension of this student n n  what is the dimension of this n cross n so what is t he dimension of this product student refer time fourteentwentyone n what about this so what is the product the final dimension of this r n now you are passing it through a function g that function is operating element wise so what is the output dimensio n student r n r n so this is again r n ok now this student refer time fourteenfortytwo so now you see the whole story right so now this n cross n guy multiplies with this n guy again you get a vector again pass it thr ough a non linearity was it s o hard it is obvious now right you just take an x just note down all the transformations that you have done that is what a function does right it passes it through the through first a linear transformation this is a linear transformation then a non linear transformation then again linear non linear and so on so just see how far we have come from where we started off right we started off with simple things like w transpose x right that was the perceptron model where we were taking decisions ba sed on w transpose x and we were saying y is equal to one if this quantity is greater than something y is equal to zero if this quantity is greater than something right that is why we started off with we made it slightly more complicated by doing this thi s was sigmoid neuron now from there where have we gone to this right so we have increased the complexity of the network with great complicity complexity comes great student refer time fifteenfortysix no power right we have already seen the representat ion power of deep neural networks right so it comes from this complexity that you have you have a lot of linear and non linear transformations right that adds to the complexity of the network it has more parameters at each linear transformation you h ave some parameters and you are also using a lot of non linearity so that is the reason why deep neural networks are so powerful right do you get that ok so just to impress again right so any machine learning algorithm that you have you should be a ble to write it in this form right that y is a function of x with some parameters and then your job boils down to learning these parameters right it just happens that here y is a very complex function of the inputs is that clear ok so i am not devi ated from the original story i am still being able to write y as a function of x with some parameters ok what are the parameters student refer time sixteenfortytwo all the ws all the bs right so w one to w l and b one to b l and the algorithm that we are going to see today for learn ing these parameters is called g radient descent but we will use it with back propagation where back propagation will help us to compute gradients it is ok  it does not it does not make sense at this point that is what the lecture is supposed to be about right so and what is an objective function student refer time seventeenseven loss function so i could just go with this loss function right ok there is an error here i thought we corrected this there is a summation so actually these are vectors right so this does not make sense so you should hav e summation j equal to one to k yij minus yi j does that make sense so this is the vector y hat ok for the i th example it will be called a s y hat high i which will have k elements right so y hat i one y hat i two up to y hat i k right so that is what my predictions are and i will have the corresponding true vector also i am trying to take the difference between them which is going to be a n element wise difference everyone understands the error in the slide how many of you do not get it how many of you get it if you do not get it please raise your hands it is a minor thing i can correct it and how does deep neural networks fit into these this paradigm"}
{"audio_filepath": "Data/Preprocessed/Output functions and Loss functions_25.wav", "duration": 1565.0, "text": "we go on to the nex t module where we will be talking a bout output functi ons a nd loss functions the question that we are going to focus on is how to choose the loss function but i will show you that it is tightly coupled with the choice of the output function also remember that we had said tha t we have a special o function as the output function i have not told you what that o is and now that is what we are going to define now the choice will be loss function actually depends on the problem at hand and that is e xactly the question which had come up right that in some cases it is to have sigmoid as the output function because your values are between zero to one but whatever there are cases where yo ur output is not between zero to one r ight so it definitely depends on th e choice of the on the problem that you are trying to solve so we will illustrate this with the help of two examples and these two examples will cover a broad range of problems that you will encounter or if you are working in machine learning right so the first problem is again you are given the input as movie you are using a neural network with l minus one hidden layers and an output layer y hat right so this is sorry this is a true one so you have an output layer and the output layer is going to pre dict the imdb rating the critics rating and the rotten tomatoes rating is that fine ok so what kind of problem is this peo ple have done machine learning this is a regression problem and notice that the output values that you want to predict are not bounded it by zero and one they are still bounded by one to ten but in general you could imagine that there could be problems so there are no bounds at all right it could be a very large number is that clear n ow here yi belongs to r three so remember in all th ese cases we were assuming that we just want to predict one value but nothing stops you from predicting multiple values at the same time  so your output is now three dimensional you are taking an n dimensional input and trying to predict three values from it o k fine the loss function should capture how m uch yi had deviates from yi o k s o this is a valid or maybe we corrected on this way  ok so this is the formula wh ich was supposed to be in there right so you take you have predicted three values and you kno w the true three values you just tak e the difference between these r ight i s that clear the first element of the predicted value minus first value of the actual value and so on for all the three values that you want to predict now you have a loss function but what should be the output function in this case can it be the logistic function yes no it will be bounded between zero to one and you know that your output cannot be bounded between zero to one ok so in such cases then what is a good output function to use one option is to scale it so i will keep that aside why do that it is unnatural and you are actually clamping it and then trying to scale it right so can you do something more natural in that just use a sum whic h is li near function right so what we could do is you could have o as a linear function so what that means is  again remember that this is a of l ok and i know all the computations that have happened so far a linear transformation non linear linear non linear and then again li near so i have computed a of l  from that i want to compute the final output right so i could just have it as a linear function of the input which is a of l in this case does it make sense how many of you feel it makes sense ok why becau se now it is no longer bounded right you could this linear transformation your weights could be adjusted in any way to get a value whatever you wanted whether you wanted between one to ten or one to one hundred or one to one thousand these weights could be adjus ted to do that r ight so at least you are not bounding it and it is free to learn what is the range from the data it should be able to run but how should you adjust these ws so that you get the desired range now tell me why would it not happen that you learn ws you start predicting values like one thousand ten thousand and so on in this particular case where yo ur input is bounded by one to ten sorry your output is bounded between one to ten why would it happen i this is my argument and you prove me wrong right i wo uld say that if you have chosen a linear transformation which is not bounded i the n network could learn weights which start producing a rating of ten thousand twenty thousand and so on because it is not bounded but you know that that is wrong because the ratings can onl y be between one to ten  so why would that not happen because you are minimizing this loss function r ight so if you start predicting values like ten thousand  when your actual rating was nine then you have a ten thousand minus ninety whole squared loss that is a very high los s so it will start moving y ou away from that configuration r ight so the training is always guided by the objective function so if your training happens we ll it will try to prevent this now suppose let us take a simple thing rate that you are given a our same ball example for probability so you are given an urn which has balls of three colo rs say black white and yellow and you have to put the balls in that so you know that the true probability distribution is actually zerothirtyfive zerotwentyfive and zerofour for red black and white ok t his is the true probability distribution you have put s ay thousands of balls in urn  now what you do is you just allow me to peep into the urn or you allow me to take some samples from there you tell me take these one hundred samples and you ask me t ell me what this probability is right so this is the true probability that you know is true right because you know it because you have est imated now you just give me a small sample from there and ask me to esti mate it and based on that i actually estimate this ok so there was a true probably distribution and an estimated probably distribution now i want to find out how wrong i went right afterwards you tell me the answer you tell me that this is what the t rue was and this is what you predicted now i want a way of c omputing how wrong i was right so how do i do that you already know this and these are two vectors what can i do you could just do the this is valid anything wrong with this in principle n o you could just treat these as any two vectors you have a true value you have a predicted value you just take the square d error difference between them right but you know this is a probability distribution right you should be able to do something better than this you know this is a special quantity this is not just any number that you are predicting you are trying to predict a distribution so you should be able to do s omething better than that right so that is what we want to see how to do somethi ng better than this that is what our quest is now again why we are at this right i also want to make a because this is something people do not immediately understand so i just want to make a case for something else so i will just do that ok now suppose there is this ipl ok and there are four teams in the semifinal let us call them a b c and d o k now i was not in town after the semifinal so i just know the results up to semifinal and then the finals also happen and one of these teams wins le t us call it the b team right the b team wins can you express this in terms of probability can you express this in terms of distribution what do you mean my zero and one b has won so it is a certain event because it has one now so what is going to be the distribution zero one zero zero r ight so this event happen s with one hundred percent probability ok n ow the same case can you ok so now let us do the same thing that is as i said i was not in town right and you asked me tell me which team would win that is i know th ese four teams have qualified in the semifinals and i know who the players are and so on and with my limited knowledge of cricket i will predict something right so say i predict this ok can you again tell me how wrong i was you know what the true label i s and you know what i predicted you can tell me how wrong i was ok so the case which i am trying to make is that even if the event is certain you can still write it as a probability distribution where all the mass is allocated to the correct output  c an you relate this to a classification problem when you see training data you have already observed it suppose there were four classes possible apple orange mango and banana if you have seen it is apple and if you ask you what is the distribution wh a t will you tell me zero one zero zero you will express it as this one hot vector where all the probability mass is concentrated on the guy which is correct right so even certain events which happen with certainty you can write them as a distribution rate whe re all the masse s are located on the true label  so that is how all classification problems when you are dealing with multiple class classification problems it is often the case that you will write it as this that your true label is given to you in this format there were four possible events four possible classes or k cost possible classes ou t of which only one is correct and then you make a prediction and you want to now find out how different was your prediction from the true label you are trying to get the set of how this relates to a classification problem and this is that is why this is of interest to us ok so this so we will see this soon now the next thing that we need is how many of you know what is entropy forget about cross just entropy ok that is why i have left two slides intentionally blank ok so so now let us see where i go with entropy ok how many of you know what is expectat ion please  fine so again the same thing now i knew that this was the distribution which i think i am into gambling am not i am into gambling and i try to bet on these teams and i bet some amount on each of these can you tell me what is the expected reward that i will get so what am i saying wait suppose this is the case that if team a wins i get tenk ru pees or my net profit is tenk rupees if t eam b wins my net profit is twentyk rupees and c and d so on right you get the setup for every even there is an associated value with it this is the value of event a winning b winning c winning  d winning so the net profit in each of these case so w hat is my expected net profit n o give me a formula sigma overall events right  h ow many events do i have here four right so rather i should say i equal to abcd right probability of i multiplied by the v alue asso ciated with that event so this is how you compute expectation ok everyone gets this so now suppose say am doing this right t here are suppose four symbols i do not know what i am teaching ok so and i am trying to communicate this from a source to a des tination ok and now suppose these are the four symbols that i give and if these one of these symbols is say with probability one and if i transmit it what is the information that this guy gets so this is assumed that a is that sun is going to rise today  if i tell you this when you are sleeping in the night what will you tell me s o basically are n ot gaining any information  well it is a certain event y ou know this is going to happen r ight now one of these events suppose i am going to say that this t here is going to be a cyclone tomorrow morning what is the probability of a cyclone happening in chennai almost one bu t still it is a very rare event  so if i tell you something which is very rare that message has a very high information content right so if event which has a very high probability has a very low information content and an event which has a very low probability has a very high information content right so you can measure the information content of an event so so the point is that what you can have is that the information content of an event you can write it as how many of you get this how many of you have seen this before a ll of you have seen this right so this is the value associated with an event ok now can you tell me wh at is the expected information content for every event now i have given you the value associated with that even so what is the expected information content summation p of i into information content of i and this like and this is of course log right so it would be so what is this called this is called the entropy now what is cross entropy how many distributions are you dealing with here one which is the p distribution which tells you how likely these messages were and based on that you are tryi ng to calculate the entropy of this situation r ight so now what is cross entropy you have a true distribution say you have a predicted distribution  o k this is what you predicted so that means according to your predictions the information content of every event is going to be log of qi bec ause that is what you predicted right b ut what are the actual properties which with these which thes e events are going to occur pi s right so then the expectat ion has to be computed over pi s right so then wha t you will have is summation pi log qi so this is what you estimated the information content to be but the actual events are going to happ en with this probability right so this is your value associated with the event and this is the actual probabili ty of the event right so this quantity is known as the cross entropy is it clear a nd this is a way of measuring when would this be in when would this be minimized when both are same t hat means if your prediction is very close to your true distributi on this quantity will be low minimized actually so that is what we wanted actually you wanted to predict some distributions in all of these cases and you wanted a measure which tells you that this prediction was good and what is the definition of goo d it is as close to the correct value so cross entropy gives you a measure of telling how close a predicted distribution is to a t rue distribution so now instead of using the squared error which was actually pi minus qi right so pi was my true distri bution and qi was my predicted distribution i can use cross entropy which is given by this model and it does the same thing it gives me a principled way of measuring how close my predicted distribution is to my true distribution do you get this refer slide time sixteenfortysix so now so this was for whatever we have done so far right till this point this was for regression right n ow i wanted to enter into classification for which i have built this set up of how to take the dif ference between two distribu tions so now let us consider this problem where we have this situation and which is a classification situation that you are given four possible classes out of which one is the correct class and this is the true data given to you this is the true distribut ion all the probability mass is focused o n one of these classes now we want to given an image class ify this into one of k classes i f you could again use a squared error loss but since we are dealing with probability distributions here we want to use something special so before we get to what the special is going to be what do i first need to tell you in the earlier case my output was not bounded was it also dependent was there any condition on if the imdb rating is something the critics rating sh ould be something else or therotten tomatoes rating should be something else no now in this case is there a tightly coupled behavior between the outputs why because they should sum to one we are trying to predict a probability distribution so the sum should one right so i need an output function which ensures this you get this setup now we should ensure that y hat is also a probability distribution whatever we are pr edicting is also a distribution s o now can i u se a sigmoid function yes it will give me values between zero to one and probabilities are between zero to one but the sum would no t be y so sigmoid is ruled out so what we use is something know n as the s oftmax function how many if you have seen this before please everyone raise your hands otherwise you will get zero on the assignment fine so what does this what does this function actually do let us look at this function right so here you had a l which was say a l one a l two a l three right suppose we had three classes ok so from here i actually want to go to hl or rather i going to want to go to y hat right which is going to consider y hat one y hat two y hat three right it is going to give me probability of each of the three classes let us assume there are only three classes right so now what this function does is how is it going to predict y one hat suppose these values were ten minus twenty and thirty so what is going to be y one hat is going to be e raised to ten divided by e raised to ten plus e r aised to minus twenty plus e raised to thirty  so now you see how the output is comp com puted from each of these values right so why did we do this e raised to stuff why could no t i have just taken ten plus minus twenty plus thirty divided by the sum because we have ne gative valu es so once we take the exponent even the negative values become positive righ t so that is why we need the s oftmax function i hope all of you wrote this in your assignment they did ok so you get this we have a different output function n ow and this output function does it make sense i t gives us a probability distribution now the summation would be one and each of these values would be between zero to one that is exactly what we wanted and now that we have ensured that y and y hat both our distributions what is the objective function that we are going to use cross entropy how many of you convinced it is cross entropy we have two distributions now we saw that a principled way of comput ing the difference between two distributions is the cross entropy so we will use the cross entropy now can you tell me something about this sum there is something special about this sum what are these three true values and these are the predicted values wh at is so special about this sum how many terms are there in this summation  k as many as the number of classes i n this case four  h ow many of those terms will go to zero all but one right except for the correct class everything else will go to zero so this ju st boils down to the following loss function that if l is the true class right for that class yc is going to be one it is going to be zero for everything else that is exactly what this vector tells you only that term will remain so were actually t rying t o minimize this quantity let us see so for classification problems this is your objective function you either minimize the negative log of y hat l or you can say you are maximizing this thing ok now what is this quantity y hat l no it is a predicted probability of the correct event right so this is a probably no wait this is an important question so you have y hat l here and this is a function of i mean this optimization problem is with respect to theta is this a wel l formed objective function does y hat l actually depend on theta yes it does so theta because why i tell is a function of all these things everything here and then a log on top of that right so it is actually a function of all your parameters s o this is a properly set objective function we are trying to minimize or maximize with respect to theta ok and you told me that y hat l is actually the probability of the predicted p robability of the correct class ok hence this quantity is also known as the ml class pattern recognition class log dash of the data student all refer time twentytwofiftythree all good and fill in the blanks so it is a priority of the x belonging to the l th class and then hence y hat l because it is the probability it is the l ikelihood of it is called as the log likelihood of the data log likelihood s o what have we done so far we started with a feed forward neural network we defined the hidden layers and the input layers and the weights and the biases we kept a provision f or the output layer to be something special right then we went to two classic problems one is regression and the other is classification in regression we wanted to predict values of all sorts of ran ges so we decided to use a linear layer there so tha t there is no bound on the values that you can predict and your objective function should take care of where the bound lies it should not allow values which are way off from the true values right and that is why we use the squared error function there the other problem that we looked at was classification where we saw that the label actually can be treated as a distribution where all the mass is focused on the true label and zero everywhere and our job is then again to predict our distribution so we are given the true distribution and we predict another distribution so the output again we want something special in this case which is a distribution so to ensure that use a spatial function which is called the who said sigmoid softmax function  fine  and then we got a prediction which is a probability distribution and then how did we find what was the objective function what is the difference between the true and the predicted the cross entropy right so we use cross entropy as the objective func tion and then with some simplification we realize that it boils down to maximize the log of the probability of the true class or other log of the predicted probability of the true class so now let us look at the summary so if your outputs are real values what is your output activation going to be linear what is the loss function going to be squared error if your output is a distribution what is the output function going to be softmax what is this loss function squared error cross entropy r ight now this grid light actually takes care of a wide range of problems that you will see right think of any examples that have been giving you so far movie prediction or sentiment prediction or image classification or anything al l of that you can fit into this frame of it and so if you know these two loss functions how to deal with them then you can deal with a large class of problems that you are going to deal and for the rest of this lecture which will happen tomorrow we are g oing to focus on this at this particular output function and this particular loss function how do we compute i have a loss function what i am going to compute now t he gradient with respect to all the parameters so this i s what we are going to focus o n right so we have seen the loss function in detail we have seen that the loss function is tightly coupled with the output function now we are all set but given this loss function how do we start computing g radients of this loss function"}
{"audio_filepath": "Data/Preprocessed/Stochastic And Mini-Batch Gradient Descent_37.wav", "duration": 807.0, "text": "n ow we look at s tochastic and m ini b atch versions of these al gorithms refer slide time zerotwenty so we will digress a bit actually we should have ended up somewhere else  but i wa s just going to digress a bit refer slide time zerotwentyfive so this is the original gradient descent code that we had and i have high ligh ted something in this red box so notice that the algorithm actually goes over the entire data once before making an update it has going over this entire for loop which is over all the data points of course in this toy example i had only two data points  but in i practice i will have many many data points i go over all the data points compute the derivatives and then make this one update s tudent refer time zerofiftyseven b ecause that is the right thing to do ok this was the exact formula that we pa infully derived right that the gradient with respect to the loss function right which we had the summation i equal to one to n remember and the true derivative was a sum of the derivatives with respect to all the data points that is what we analytically d erived and hence we are doing that it was that is the right thing to do not for any other purpose ok that is what it should always be right  so that is the right thing to do because this is a true gradient and we actually derived it refer slide time  onetwentyeight and hence this was not an approximation  so all the theoretical guarantees hold if i do this i know that now this is the true gradient or the true derivative and if i move in the direction opposite to the gradient everything falls in place bec ause i proved it using taylor series b ut what is the flip side of this this is the right thing to do  but what is the flip side if you have millions of point we will go over all these million points and make this one update now imagine the consequence when you are in a plateau region right even that momentum or whatever your movement in the plateau is going to be relatively smaller right you are going over these million points and making that tiny delta update right so imagine how much time it wil l take your algorithm to converge you get the problem so the algorithm will take a million calculations and then make one tiny update to your w ok this is going to be very slow can we do something better always right  so let us take a look at stochas tic gradient descent fine refer slide time twothirtysix so i have done a very subtle change to the code what is it do not tell me indentation  but that is what i have done  so you can tell me that  so what is happening now for every data point i am maki ng an update to my w values now the algorithm updates the parameters for every single data point if you have a million data points how many updates will be make in one pass over the data a million for every data point will make an update right  so th at slowness factor in what is known as batch gradient descent right batch gradient descent is when you look at the entire data and then make one update refer slide time threenineteen w hat is the flip side what does this module titled stochastic gradient de scent so what is the flip side these are not the true gradients the true gradient is summation over all the points now this is no longer the true grading this is just a point estimator this is just a approximation of the gradient right and stochast ic because we are calculating the gradient based on a single data point right it i s a sampling one data point and computing the gradient that this is what the entire population looks like right this is almost similar to tossing the coin once and saying that this is what the probability of heads is if it lands at heads then the probability is one otherwise its zero right you see the error you see the problem with that right as opposed to tossing the coin a thousand times and then deciding the probability is just tossing it once  so this is always going to be a erroneous right this this is going to be bad so now there is no guarantee that each step will decrease the loss why because the guarantees were only when you are doing the right thing which was to compute the gradients over all the data points now there is no theoretical guarantees right because it i s all stochastic now  so it is possible that in a particular data point your loss might increase also the overall loss on the data with respect to that point it might decrease but the overall loss right so now let us see this algorithm in action and i want you to make certain observations about this  so this is the code that i am going to run now  so let us see refer slide time fourfortyfour so i will start and you have to observe and let me know and this is really becoming an eye test for all of you  but that is good  so for nothing interesting to observe or already maybe r emember i am running gradient descent this is not momentum not neste rov this is gradient descent ok i have already given you the answers what do you observe s tudent i can still pretend an answer a let us do that we see many oscillations why why do we see the oscillations are these oscillations the same as the osc illations that we see in momentum no these are different everyone gets that right why are there oscillations what is each click here correspond to one data point right  so what is happening here because we are making greedy decisions right w e are loo king at one point this point says to decrease the loss with respect to me move in this direction and we blin dly move in that direction now we look at the next point it says oh no no wait you need to move in this direction so we again move in that dir ection  so all these points are actually trying to just make things better for themselves they are not thinking about what is happening to all the other points in my data right  so all these points are actually competing with each other so some decisi on which i took with respect to where to move which was locally favourable for one of these points may not be good for the other point right hence i keep these tiny oscillations which i make these are the stochastic noise that you are seeing now now c an we reduce the oscillations by improving the stochastic estimates always yes fine  so let us see what do i mean by that refer slide time sixfiftyseven so we look at a mini batch version of this  so what i am going to do is instead of  so this code is actually for mini batch stochastic gradient descent it i s a very minor alteration on the stochastic gradient descent i will just let you stare at it for a minute or so what i am doing here is i am instead of doing it for every point i am waiting fo r a certain number of points and then making the update right that is what i am doing here now for this i have kept k equal to two what does that mean  i look at two points compute the derivatives with respect to them and then make an up date for two point s at a time what do you expect no what do you expect with respect to this code refer slide time sevenfortyseven so let us see we will try to run this now and you will start seeing a red curve here and make some observations about this  so this is the red c urve yeah its visible oops i do not read any of those s tudent refer time eightseven i f you need to fix this right these bul let us should come only after the curve has finished this journey ok do not read any of that ah  so what do you see about the r ed curve it i s completely contained inside the black curve  that means its oscillations are smaller than the black curve right does that make sense why this is happening because now you are not listening to just one point you are listening to two poin ts and then at least you are doing something better right instead of just taking one so what is the analogy with respect to our coin toss experiment you are tossing the coin twice and then deciding what is the probability are heads or tails right  so i t i s always going to be slightly better than tossing it only once right and now what would happen in the limit if i keep increasing this you will end up with a batch gradient descent where you look at the entire data so looking at only one data point is bad because it i s very noisy looking at the entire data is bad because it i s very time consuming  so you need to do something in between which is mini batch gradient descent ok and typically you look at values of sixteen thirtytwo sixtyfour but it also depends on the amount of data you have and if you have a billion points you might actually want to look because if you have a billion points and you have a batch size of sixtyfour you will take one billion by sixtyfour times to finish the data once so you might want to keep a larger batch size at that point right  but just ignore that  but you will try different batch sizes and see which one works better  so in the assignment i will be asking you in to experiment with bad sizes yes ok no sorry wrong question i will be asking them to implement stochastic and mini batch also or only vanilla s tudent mini batch m ini batch fine that is fine ok  so you will see this in your assignment  so everyone sees what was the difference between stochastic and mini batch y ou have better estim ates now and therefore this red curve is contained inside the black curve fine refer slide time teneight so you have some things to remember one epoch get used to this terminology one epoch is one pass over the entire data one step is one update to the parameters n is equal to the number of data points and b is equal to the mini batch size now you have to fill in the second column in vanilla or the batch gradient descent what is the number of steps that you take in one epoch s tudent one one i n stochast ic gradient descent s tudent n n n s tudent n i n mini batch gradient descent s tudent n by b n by b everyone gets that  so get used to this ok  so this epoch step batch size all this is something that you will see regularly when you are reading pape rs on deep learning fine refer slide time tenfiftythree so similarly we can have the stochastic versions of momentum based gradient descent and nesterov accelerated gradient descent refer slide time eleventwo so these are just the codes it i s very easy to see what is happening here again basically this is just an indentation right  so if you look at the difference between the two codes i have just indented it inside  that means i am making these updates for every data point right and same thing you c ould do for nesterov also refer slide time eleventwentyone now let us see ah this guess what is it this is the gradient descent stochastic gradient descent now let us see if you have really understood nag and momentum based gradient descent one of these curves here corresponds to stochastic nag the other one corresponds to stochastic momentum tell me which one is which s tudent blue pill b lue pill red pill blue is s tudent refer time twelveone h ow many of you say that ok i am confused ok how many of you say that blue is momentum how many if you say that red is momentum oh there is so many you do not have an opinion s tudent sir not clear n ot clear i will buy that so ok  so look at this who is taking longer u turns momentum or nag moment um roughly which guy is taking the larger u turns s tudent r ed guy r ed guy right i mean roughly speaking there is only one point to judge by this here because here they are almost same and that could happen in practice right because this is now nois y  so the red curve corresponds to s tudent momentum m omentum because it i s taking a larger u turn we saw that momentum takes larger u turns and the blue curve is corresponding to nag ok  so no i remember this was an error on the slide yeah  so this has to be red and this has to be blue  so ok  so the momentum is actually red and the nag is blue because it i s taking a shorter u turn and the reason you do not see it very clearly is because both of these are running in this stochastic mode b ut you st ill see the relative advantage of them that nag still takes shorter u turns both of them are faster still faster than vanilla gradient descent refer slide time thirteenthirtyfive y ou see that black curve at the top and both of these are faster than them both o f them all three have run for the same number of iterations after sixty steps you see what happens to stochastic gradient descent and what happens to nag and momentum basically gradient descent and of course you can have the mini batch versions of momentu m and nag also"}
{"audio_filepath": "Data/Preprocessed/SVD for learning word representations_76.wav", "duration": 836.0, "text": "so in this module we will talk about using svd for learning word representations so what does singular value decomposition do yeah these are all possible variants so people have tried various things and one of the ppmi one is the is the most reliable thing that is what is given but you can think of i mean you said one there are ten different things which we can do for the cooccurrence matrix right but this is the most popular and most stable thing to do yeah what is the single value decomposition do can you read it from the slide please it gives the rank k approximation of the matrix so let me start defining a few things so from now on when i refer to the cooccurrence matrix i would mean the xppmi matrix right which was the positive pmi which was replacing all negative pmis by zero and just do not have this nasty variable i will just call it as x so from now on whenever i say x i mean the positive pmi cooccurrence matrix ok so that is what this matrix is ok and we know that svd gives us this reconstruction of the original matrix and fine it gives us the best rank k approximation of the original matrix and it discovers the latent semantics in the corpus everyone remembers this like that is what we were by we were using pc and svd and auto encoders it was able to discover some latent semantics and we will concretize this intuition with the help of our current example but for now i just want you to recall that it helps in discovering the latent semantics now  notice that this product and i think i have done this in one of the assignments or something can be written as a sum of the following products so i can write it as sigma one u one v one transpose sigma two u two v two transpose and so on can you tell me what this sum is this is the rank two approximation of the original matrix and i keep taking more terms i get more and more rank approximations of the original matrix ok now  and we all know that ok we all hopefully know that what is the dimension of this it is a scalar vector matrix scalar vector matrix studentmatrix ok now of course you will say matrix but what is the dimension of the matrix why is it a matrix it is an outer product of two vectors right this what is the size of this n cross one into n cross one so that sorry one cross n that gives you n cross n matrix everyone gets this otherwise how is it a rank one approximation you have to get the original dimensions right everyone is clear with this is an outer product and it belongs to r m cross n ok and if we truncate the sum at the first term we get the rank one approximation and by svd theorem we know that this is the best rank one approximation now  what does this actually mean that this is an approximation what do we mean by that so we will see that on the next slide and similarly in the same way if we truncate it in the second term you get the same best rank two approximation now  what do we mean by approximation here actually and i mean to say approximation always in this course at least try to think in terms of compression how many elements are there in the original matrix m cross n that is how many elements you need to describe the matrix completely  if you do a rank one approximation how many elements are you using m plus n plus one right so the original matrix has m cross n entries entries and when you do a rank one approximation you have m plus n plus one entry  so that that is the approximation right so you are trying to really compress the original data using only these many variables you get that ok and if we do a rank two twice this right so as many rank i mean as deeper as you go in the sum you will have that many elements to do the approximation ok but what is important is that the svd theorem tells us that this is not just any random approximation but this is the best approximation that you could have done that means if you wanted to use only these many elements these are the best elements to use right everyone gets that so as an analogy consider this right suppose you are given eight bits to represent colors ok and this is how you represent very light green light green dark green and very dark green this is what your representation is in this original eightbit representation there is some similarity between the colors but it is still a bit latent but now if i were to ask you to use only four bits to represent these colors what would you do the lowest significant bits if you use the first four no then use only get very light that is not the essence of that color right you need the color to be there so if you compress what would happen is so that is what happens in when you go from two hundred and fiftysix bit colors to higher or lowerright the distinctions between the colors go of f so all of them would be compressed to green well that is the most important important information in terms of the color right because you need to be able to distinguish between green and red as suppose to very dark and very light that is the more important information that is there right so when you compress it the most important information in that entity should be retained and that is exactly what svd does when it does a compression it retains the most important information in the corresponding entries is that clear is the intuition clear fine so let us actually do this so this is my original cooccurrence matrix x and i just repeat when i say x i mean x ppmi and now i have done svd and i have done a low rank approximation of it i do not know what was the value of k i selected but some value of k it was definitely greater than one or two so now you see a low rank approximation of x what is the first obvious thing that you notice it is dense now it is the longest sparse now  can you tell me something about the colored entries what was happening in the original matrix x the word system and machine was never cooccurring because of which their value was zero same for human and user but remember there is some important information in this matrix which also tells you what are the words with user appears with and what are the words with human appears with and that actually gives you intuition that these two words are actually related right same for system and machine system and machine both would appear in the context of words like interface install run and so on so you know they are similar it just happens that these two words never appeared together  so this similarity between them was latent or hidden in the original cooccurrence matrix now once i have done the svd what has happened because i have forced it to compress the data it has retained the most important in information and under that information these two words have actually come closer to each other  right so you see that now you have a nonzero entry for the similarity between those two word pairs do you get the intuition and can you imagine that this would happen with svd and what is wrong in imagining you can but i guess right that is what is happening with this so you think about pca you think about svd you think about auto encoders all the intuitions that we had build there the same is being applied here right all if you get this ok fine yeah after svd you could have right that is not necessary that it should be positive in the original matrix you do not have negative entries now  here is a question right recall that earlier each row of the original matrix x served as the representation of a word ok this was my original x ppmi not the rank approximation now in that case what would x x transpose give me what would the ij th entry of x x transpose be so let us look at this toy example you have this x matrix you have xi and xj now i take x transpose ok now  this is xi this is xj just standing now what would be the ij th entry of x x transpose it will just be the dot product between these two right is that fine so this is just the dot product between them and we know that dot product is more or less the same as cosine similarity module over the normalization right you just need to normalize it by the norms of x and xi and xj in this case right so i will just assume that this is a substitute for the cosine similarity  ok so every entry at every ijth cell in x x transpose is the cosine similarity between the representations of the ith word and the gth word is that clear to everyone ok fine and in the original case which was the xppmi the cosine similarity between human and user was zerotwentyone now  once we do in svd what is a good choice for the representation of the word i after svd what is the dimension of x hat it is again n cross m because it is a sum of m cross n matrix so that the dimension of x hat is m cross n although it has been constructed using fewer information but the dimension is m cross n right that means what is the size of the representation of every word still high dimensional still the same n or vwhatever everyone gets that is there any confusion with that ok now you could say that ok i will just take the ith row of the reconstructed matrix and use that as the representation because i know that now this representation is better some of those zero entries have changed they have captured the latent semantics between the words so this is definitely better none is denying that that this compression has given us better representation because we are only keeping the most important information now  if i do x hat x hat transpose remember x hat is the reconstructed matrix then again by the same argument the ijth cell actually gives me the cosine similarity between the i th word and the j th word and you can see that now the cosine similarity between human and user has actually increased so this is just for me to convince you that we have learned more meaningful representations so now what do we choose as the representation i have still while computing this cosine similarity i have still used xi which is high dimensional which has the entire vocabulary as the number of columns as a representation right so there are two things coming out of here one is i really like this cosine similarity i see that it has improved that means the representations were computing something meaningful but on the flip side i am still not happy because the representations are still high dimensional so can you construct a wish list for me based on this i would want the same cosine similarity to be present as given by x hat x hat transpose right but i would like to represent it by fewer dimensions that is exactly what my wish list is ok so let us see how do we do that now for no reason i am going to construct a matrix w word equal to u sigma what is u sigma it is the part of the svd right the svd told us it was u sigma v transpose so i am just considering this matrix i am going to call it w for no particular reason ok now  let me take x hat x hat transpose i can write it as this is that fine now what is the next step what does this mean i want an answer right this is that aha moment should be there or otherwise there is no point what is how many rows are there in w the same as the number of words in our vocabulary what is the dimension of each row k so now w word has low dimensional representations for the words in the vocabulary  but while doing this what have we not sacrificed the cosine similarity  the cosine similarity obtained by this is actually the same as this do you get that how many if you see this is very very important that if you have not understood this everything is meaningless so you see how from svd we got a low rank or a low dimensional representation for the words right w word is just to be clear k and k is very very less than v right so now we have representations for words which are much smaller they are no longer v dimensional remember in practice this k would be of the order one hundred two hundred three hundred and remember your vocabulary was of the order fifty k one thousand k and so on right so the huge reduction that you have got and you have still been able to learn meaningful representations which give you better similarity between related words right so conventionally w word which is u sigma and belongs to m cross k so i am sorry for messing this up but i have used m n and v are interchangeably  so you would understand it from context that m is v and the other matrix which is v is known as the w context matrix right what is the size of w context n cross k or k cross n right that means it has the representations for all the context words and w word has a representation for all the target words right so we had these words on the rows and the context words on the column so w word has the representations for the rows and w context has the representation for the corpus ok so this what we have seen so far and this is where we learn today is what a nlp was six years back right before the advent of deep learning if you wanted to use word representations this is what you would do you would do con construct a cooccurrence matrix try these tricks of pmi ppmi positive negative zero and all those things those heuristics then do a simple svd retain the most important one hundred two hundred dimensions and treat that as word representations and use it for whatever you want to do now  what needs to be seen is what happened with deep learning and how have this way of computing word representations changed over the past few years right so that is what we are going to see in the next lecture right"}
{"audio_filepath": "Data/Preprocessed/Backpropagation: Pseudo code_30.wav", "duration": 314.0, "text": "so we move on to the nex t modu le and now we will write pse udo code to for back propagation so we have all the pieces of the puzzle we have the gradients with respect to the output layer that was the special layer because the output activation function is different they are the gradients with re spect to all the hidden layers that means i have the gradients with respect to the activations as well as the pre activation s so in the hs as well as the as and i also have the gradients with respect to the weights and the biases and this is all inde x agnostic right that means i am just using k as the index everywhere i have a generic formula which applies at any layer for the weights as well as the activations and the pre activations right ok now we can put all this together into a full learnin g algorithm so let us see what the pseudo code looks like so we have this t equal to zero well run this for some max iterations we initialize all the parameters to some quantity will randomly initialize them ok now for these max iterations can you tell me what is the first thing that i will do so there will be two functions here ok tell me what those two functions would be student forward forward propagation and then backward propagation right so you do a forward propagat ion and you compute all these activations pre activations output layer loss everything and then you do this backward propagation where you feed all these things which you have computed these are the quantities which you have computed you will pass this t o your backward propagation algorithm it would not look so nasty as this it will not take so many parameters you could write it smartly and then you will just do the parameter update so what will the back propagation give you actually all the gradien ts all the partial derivatives right and then once you have the partial derivatives you know how to compute the update law so now let us look at these two functions more carefully the forward propagation and the backward propagation refer slide time zero twonine so forward propagation is simple for all the hidden layers that means from layer one to layer l minus one what will i do give me the code a k is equal to good then ok and what it what is h of zero you are starting the loop from one right so you will n eed h of zero that is x and then you will have a special treatment for the output layer and your final output will be whatever output function you use ok this makes sense you can write this in python you will h ave to write this in python refer slide time twofortyfive now we have computed all the hs and the as what have we computed all the as all the hs and all and the y right now you want to do back propagation so back propagation the loop will be from i equal to one to n minus one good so the first th ing i will compute is the gradient with respect to the output layer see even here the output layer was outside the loop the same thing would happen here also in the back propagation also first you will compute the gradient with respect to the output la yer and this is the formula if you remember from last class right that is the formula which i have substitute here and note that f of x is known to you because you computed that in the forward pass and e of y one hot vector which with a correct label said to one and you know what the correct label is because we have given you the refer time threefourteen data right ok t hen what would the loop be l to one or l minus one let us see first you compute the gradients with respect to parameters it is l so because we ar e using k minus one then you compute the gradients with respect to the layer below computes gradients with respect to the pre activation right this is exactly how you will proceed this is clear to everyone the same three components that we have used y ou migh t be a bit confused about the ordering in which we have put them because we computed the gradients with respect to pre activation first and then the weights but once you go back you will realize because it is the way we have indexed it because this is al ready outside so this has already been computed so you can already compute the gradients with respect to the weights of the outermost layer is that fine so this is straightforward you can go back and check this ok now anything remaining or you have everything can you just take a minute and see if you can visualize the python code and we will just assume that you are done the assignment you can read you will have multiple these vectors and matrices and so on and you are just doing a lot of matrix op erations using refer time foursix or refer time foureight or whatever you prefer right now what is missing here input is missing ok input we have given right the ominous data set has been given is there something that yours i have still not shown you how to compute oh i did not update the parameters here is it no the parameter update will happen in the outer loop  right so those forward prop back prop and then update the parameters right so the main algo rithm was forward prop back prop update th e parameters when we saw forward prob an o bvious seeing backward prop  so what is missing one thousand iterations something in the last line before end of course do you know how to compute this"}
{"audio_filepath": "Data/Preprocessed/Contours Maps_34.wav", "duration": 625.0, "text": "so we look at something known as c ontours refer slide time zerosixteen so now visualizing things in three d can sometimes become a bit difficult especially for the person who is making the slides  so we can can we do a two d visualization of this traversal have i done this in the ml course no good can we do a two d visualization of this traversal al ong the error surface so for that we need to understand something known as contours how many of you have looked at contour diagrams before how many of you know how to read them a ll of you know how to read them refer slide time zerofortyseven so let us s ee now suppose this is what my error surface looks like and i have a single scalar variable  so this is just a function of w for example and this is what my error surface looks like now what i am going to do is i am going to take horizontal slices on this error surface fine now can you tell me how this is going to look from the top s orry let me you should start answering before understand the question this is this error surface is actually  so i was wrong in saying this is theta assume this is w comma b and you are just seeing the front view of the error surface what you are seeing here is just the front view this error surface is actually like a dementors hat  so right  so imagine that it i s a hat place like this and you are just seeing the f ront view of this otherwise a top view does not make sense right  so now i am going to slice this hat at two vertical positions and now you are looking at it from the top what are you going to see s tudent ellipsis e llipsis everyone agrees with tha t refer slide time onefortynine so we will see something like this do you see something peculiar about this is this a contour map i s this no ok and all of you raise your hands when i asked do you know contour  so do you see something peculiar about t his what is it how many if you get that  so what you are seeing here is this portion right where the slope was very steep the difference between the two circles or the two ellipses is small and you can visualize it if you try to look at it from the to p this distance is actually going to be small right and in the areas where the slope was gentle relatively gentle the distance is more and you can again visualize it right from if you look at from the top this is the distance that you are going to see a nd what do you say about these guys what does that indicate they are the same s tudent refer time twofortysix v alue across that entire region the value is same because you have taken a verticals you have taken a horizontal slice at a particular vertic al position right  so you have taken a horizontal slice at this position  that means the error is going to remain the same throughout that rim is this clear to everyone ok i t i s very important that you understand this so there are only two things tha t you need to understand if you want to read contour maps one is a small distance between the contours indicates that the steep slope exists along that direction and a large distance between the contours indicates that a gentle slope exists along the di rection  so everything today is going to be about steep and gentle slopes and the other thing that you know need to know is that whenever you see one circle the error is the same along that circle or ellipse whatever you boundary that you see the erro r is the same because you are taking these vertical slices so we are ready with this rule everyone understands this perfectly refer slide time threeforty so i will just give you a couple of exercises and you have to tell me whether you understand thi s or not refer slide time threefifty so i have plotted a three d surface or two d i have what is this s tudent refer time threefiftyfour n o s tudent there is a contour t here is a contour everything is not going to look like clean circles always right ok  so this is a contour every line that you see here represents one cut along the vertical axis right  that means the error is the same there now what you are seeing is a contour i want you to guess the three d surface from this you just guess it i mean just k eep it to yourself fine t h e color is the same right blue is good red is bad so blue means the darker the shade of blue the lesser the value of the error the darker the shade of red the higher the value of the error ok i want you to imagine the three d s urface if you can do that then i will be sure that you understand what how to read a contour how many if we can imagine this y ou can just say yes right i can never figure out whether you actually speak it so let me help you with the first one and then we will do a few more  so let us start with the extremes right so let me see how to do this  so this portion i also need to do it for the video ok  so let me just do it here  so this portion what do you think about the slope there very flattish wh y because this is the line that you see and the other line is not even in the figure right so it i s basically very flat the slope is very gentle is it a low region or a high region  h igh region fine ok now what is actually happening here what is the slope here s tudent h igh v ery high that is why these two regions are very close to each other  so from this high region what is happening suddenly there is a slope and you are going down and you know you are going down because you are reaching a blue region right ok now what is happening here s tudent very flat v ery flat and this also flat  but slightly upper than the lower guy  i s that fine now can you all imagine this ok and is this what you thought it is perfectly yes right is exactly what you thought ok just a minute  so the orientation here has been changed a bit right so this portion actually corresponds to this portion are the two this is clear this portion corresponds to this portion right the just orientated fine so you start o ff this high plateau region which is here then you start going down go down and then you see a fold here right that is this fold  so you went to a darker shade and then you came up to a slightly lighter shade the shades are ok g uess the three d surfaces h ow many if you want to play this forever now start with the extremes the bad guys the good guys the plateaus and the valleys and then see how do you go from the plateau to the valley ok tell me the corners first this plateau or valley s tudent plateau p lateau t his plateau  h igher than this or lower than this s tudent lower than this l ower than this this s tudent valley t his towards the valley it i s still between red and blue right it i s not like right down there and what happens to all these guys all are very steep slows all converging down into the valley  so can you perfectly imagine this and you will tell yes when i say when i show you the three d surface right again you need to reorient yourself  so this corner here is this corner this corner here is this corner  so we had these two plateaus at the top we had this slightly higher valley slightly lower valley and then all of them going into a very deep valley you see that everyone gets this how many if you have a problem with this if you have a problem with this you w ill just sleep off in the rest of the lecture  so i want you all to understand this very carefully i do not mind repeating it how many of you understand this you understand the regions with gentle slope s tudent yes t he regions where you have a steep slope and you end up into that valley which is the valley here can you point it out fine ok so we will move ahead refer slide time eightten so now we know what contour maps are and how to visualize them and so on right  so now we will try to see the gradient a descent algorithm instead of running it on the three d error surface we will try to run on this two d contour map refer slide time eighttwentythree so this is what i already showed you right i started from here a nd i showed you how it comes here or something like this right t hat was the gradient descent let me just erase this ok that is something like what the gradient descent algorithm now again you just need to reorient yourself  so let us see this corner is this corner this corner is this corner and so on right so you get the reorientation right it just shifted now i am going to start my gradient descent algorithm from here from this point ok everyone see is that ok i am going to start from there and you have to help me and i am not going to just keep clicking you have to tell me what is going to happen  so what will happen initially fast movement slow movement s tudent slow movement s low movement right  so i am running it one two three four five six seven eight it just keeps running very slowly now what will happen s tudent fast f ast ok  n ow you see actually you can see the arrows these arrows are the quantity the magnitude of the movement right  so earlier this movement was so small that you could not even se e the arrows i have been drawing arrows right from the beginning  but you could not see them at the beginning now you can see them right now what will happen s tudent slow s low right  s o you see the exact same movement that i did on the three d surface now you can visualize it on the two d surface right and you can easily tell me where it will go fast where it will go slow right and where it will just k eep moving very drag its feet and so on ok  so this is where it starts dragging its feet and the same thing happened when we were in this region right so just you just make the connection that we are in the corresponding three d region there ok fine  so we are moving very slow and it just keeps running so that is where we lend this module  so we just r evised gradient descent we saw that things are proportional to the gradient that is why gradient descent and the smaller the gradient the slower the movement the larger the gradient higher the movement gentle the slope s tudent smaller s maller the gr adient steeper the slope larger the gradient"}
{"audio_filepath": "Data/Preprocessed/Relation between input size, output size and filter size_87.wav", "duration": 702.0, "text": "ok s o now we will g o to the ne xt modu le where we will try t o lea rn the relationship between the input size the output size and the filter size ok so so far we have not said anything about the dimensions of the inputs i have just been very vague that its m   n and also for the filter i have just said three   three five   five and so on and in fact i am not even told you what the dimensions of the outputs are except that i be got some intuition that it seems that the output dimension is smaller than the input dimension right so let us look at these in more detail and see what do these different outputs look like why do i care about these things what do the inputs and the output sizes look like because these are yo ur matrices that you will be dealing with this tell you these tell you how many parameters you are going to have these tell you what is the size of the memory that you need to load this entire network into your memory and so on it so that is why this co mputation is very very important and i will have some more things to say about it towards the end of this lecture or some lecture ok so if you first define the following quantities so we have the input which has a width wone height hone and depth done so if you are looking at the original image then the depth is going to be three in most cases it is going to be rgb ok there is something known as the stride s i will come back to it later on but i am just defining it here or others just introducing it here and then you have number of filters so i said that every filter that you apply you are going to get one feature map which is two dimensional if you have k such filters you will get k feature maps each of them is twod rig ht so we will have something known as number of filters k and then you will have something known as the spatial extent of these filters so that is the number three  three five  five which i have been saying so  that is known as the spatial extent i am going i am going to refer to it as f ok and we are going to always assume square filters so it is always going to be f  f is that fine ok and the depth of the filter is one more thi ng which i need to worry about but i have already said that i am going to assume that the depth of the filter is the same as the depth of the input ok now the output is again a volume which is of size wtwo htwo and dtwo and my quest is to find out how do i compute these wtwos htwos and dtwos that is what i want to figure out it is not very difficult but i just want to do it properly so that is what the setup is right so we have defined the sizes of everything on the input and the filters now you want to l ook at how do we get the output sizes ok so let us compute this for one example so this is my original image so i am looking at a two dimensional input which i believe is seven  seven and i am applying a three  three filter to it so every time i slide the filter i will get one pixel in the output and i got the entire feature map now it is obvious that the size of the output is less than the size of the input why is it so because the re are certain pixels at which i cannot place the filter why you will go out of the boundary right so i cannot if this is my pixel of interest i cannot place my filter there because then the filter will go outside the image and i do not know what the average to come to how to compute the average right those values are undefined do you get that ok so in general for let me see for the three cross in fact this is true for all these pixels which have been shaded or any of these pixels i cannot place the filter because you will go outside the boundary so now for a three  three filter what is the reduction in the size of the output compared to the input the width decreases by two and the height decreases by s tudent two two right so can i be bold enough and say that the width and height decreases by not yet ok so let us see if we had a five  five kernels ok then which are the places at which i will not be able to place the filter these two shaded boxes and both these boxes i cannot place the filter because the filter will go outside the image so now can you tell me how many so in this case the size reduce is by how much the width reduces by student r efer time fourtwenty no the width reduces by four sorry and the height reduces by student four four so now can i say that the width and height actually reduce by f minus one where f is the size of the filter is that ok how many of you get this so you did not ge t this no you did not get this ok so in the three  three case you see that that is one row and one column on each side left and right which i cannot apply ok so let us focus on the columns so columns give me the width right so when i have a three  three filter there are two columns which get subtracted because these are the boundary columns when i have a five  five filter how many columns get subtracted two on the left hand side two on th e right side is a total of four if i have a seven cross seven filter three on the left hand side three on the right hand side so total of six so you see the relation it is always f one right so your new width and height is always going to be wone f one which is w one f one is that ok everyone gets this and same for the height the width and height are going to be symmetric because the filter we have chosen to be symmetric it is f  f ok now but what if we want the output to be the same as the input what do we do padding ok you can use you know something known as padding so that means now i will have a boundary of zeros so i am saying that this is my original image and outside it there is a black border or a white border i do not know whether zero stands for black or white it is embarrassing but student black black ok so it is a black border outside the image ok and now i am going to take an average that way now this was the pixel earlier on which i co uld not place the filter but now i can artificially place on it assuming that it there is a black boundary around it so now what would be the output size student refer time sixeight same as the input so now can you tell me so i have wone i have f and now i have something known as p also ok so i know that w output rather wtwo is the output is one now when i add the padding what would the formula be two p is that fine everyone gets that ok so now if i have a five cross five f ilter and if i want the output size to be the same as the input size what is the padding that i should use student p p is equal to two right that is clear from the example that we saw that there were two shaded columns and rows which were problematic so  i need to do a padding of two and then if i substitute in this formula you can just see right so five four one right so you will get back wone is that fine is that ok how many if you get this how the formula works with the padding how many of you do not ge t this please raise your hands you do not get this ok so remember in the three  three case there was one column on the left hand right which was a problem so when i say a padding of one i add one column to the left one column to t he right one column to the bottom and one column to the top and that is exactly the problematic region in the three  three case right so that means this was my original formula ok now the new width is going to be plus two times the pa dding which i am going to use because i have used one padding here and one padding here right now in the three  three case that is ok now in the five  five case how many columns are problematic two so that means  i have use a padding of two when i say a padding of two i add two on all the sides so now again if you substitute in this formula so you would have wone five one four so that will give you back wone right so that is how it is right now the question is you have if you have taken care of filter size and padding now the other thing that we need to look at is stride ok so the stride defines the interval at which the filter is applied now what do i mean by that so remember that stride is basically a step right the same definition as it is applies to walking rate so right now what we were doing is we placed the filter at as this pixel at the center then this pixel has the center and then this pixel has the center instead of t hat i could take a that means my stride is one i am taking one step at a time so if i do a stride of two what would happen student refer time eightfortyeight i will apply two alternate pixels right so this is how i will move so then what would happen to my output size if my stride is two student refer time eightfiftyseven what would happen student refer time eightfiftynine it will become half ok so what would the formula be then so so far my formula was now if i have a stride of two what would the formula be stud ent refer time ninetwelve they divide the whole thing by two student refer time ninesixteen by s right if it was s was three then this would have become one third roughly right if s was four this would have become one fourth so what should i divide by student s s so i should divide the whole thing by s student refer time ninethirtytwo s ok so it turns out that is not exactly that but you get the intuition and you can work out the formula so you do not divide this by s and you w ill figure it out it is easy to see because of some ceiling and flooring and things like that so you can go back and check that out and basically you could just think of this that this was your original weight in the absence of stride or rather than t he stride was one so now if you are going to take longer strides you have two account for that if you take a stride of two stride of two your width is going to become half you should take a stride of four your width is going to become one fourth is that fine do not worry about this additional plus point you can go home and figure it out finally coming to the depth of the output what would the depth of the output be so let me just see right now all our formulas were wtwo htwo in the presence of filter padding and size a stride sorry but we never had a formula fordtwo so that is what i am asking now what is the depth of the output same as the every filter is going to give you one two dimensional output if you have k filters stu dent k you will get k two dimensional outputs that means the depth of your output is k right so the depth is very simple it is just equal to k the number of filters that you have so i want you guys to note down these th ree formulas now let us do some exercises so this is my original input which is two hundred and twentyseven  two hundred and twentyseven  three i have decided to apply eleven cross eleven filters and i am not going to tell you th e depth of the filter because it is going to be the same as the input ok and i have ninetysix such filters i have decided to use a stride of four there is no padding can you tell me what is the output volume going to look like what are the dimensions of the outp ut volume ok so dtwo is simple what is dtwo ninetysix what is wtwo student fiftyfive fiftyfive htwo ok you guys have the last class fine so similarly you can do it for so actually the computation which i had that this was just not some random computation this is actually the first convolution layer from alexnet right so one of the popular architectures that we are going to cover later on right so this is what aalexnet does at its input it takes the rgb input and gives you a volume of this slice this side and then ther e is something else with this volume right so we will see that later on there is one more exercise you can do it later on i do not want to do it now ok"}
{"audio_filepath": "Data/Preprocessed/Long Short Term Memory(LSTM) and Gated Recurrent Units(GRUs)_109.wav", "duration": 1835.0, "text": "so with that motivation let u s go to the next module where we will talk about long short term memory and gated recurrent units ok so now all this was fine in terms of ok i gave you a derivation on the board and say that this is not required but can i g ive you a concrete example where rnns also need to selectively read write and forget right only then you will be convinced that this kind of morphing is bad in the case of rnns so i will start with that example and then once we agree that we need sele ctive read write and forget how do we convert this into some mathematical equations right because conceptually it is fine but you have to write some equations so that the rnn can do some computations where you have selective read write and forget ri ght so that is what we are going to do over the rest of the lecture so first let me start with the concrete example where you want to predict the sentiment of a review using an rnn so this is the rnn structure we have do ne this in the past that you have a sentence one word at a time is your every time step you will feed this to the rnn and at the last time step you will make a prediction and as i said the rnn needs a document from left to right and by the time it reache s the end the information obtained from the first few words is completely lost right because it is a long document and you are continuously writing to the same cell state so you will lose the information that you had gained at the previous time step but ideally we want to do the following we want to forget the information added by stop words like a an the these do not contribute to the sentiment of the i can ignore these words and still figure out the sentiment of the document i want to selectiv ely read the information added by previous sentiment bearing words so when i have reach the last time step i should be able to read everything else which had some sentiments before it and focus on those words just i want to selectively read from these sentiment bearing words and also i want to selectively write the new information so i have read the word performance now i want to selectively write it to the memory whether i should write it completely or should i only write parts of it or not that is what i need to decide so that is fair this is a typical example where rnn also when it is dealing with long documents it needs to understand what is the important information in the document that needs to be retained and then selectively read write and forget ok so i am spending a lot of time on this analogy because you need to really understand that this is important and this is where rnn suffer right if you are using them for very very long documents if we have document of the size one thousand words which is not comm which is not uncommon right because wikipedia pages have much more than that per document so it is going to be very hard to encode the entire document using an rnn not that it is going to become significantly easier with lstm or grus but to certain extent it will become easier ok now the next part is how do we convert this intuition into some mathematical equations right so let us look at that so in this diagram recall that the blue colored vector is called the state of the rnn it has a finite size so now i will just call it as s t belongs to some r n and the state is analogous to the whiteboard and sooner or later it will get overloaded with information and we need to take care of this right so now our wish list is selectively read write and forget ok so let us start with that so what we want to do is that and this is the problem definition now that we have computed the state of the rnn t his is a blue colored vector although it is not blue but this the blue colored vector from the previous diagram where the state of the rnn was computed i know what the state is at time step s t minus one now i want from here to here go from here to here  that means from s t minus one i want to compute the new state of the rnn right so i had something written on the whiteboard i want to write something new i want to change the state of the whiteboard and this is the new information that has coming to me right the x t is the new information at time step t and while doing this i want to make sure that i use selectively write read and forget so these three operations have to come in somewhere in the between so that i am true or faithful to the analogy which i have been giving right that is the this is the our problem definition now going from s t minus one to s t and introducing these three operations along the way that is what we are interested in doing i will go one by one we will implement each of these three items right so we will start with selective write so recall that in rnns this is what happens at every time step t you take the previous time step previous cell state you take the current input do you recognize the op eration here how many of you recognize the operation here raise your hands ok so this is nothing but the following operation and as usual i have ignored the bias ok is that fine so that is what i am representing it as but now so one way of looki ng at it is that when i am computing s t  i am readin g or i am taking the whole of s t one  so once i have computed s t one  i am writing this to my whiteboard and then whole of it would be used to compute the next time step ok but instead of doing this what i want is i want to read only selective portions of s t minus one or rather i want to write only selective portions of s t one  once i have computed s t minus one i do not want to write the whole of it because then the whole of it will be used to compute the ne xt cell state i do not want that i just want to selectively write some portions of it ok now in the strictest case since i know that s t minus one belongs to r n it is an n dimensional vector in the strictest case what i could have done is i could have used a binary decision that of all these n entries i am going to read some entries and ignore the others so all the other entries i am going to set to zero fine that is the strictest thing that you could have done now for any of these strictest things what is the soft solution so for binary what is the soft solution binary zero to one so what is the soft solution for that between student refer time sixfive zero to one so read some fraction of each of these dimensions right so let us try to understand what i am trying to do here ok so and the third bullet some of these entries should have gone to zero right ok so instead of doing this what we want to do is we have this vector which has n entries this is the cell state at t minus one i do not want to write the entire vector onto the final cell state what i want to do is i will take some fractions of it is say zerotwo of this zerothree of this zerofour of these and then write only that do you see the operation that i am trying to do right i want to take some fract ions and write only those to the cell and as i said this is the softer version of the hard decision which would have been zero for this one for this again zero for this and so on right how to do this why to do this all that is not clear i am just telling you the intuition how and why will become clear later is that fine ok so we want to be able to take s t one and write only selective portions of it or pass only selective portions of it to s t so when ever we compute s t  we do not want to write the whole of s t minus one just want to use selective portions of that so what we do is we introduce something known as a gate and so this gate is o t one ok we take the original cell state s t one do an element wis e product with a gate which is known as the output gate and then write that product to a new vector which is s t one ok so initially this will look confusing but it will become clear by the end of this lecture ok so is that fine this is what i am tryi ng to do again how to do this is not clear but this still matches the intuition which i have been trying to build that i want to write only selective portion of the data which i already have is that fine ok so each element of o t one gets multiplied by the corresponding element of s t one and it decides what fraction is going to be copied and this o t minus one is going to be between zero to one but how do i compute o t minus one how does the rnn know what fraction of the cell state to get to the next state ho w will it do it we need to learn something whenever you want to learn something what do we introduce everyone student refer time eighttwentysix parameter sorry what did you guys say back propagation back propagation will do what it will work in the air or propagate to what student refer time eightthirtyseven whenever you want to do some kind of a learning i want to learn some function what do i introduce student refer time eightfortytwo parameter parameter right so that is what we are going to do we are now going to introduce a parametric form for o t one right and remember this throughout in machine learning whenever you want to learn something always introduce a parametric form of that quantity and then learn the parameters of that function do you get thi s how many of you get the statement ok this is what we have been saying day from right from class two or class three right always introduce a parametric function for your input and output and learn the parameters of this function s o that is exactly what i am going to do i am going to say that o t one  is actually this function i am just giving some time to digest this so this is at time step t one so it depends on the input at time step t minus one it also depends on the output at output means whatever comes out of this right so the same operation what have happened at time step t two so whatever was the output at that state it will also depend on this you just take a while to digest this equation you will see at least six mo re equations of this form in this lecture so if you are comfortable with one all of them would be clear so try to connect the whole story i have s t one  i do not want to pass it as on pass it on as it is to s t  so i am computing some intermediate valu e where i will only selectively write some portions of s t one and selectively write in the strictest case it should be binary but that is not what we are interested in we introduce fractions if the fraction has to learn binary let it learn but we will make it fractional that means we will make it between zero to one hence the sigmoid function right remember in one of these lectures we had said that sigmoids are still used because in rnns and lstms remember in we had said that sigmoids are bad use sta nage refer time tenthirtytwo or use relu but we had ended with sigmoids are still used in the case of recurrent neural networks and lstms so this is where they are used ok how many should get that connection ok good fine so we use sigmoids because we w ant the fraction to be between zero to one and we also want some parametrization right and this is the particular form that we have chosen there are various equations possible various things you could have done here in fact there are ten to fifteen different var iants of lstms i am covering the most popular one which uses the following equation right so it is says that this is how you will compute the output gate and that gate will regulate how much of the cell state should be passed on from t minus one to the next state ok everyone clear with this ok so now if you are clear with this give me an equation for h t minus one student refer time eleventwentytwo loudly everyone s t minus one is that fine right so this is the equation that we will have so we have don e selective writing and these parameters are no special they will be learned along with the other parameters of the network ok so let us spare some thought on that you got a certain loss at the output ok earlier you just had these parameters w u v which were the parameters of rnn which you are adjusting to learn this loss now in addition you also have the flexibility to adjust these parameters so that if the lost could improve by selectively writing something then these parameters should be u pdated accordingly right may be you are being over aggressive and making o t minus one to be all ones that means you are passing everything to the next state right now it has the chance because they have introduce parameters if it helps the overall los s it better make these fractions more appropriate so that only selective information is passed to the next state how many you get this intuition so that is why anytime you introduce parameters you have more flexibility in learning whatever you int end to learn there is remember one clear difference here right and that is where i said that while i was giving the analogy i was really setting up things but here there is one distinction what is the distinction that is there ideally what would i have wanted suppose i take the example of the review ok and the review was say the movie was long but really amazing ok now which is the word here which is actually trying to mislead so overall sentiment is positive right everyone agrees with that but which is the word which is misleading student long long right that means i need to do what with that word student refer time thirteeneleven forget that would right now ideally i would have wanted someone telling me retain retain retain forget retain retain retain i would have a label for each of these words and then i could have a loss function which tells me whether my gates were actually athering to this decisions or not so remember my gates are learning some distribution o t one  which t ells me what fraction to retain and at this particular time step i would have wanted o t minus one to be all zeros ok i would have wanted to forget but this kind of not just o t minus one this will become more clear when i do all the other gates also so what i am trying to say is that you should have had some supervision which tells you which information to retain and which information to forget but you do not have this supervision right no one is telling whether these are the important words these ar e not the important words so that is the difference between the whiteboard analogy there you knew exactly which step is important and which step is not important here you do not know that all you know is that you have a final loss function which dep ends on plus or minus whether the this prediction is close to positive or close to negative and what is the loss and that loss is what is being back propagated but the difference now is that you have introduced a model which can learn to forget some thi ngs right earlier you did not have a model which could learn to write or read or forget selectively now you have introduced a model this is a better modeling choice right so the same as we have had arguments that you could do y is equal to w transpos e x or you could do y is equal to deep neural network of x right you are making different modeling choices here and with the hope that one modeling choice is better than the other choice so just as rnn was one modeling choice now you are using a differe nt modeling choice where again with the help of these gates and all you can definitely write a function of y is a function of the input and that function is going to be lstm function which we will see in detail so this one part of that function and whil e doing this you are just making a better modeling choice which allows you to learn more parameters and along the way if important do selective write read and forget is that clear right so you would see the difference what would have been the ideal c ase and what is it that you have the ideal case would have been explicit supervision for what to forget read and write you will never have that but you are still making a modeling choice which allow you to do that so if it required to model while ba ck propagation should be able to learn these parameters so get you are able to do that i know i am repeating myself but it is very important that you understand this situation how many of you get this now and as i said the se parameters will be learned along with other parameters and o t is called the output gate because it decides what to output to the next cell state ok still you see that there is a lot of gap here we have not reached s t yet we are still at s t one  we ha ve computed some intermediate value but we have not reached s t yet and along the way we had three things selective write read and forget we have only taking care of selective write so far ok now let us look at selective read so what this selective read do you are going to get new information at time step t which is x t right and now instead of this original cell state you have used the selectively written cell state because that is what you have written now so that is wh at you should use now using a combination of these two i am going to compute some intermediate value ok and just stare at this equation this equation form is very similar to the rnn equation form right only thing is that instead of s t minus one i am using h t minus one and for good reason because i know that h t minus one contains only selectively written values from h t one  is that fine and x t is the new input still there is some gap here i have not reached s t yet i am still at an intermediate value so this is the new input which i have received now what should i do with this new input selectively read this input i do not want to take all of this input because may be the input which i have got now is a stop word and i do not want to read all of i t right do you get that so now it captures all the information from the previous state as well as the current input and we want to selectively read this so now what would you do to selectively read again the same situation that you have a s the answer is already here you have s tilde and you do not w ant to pass it on as it is to s t  this is s  s t is somewhere here which you do not know how to get to but you know that yo u do not want to pass on all the input that you have read you want to selectively pass it on so what will you do now again introduce a student gate gate and this gate will be called student read gate input gate or the read gate right ok refer slide time seventeenfiftyfive so now what can you give me an equation for the gate i t is equal to sigma of that is good because sigmoid is what we need it is going to be a fractional thing let me add the easy part w into student h t one h t one  that is telling y ou what ha s happened so far and u times x t  you see the same equation same form the parameters have chang ed so these we will call as w i u i and v i and they are depending on the input as well as the previous state previous temporary stay that we had comp uted ok so that is exactly what your input gate is going to be and now this operation is the selectively reading operation how many you are fine at this point ok and then this product is going to use to be it is will help us to read selectively from t his temporary value that we have constructed or the input that we have taken ok so far what do we have we have the following we have the previous state which was s t minus one then we have an output gate which was o t min us one using these two we have done selective write right we have taken the previous state and the gate and then a selective write is that fine ok we need to check if the sigmoid should come here because sigmoid is already there in the computation of s t minus one right oh it is not there so this already has one sigmoid right yeah so then again a sigmoid on that is it there ok we will figure it out just check the equation right so there may or may not be the sigmoid the sigmoid already applied to s t minus one but we can figure that out ok so this is the selective write portion then you compute the current temporary state ok and just look at the similarity between these equations then you have an input gate and using these two we have done a sel ective read ok so you have taken care of selective write and selective read but you are still not reached s t i still do not have an arrow here i still need to f igure out how to compute the s t finally ok so what is the operation which is remaining now selective student forget forget ok so what do you think should we forget we want to find new s t  so let us see what we will forget right so the question now is that you had this s t minus one and now you have a temp orary state s t which is here how do we combine these two to get the new cell state ok so the simplest way would be that you say that s t is e qual to whatever was there in s t one plus selectively reading from the current input is that fine this is the one way of doing it ok but now what am i doing here what is the problem he re i am reading i am taking s t one as it is right so what should i do i should forget some parts of s t one  so what should i do for that introduce a what gate student forget gate forget gate right so we may not want to use s t one as it is but we are want to forget so there is at this point all of you should get some confusion if you do not then i would be worried if you are getting some confusion good right you should all get confused at this point why are you confused because you already did selective write and now again you are doing a selective forget also right but there is a difference because the selective write was then used to compute  how to read the information right but now once you have read the new information you want to see how to assimilate it back with the old information that you had right so that is why you introduce a separate gate so think of it as this way that you are keeping these functions separate input output and forget so they can separately learn things ok so whatever you want to selective write let it be a separate function these h t minus one is not going back to s t right let us just by use so that yo u can compute these temporary states so that is what is being passed to the next temporary state let i t only decide how much of this input should be read ok and then when you want to combine these two just use a separate gate and this exact idea which is confusing all of you why have a separate write gate and a separate forget gate led to something known as gated recurrent units where they merged these to gates we will get back to that ok so at this point it is fine i am just telling you the ori ginal equations for lstm and this was the motivation that they had so as i said there are at least fifteen to twenty different variants of lstm which use different equations they tie some of these weights so one thing could be that forget is the same as one min us remember right or output could be same as one minus input right you could have tied these gates instead of learning separate parameters for that so in the most parameterized form you have a separate parameter for all of t hese ok so we introduce the forget gate again can you tell me the form for this forget gate f t is equal to first term stud ent w f w f  second term u f  what wil l be there in the second term x t and the first term ok so this is what it will look like ok  so if you remember one of these equations you will be able to write all of these not that i am going to ask you to write them in quiz or something but why take a chance so and then once you have completed the forget gate instead of this equation can you tell me what is the equation and you are going to use what is the first term going to be it is s t one here what is it going to be now student f t into f t into student s t one s t one that fine ok so now we have the fu ll set of equations for lstm we have certain gates and certain states what are the gates output gate student input gate input gate student forget gate forget gate why do you guys has this momentary amnesia like suddenly you forget everything ok so output gate input gate and forget gate all of these are the same form with different parameters ok what about the states which are the states th at we have completed one was s t the other was h t and the third one was s t ok s t from s t  we get s t and from s t  we compute h t ok so in the diagram that you see here at the top tell me which are the computations which are happening at one time step at time step t which are the computations which are happening is it i will give you the options right is it this or is it this let us call this one let us call this two or this three or this four which are the computations happening at one time step and you see the order also here this should be straight forward right why student refer time twentyfivesixteen how many of you say four that is the one right because you start with selective reading right and you can just go by this right these are all indexed by t right is that fine ok so these are the computations which happen at time step t and these are exactly the computations which were written right so we have the three gates which you need to compute at every time step and you have the three states which you need to co mpute at every time step is that fine and this s t minus one is not being computed it is just taken from the previous time step is that fine so you have these six computations which happened at every time step and the output final output of an lstm so when you use tensor flow or something the output of an lstm would give y ou two things it will give you h t  s t because these both the states that are being computed one is the running state and another one is the current state which is being computed ok and i choose the notation s because that is what we have been using for rnns but in lstm in all the literature instead of s you will find it to be ct because it is called th e cell state so that is why s t ok so all these equations wherever you see a n s when you are reading some standard blogs or things like that you will see c instead of s so you just do this mapping in your head ok so lstm actually has many variants with include different number of gates and also diff erent arrangement of the gates so as i was saying that you could say that input is one minus output or input is one minus forget or things like that and also why this particular parametric for m right why not make w zero into s t one instead of h t one and so on so the all points of things that you could do or all of these are valid these are all valid variants of lstms so there is this paper called lstm a search space odyssey so you can go and look at i think we link it in the in the reading material right ah so you can see that there are actually many variants of lstms but this is the most standard and default variant which you will find in most platforms on tensor flow or pytorch refer time twentyseventwentysix form and there is another v ery popular variant of lstms which is called gated recurrent units so we will just see gated recurrent unit so i will just give you the full set of equations for grus so you have gates but unlike lstms you have only two gates you have an output gat e and you have an input gate you do not have the forget gate ok so what am i going to do for the forget gate so this is what i am going to do you see the last equation so instead of forget gate i am just saying that this is what you are going to se lectively input from the current temporary state so the rest of you rest of it you take from the previous state right so i have just tied the input gate and the forget gate any other changes do you see in this so earlier we had s t minus one everywher e right now we have s t minus one itself is that fine ok so the basic idea these equations are many and you could think of your own equations you could say that i will not really use this input information at all or i will choose to use it differently or what not right there are several things that you could do at a very abstract level this is what you need to what is this student refer time twentyeightthirtyeight so these parameters could then make a difference right they could adjust it accordingly and so on r ight so that is what i was coming so the there are various ways of realizing this right at the abstract level you need to understand that the original problem was trying to store all the information from time step one to time step t capital t right whi ch is not feasible because of this finite size that you have so along the way we built this intuition that it should be good to have these operations which allow you to selectively read write and forget right how do you mathematically realize these op erations there are various choices for doing that and we saw a few choices for doing that right there are many others you could have done but this is largely what whenever you say that i have used an lstm most likely you are using the set of equations which i saw which we saw on the previous slide and whenever you are using a gru these are the set of equations that you will be using ok and again remember this that there is no explicit supervision here it is just that we have a better modeling choi ce we are just introduce more parameters so that if required these parameters could be adjusted to do a selectively read write and forget right so it is often it is often valuable if you are doing some task with rnns or lstms you should visualize t hese gates right you should see that at time step t if you thought that it should have forgotten everything that it has learnt so far because suppose you had this the movie was long but i really loved it because the direction was superb and so on no w this word but actually changes everything right because it whatever was written before it does not matter anymore right so is it really learning those kind of gates where everything before but was forgotten right so it would be helpful to visuali ze these output gates and see what kind of values they are learning what kind of things they are remembering forgetting and selectively reading and so on right so as i said i will just again summarize the key thing here is the intuition and then the r ealization in the form of equations there are multiple choices we have seen a few of those right that is what i will end with and in particular in grus there is no explicit forget gate and i nstead of h t one you use s t one everywhere"}
{"audio_filepath": "Data/Preprocessed/Gradient Descent with Adaptive Learning Rate_40.wav", "duration": 2403.0, "text": "i n this module we look at g radient d escent with a daptive l ea rning r ate  so first we will see motivation or intuition for why we need this and once you get the motivation  i believe the rest should be straightforward refer slide time zerothirtyone so far what we have been doing is please pay attention on this slide i need to define some notations and you should no t get confused with that so far we have been dealing with the situation where we had just one feature which was x and one weight corresponding to it which was w and one bias which corresponded always on i nput righ t n ow we are going to look at the situation where we have more than one inputs th at means earlier we were basing our predictions only based on the director and now we are the director actor genre imd b ratings and so on so here x one x two x three x four these are four different features or four different inputs that i have and this is not x square  j ust i know it i s obvious  but i am just making it clear right so this is x one x two x three x four ok  it is n ot probably the best choice of notation  but i will just stick to that  so now each of these has a corresponding w one w two w three w four ok and this is how you r decision looks like  it i s the dot product between the weight vector and the input vector ok  t his is how i am going to decide and that is a sing le sigmoid neuron again now given a single point xy do i need to again go through this computation  s orry w p oh sorry ok  i will just erase this so this w is actually the vector w  so it includes w one w two w three w four and i am trying to take the derivat ive with one element of that vector  d o i need to show you how to compute this  h ave you seen this before c an you tell me i will show you the derivative with respect to w one can you tell me it will be a product of some terms c an you tell me what is the l ast term going to be refer slide time twofortyone e veryone gets this  yo u remember this f o r m  so only thing which is changing is this guy right so this part is exactly what we have derived and when we had one input we just call it x and now we have m ultiple inputs  so it will depend on that particular input right which ever w one corresponds to n ow make an interesting observation there  so sorry before that yeah this is obvious if there are n points we will just take the sum of the gradients with respect to the n points ok  n ow what happens if the feature x two is spars e w hat do i mean by that  i t is mostly zero ok  w hat does that mean so i am looking at lot of movie data ok  a mir k han acts in a very few movies  so if i have a feature which says actor a mir k han then that i s going to be zero for most of the movies in my data scene right th at is what i mean by sparse so if i have tenzero movies then probably only fifty of them would have this feature as on e ok  d oes that make sense so it i s going to be very sparse  n ow if the feature is sparse why do we care about it w hat will happen w hat do we really care about when we are talking about optimization in this course  t he gradients right th at decides how well we move in the plane that we are c onsidering w b plane or the other in the end dimensional region that we care about so now if x two is sparse what would happen to this  i t will be zero lot of times because x two is zero lot of times right so now just take a minute to understand this right s o now remember let us talk about stochastic gradient descent or mini batch gradient descent or even batch descent  y ou are going over all the tenzero points that you have you are computing the gradient with respect to all the parameters o ne of those par ameters happens to be w two right yo u have gone over tenzero points but in how many of those you will actually get the gradient for this  o nly in the fifteen which x two was present right e verywhere else the gradient would be zero so that means your sum of the g radients overall the endpoints is going to be small or big s tudent s mall s mall for this particular feature or for this particular weight i t i s going to be small right because you do no t have enough samples where you are seeing this  so now what woul d happen to the update  y ou started with a random value for w two a fter one epoch or making one entire pass of the data what would happen to the updates for w two v ery small very few updates compare this to a feature which is dense  d o you get a lot of upd ates so you see there is something unfair happening here  if a feature is sparse it i s not getting updated enough n ow that was ok  i n one situation if this feature was not really important  b ut now consider the exact example which i gave you which is this  a n a mir k han movie or not but suppose i am doing a classification whether this movie is going to be hit or not  i would believe this feature is very important because almost always when he is the actor the movie is a hit right so you really can not ignore this feature  y ou want to learn the parameters correctly for this feature  d o you get the setup right  t here could be cases where your feature is very sparse  but at the day at the same time very predictive of the output that you are trying to l earn right and in this case the output is whether the movie would be a hit or not t he other example could be is c hristopher nolan the director  so yes probably directed less than ten movies  but all of them have been at some point in the imd b top two hundred and fifty o r something right so that i s a very important feature  but you will not get it very frequently in your data right so you can no t really ignore these features  th at means you still want to learn these features properly  so you have sparse features y ou have dense features w e understand that for the sparse features the updates would be slower and for the dense features the update would be faster  t he sparse would be zero in most case s n o no  so you will do this zero mean thing n o  but if it i s a same v alue and you are going to zero mean the data right so the value even if it i s one it is going to be very close to zero right so you always assume zero means otherwise all this does not make sense right because if your features are not in the same range then anyways you are in trouble right fine so this is what i was trying to say that the gradient with respect to w t is going to be zero for most inputs and hence w t will not get enough updates and as i said if this is an important feature we can n ot really i gnore it  w e have to make sure that it learns better so what is the case that i am making for w hat do we actually need c an you relate it to the discussion on learning rate that we have been having so if the feature is sparse you know it i s going to get very fewer updates  so can we change its learning rate  s o that feature gets updates a bit faster as compared to the other features  so you get the motivation right  h ow to do this is a separate story  but at least we need to do this refer slide time sevenfortynine so the intuition is decay the learning rate for parameters in proportion to their update history  so you have been recording the update history you have been looking at the parameter you know all the gradient w two that you had calculated so far right h ow many times you had computed the gradients and what those values were actually now for th ese sparse features  th ose are going to be zero so your cumulated history is going to be small right f or a dense feature it i s going to be high  so why not make the learning rate inversely proportional to this history  t hat means if the feature has been updated fewer times give it a larger learning rate  i f it is not updated if it i s updated many times give it to a smaller learning rate  c an you give me a mathematical formula for doing this  t his is the intuition  j ust think about it for a minute l earning rate inversely proportional to update history ok good  h ow many of you get that  but most of you will get it once i show you the answer t his is my gradient which i had computed so far  i mean at this time step i will keep accumulating it in a history vector  so at time step zero i will take the magnitude of this again  i am taking the magnitude right because it does no t matter whether you made a n update in the positive direction or the negative direction you just matters that whether how much by how much it move  so i will just square this quantity  so that i can get rid of the sign  so i am taking the magnitudes and i am storing all that  so at time step t what would vt contain  it is grad w zero square plus w one square grad w one square and so on up till time step t now this was my if i ignore this quantity this was my normal gradient descent update rule  n ow do you see what i have don e i ha ve divided the learning rate by whatever history i had a c cumulated  so for the dense features what would happen is the learning rate will increase or decrease with time the learning rate will decrease right and for the sparse features  r elatively less in fact if you have written gotten zero updates so f ar  so when you have to updat e the first few times you will have a very high learning rate d oes that make sense right because this quantity would be zero  so our eta would actually be very large  so you see how that intuition got converted into some reasonable formula refer slide time tenthirteen n ow can you tell me a way of actually realising this  i want to show you that what happens when you have sparse data and i want to do this with the toy example that we had where we had only one feature and other feature was always on right so how do i create this sparse data so you should think about these because these are things you will have to do when you are practising machine learning and if you are w orking with the problem and you want to create some simulated data  so that you can verify some hypothesis that you have  so how would you do this s ee i am going to create thousand data points right which is x y points and of course i have this x zero wh ich is always on right so x zero is always on  i can no t make that sparse w hat about the other feature  i f i am creating thousand data points what should i ensure is that eighty percent of them or some ninety percent of them is always zero right j ust as the a mir k han case and most of the data it i s going to be zero  so what we will do is as i said we just have two parameters w and b  b cannot make sparse is always going to be on  so what we will do is we will make x sparse we just create random x y pairs and then for eighty percent of those we will set x to zero right so now this x feature is going to be very sparse so now i have created some data which is sparse  o ne of the features is sparse and now i want to see what happens when i run gradient descent momentu m and n esterov accelerated gradient descent and how does the algorithm behave and now  if i apply this algorithm which i did not name it i s called a dagrad ok  t his algorithm is called a dagrad if i apply this algorithm then what how does the situation ch ange refer slide time elevenfiftyfive so this is what gradient descent momentum and nag do  n ow at least the d ifference between momentum and nag should be clear nag blue curve is inside the red curve right so oscillations are slightly smaller  t his is how they behave now there is something very interesting that these algorithms are doing for this particular data set that i have created  c an you spot it  w hat is the interesting thing happening here i want you to take some time and think about and relate i t to the discussion that we just had how many of you see what is happening here  v ery few i will give a hint ok it i s almost as if these algorithms went to a school where they did not teach p ythagoras theorem  n ow related to the discussion that we just had what is happening initially so initially what is happening is you started from here ok and this is the w b planes  so you have w on the horizontal axis and b on the vertical axis w hat is happening to all your updates initially where are you movin g y ou are moving along the b direction  a re you making any movements along the w direction n o w hy w was sparse its gradients are mostly zero i t was not being able to make any updates in the w direction or it was able to do make updates in the b direction  i t did as much as it could do after reaching here it realizes that there is no point in going to b e further right i t actually took u t urn because it realise that there is nothing i can no t really go ahead  i have to now start working in a direction of w so now in practice although in this toy example it does not it still converges fast  but in practice what will happen is you have just moved in one direction reached a point and now from there again you are going to take right turn and reach to your destination right so you are taking you are doing something which is not fast  t his is not how you would go from this point to this point  t here has to be a better wa y r ight and this is happening because w is not getting updated frequently al l the up dates are initially done for b n ow when it is no longer possible to change b because you reached the optimum value for b then only you start changing w and that to very slowly because it will have to wait for many updates to happen  f or that to happen h ow many of you get this so this is exactly what is written on the slides because in our data the feature corresponding to w is sparse and hence w undergoes very few up dates and b is very dense and it undergoes a lot of a updates now such sparsity is v ery common in large neural networks which have thousands of features right so you can imagine this  n ow if i have thousands of features now suppose i am doing credit card fraud detection ok now say one of my features is corresponding to some educatio n that the person had and suppose he has done some very less sort after degree or less sort after curriculum so that feature is going to be sparse where most of the cases  but i can no t ignore it  m ay be this is the most predictive feature that i might ha ve right so you could think of various cases where you have thousands of features out of which many are going to be off for a given example right e veryone sees that this is the real world scenario where lot of your features are going to be sparse and in many cases you cannot ignore the sparse features ok fine now let see what a dagrad does a ny guesses refer slide time fifteeneighteen so i am running this we should start seeing something a green curve starting from here d o you see what is happening e xpected n ow try to guess if you are going to run into a proble m i have deliberately halted the algorithm  i just want you to think if you are going to run into a problem ok  a ll of you think you have something which makes sense  so now i have run it f or in this case again this is the toy example  h ence you do no t see a lot of difference between these algorithms in terms of number of steps taken to converge  but in real world application it would be very different  but now what has happened is i have run the algorithm for as much i can and i am then stuck here  i am not being able to move forward w hy is this happening w ell  i am the histories accumulating it i s growing  no w what am i doing to the learning rate  i am just killing it right it i s eta by a very large constant  n ow that i s going to be very small  so no matter how big my gradient is it i s going to get multipl ied by a very small learning rate and i can not just move any forward anymore right so see that will happen  that is why in th is case this is some point here which i do no t want to go over now and it i s this in fact i do no t have an explanation for that  but this one observation which people have made that remember we have the square root in the denominator  i f you remove the s quare root in principle you are still doing the same thing right y ou are still making it inversely proportional to a cumulated history  but it does no t work well when you do that  t hat i do no t know why it happens and i just read th ese comments at sever al places that it does not work when you remove the square root from the denominator  but that is not important for this discussion  t hat is just point for reference later on so right now what i am trying to say is that it did the right thing  it started making updates for w also and started making larger updates hence we se e this simultaneous moment in both w and b direction but the flip side is over a period of ti me the effective learning rate for b will decreas e so m uch that we no longer be able to move in the vertical direction right and if i am not being able to move in the vertical direction we will not reach the minima  i n this particular example not always  but in this particular example you need to move further in the direction of b  but a le arning rate is not allowing you to do that so that is what is happening so now can you avoid this y es ho w m ultiply by  so first divide it  so that the decreases then multiply it  so that does not decrease  a ll of these are interesting ideas  i am not i mean it i s very hard to say upfront whether this is wrong or right  but yea h these are you get the idea basically something is happening which is you are aggressively killing the learning rate refer slide time seventeenfiftynine now i just want to make sur e that you are not so aggressive so what happens because of the aggressive killing is the frequent parameters they start receiving fewer updates now this is what rmsp rop does  i want you to stare at this for a minute  a ssume that beta is going to be something which is greater than zeroninety or zeroninetyfive or something and try to make sense of what is happening try to imagine what is vt is going to look like in terms of grad w zero grad w one and so on t o start from v one and see what happens what was v one earlier and what it i s going to be now ok  but it still grows my magnitude when i am still adding stuff  so how does it help me in not blowing of the denominator s s o yeah i think you most of you get  so again this is the trick is basically you are using this expon entially exponential moving average so even at the first step earlier i was doing grad w t square  n ow actually doing zerofive into grad w t square oh sorry grad w one square right so that i s what my v one is going to be  n ow what is my v two going to be  i t is going to be zeroninetyfive into zerofive grad w one square plus grad w two square right so this quantity is even shrinking further and at each step this is going to keep a zerofive ok and you see now at each step this is going to get multiplied by this quantity and shr ink further so now i am not aggressively growing the denominator  i am not considering the full gradient  but only a fraction of it and i n fact a very small multiple of it  so i am still accumulating the history  but i am not being very aggressive whil e doing that right so you understand this everyone get s this refer slide time nineteenfiftyfour so now let u s see if we run what would happen a ny guesses ok so initially now this is i think a brown curve  it i s already there  but you can see it  so i w ill keep running it and at some point it will diverge from the green curve  y eah do you see that  n ow  i have reached its destination right so at the point where the b learning rate the learning rate for b was getting killed  i n this case that does no t happen because you have prevented the denominator from growing very large actually multiplied by its small values  so that it does not grow very fa st so a dagrad got stuck when it was close to convergence because the learning rate was killed and it was no longer able to move in a direction of b  but for rmsp rop it overcomes this problem by not growing the denominator very aggressively ok  n ow can you think of any further modifications th ere is everything that you learned so far and my everything yea h y eah i am not very sure why that i agree that i am also bit surprised that it completely overlaps with it  i checked it and that is how it turns out to be and guessing it i s a n artifact of the artificial data that i have created so it is trying to sa y is actually making sense that it should not overlap so much right i nitially it should slightly be biased towards b and then probably that i s what you are trying to say right  but i told it just an artifact of this data that i have  but what matters is from as going to say illusion  but from the illustration is that it actually does not kill the learning rate refer slide time twentyonefortyeight w hat is the one idea t hat now think of everything that you learned in starting from gradient descent then you tried to improve it using something then you tried to further improve it and s o on and now we have taken a slide d two f rom there you are now focusing on the learning rates  but there were other things which you are doing earlier c an you bring those back add momentum h ow many of you say add momentum as if i can just added  y ou are right actually so let u s see what we can do  so it does everything that rmsp rop does t hat means i t tries to make the learning rate inversely proportional to a sane cumulated hi story  by sane mean it does not allow the history to blow up and it also will use the cumulative history of the gradients  so let u s see the update tool for a dam  so what is this term doing  a ctually it i s taking a moving average of there is the same as the momentum base role right j ust taking a moving average of your gradients ok  t he same analogy that i am going to phoenix market city  i am just taking all my history into account ok and vt is again a cumulative history  t his is the same as what was happening in rmsp rop right where you get lost now what would be the next step be  c an you give me the final update rule a t least think about i t m t into vt no  o k just try to think about it and it i s very hard to say it out there are too many grads and suffix e s and so on  so just think about what you did in the momentum case ok  n ow there is one more step which i am going to ignore  i will just say what that step is and the n i will come back to that later on so this is something known as bias c orrection ok  j ust ignore it for the time being  i will come back to this discussion  j ust for the time being just assume that i am taking mt and dividing it by some quantity right so for all practical purposes i am just using mt just dividing it by a quantity ok j ust for now  t hat should suff ice and then my final update rule is going to be this so let me go over this  w hat did you expect here in a normal gradient descent t hey should have been grad w t  th at means the derivative with respect to c urrent w ok in stead of that i am using a cumulated history  i nstead of using just this quantity  i am using a cumulated history  d oes it make sense t his is same as momentum base gradient descent h ow many of you get that o k and now this quantity there is nothing new  t his is the same as what rmsp rop suggested that you divide the learning rate by a cumulated history of gradients right so just a combination of these two  one is take care of the learning rate and the other is use a cumulative history d oes it make sense now o k fine now this part is something that i need to tell you about  so i will tell it to you after i run the algorithm and then  i will come back to that  but is the update rule clear that it i s a combination of momentum plus kill ing the learning rate ok fine refer slide time twentyfivenineteen it is a similar set of equations for bt refer slide time twentyfivetwentyone n ow let u s see what happens to this algorithm is actually call at a dam  it stands for a daptive m oments right ye ah what is can you tell me why that name w hy moments s tudent s ir mean is g ood where is the mean he re this is a mean t his is a moving exponentially weighted average right t his is a n exponentially weighted mean  w hat about this w hat is this quantity  if you tak e the average of this is the second moment right exponentially weighted second moment right so using the first moment and the second moment we come up with an adaptive learning rate so now i will run this algorithm  a re you able to see this s ee a co loured curve o k  so it i s here you see that now ok do you see what happen d o you see this curve ev eryone sees that ok  so what is happening it i s taking u turns right so again whatever happens because of momentum it is happening in this case also and then finally it will converge again  le t me be clear that in this case now it should be very clear  w e need to change who is ta for the slide  so this colour needs to be change d or it should be bright right from the first so what is happening is it i s getting overlaid and then it becomes bright when we need to have a brighter colo u r right from the beginning ok s o this again in this toy example right y ou do no t really see the speed as such because all of them are converging you know a lmost th e same number of steps  but this again i repeat for the toy example  but at least you see that the behaviour is very different and behaviour is consistent with whatever you have put into the update rule right i n one case the learning rate gets killed  in the second case it does not decay and in third case when you using this moments sorry this momentum term you again have this behaviour similar to the momentum gradient descent where you actually overshoot and then you come back ok  so is that clear a ll these algorithms o k now here is the million dollar question refer slide time twentysevenfiftythree w hich of these two you use in practice so what are the options that you have for your back propagation assignment  e ven if you have not read the assignment yo u should just tell me based on whatever you have learned you have gradient descent s tudent m omentum m omentum n ag rmsp rop s tudent a dagrad a dagrad a dam ok  so which of these would you choose and if there is one or which is called eve but it did not really gain much momentum  but a dam so in practice a dam seems to be more or less the default choice  i should tell you that recently there was a paper or called couple of papers which actually show that there is a slight error i mean there i s you could showcase where a dam will not actually converge as expected with but still then after that as is the case in whole of deep learning resources that one person says this work and immediately the next is someone else t his does not work or vice versa right so someone show that this does not work  a dam does not work in some cases  but then someone else did detailed study showing that in most practical applications ok you have taken a toy data set where you can show something under some conditions  a dam will not converge  but if i look at real world data sets like mnist image data or something those conditions do no t hold there  so a dam really works well  so in practice a dam is more or less the standard choice  n owadays at least all the image classificati on work which deals with convolutional neural networks and convolutional neural networks and so on that uses a dam as the optimization algorithm w e have used it largely for a lot of sequence to sequence d learning problems and it works well  a lthough it is supposed to be robust to the initial learning rate right because you are tampering with the learning rate as you go along right y ou are not sticking to eta  but you a r e conveniently blowing it up or shrinking it based on your requirement so it should not be sensitive to the initial learning rate  but we have observe d that at least for the sequence generation problems if you use one of these learning rates as a starting point they work best of course  of course these are heuristic right w e also depe nds on how much data you have and so on i f you are going to train  but only thousand samples and first of all of course you should question why are you using deep learning  but you have gone pass that question already h as everyone else has then you are still be using a deep neural network and in that case may be these learning rates are going to be very small  but in general for a large number of data sets out there which lot of academic research happens which are of reasonable size these learning rates happen to be well in practice now having said that many papers report that sgd with momentum either the n esterov momentum or the v anilla momentum with a simple annealing learning rate w e remember we did this learning rate decay either a constant decay or that heuristic decay that after you look at the validation loss and then decide whether to decay or not  t hat also seems to work at par with a dam right so my advice would be that if you really know what you are doing with sgd and momentum right  t ha t means if you really know how to look at the loss how to track it how to adjust the learning rates and so on w ith a little bit of manual tampering it should work as well as a dam there are people which show that it works well as a dam  but if you are j ust a practitioner who does no t really want to bother too much about setting the learning rate setting the momentum setting the schedules on both of them remember for momentum also we had a schedule and was just given by one of these papers and it might differ for your application  y ou might want to tweak that a bit  so if you are not really bothered about doing all these things then a dam would just be over all the best choice right with very minimum tempering of the initial learning rate a s i said s ome recent work suggested there is a problem with a dam and we will not converge in some cases  but then i t still i mean i would say that juries not out on that yet because there is of course theoretical angle to it and also the practical angle  a gain prac tice has been used widely for the last three to four years  a t least and it works well in a large number of application s right so that i s why a dam would typically be the overall best choice now there is this one thing which i need to do which is i need to te ll you why do we use this bias correction so now what do you actually want to you are taking a mean ok you do no t want to rely on the current estimate of the gradient  but you want to take a n exponentially moving average of the gradients now what wou ld you actually would be doing all this what is the intuition behind this since you are talking about moments and so on  c an you think in terms of probability distributions so let me just try to say this we write that your gradients your values of grad wt right and i will just i think alternately use gt instead of grad wt  j ust needs to gradient in that form  i t actually comes from some distribution depending on the point at which you are right t he gradient would change  but it comes from a certain di stribution and now what you actually want at any time step when you are making this update this particular update ok  i s it clear  y eah when you are making this update what would you actually want i t should not move too much away from sorry s o now yo u r gradients how you are computing say if you are doing the stochastic version you are computing it for every point that you have right wi th respect to that point you would have some loss function and some derivative with respect to your parameters  i f you move on to different point you will have some different parameters so there is some randomness in this ok  so i am saying that these gradients would be treated as random variables which can take on values according to a certain distribution and n ow what do i mean so what would i actually want when i am making an update  so i have to one basic choices  i could have just use grad wt which is the derivative with respect to the current time step  a dd the current time step ok  i nstead of that i kn ow why i am not happy with that because it has this problem that it could pull me to the extreme  so at this point is actually saying change it change your w value in a particular way which is more suited to me some other point would say something else so what we want is that whatever update we make should be very close to the dash of the distribution mean of the distribution right and instead of computing the mean we are computing a moving average and exponentially moving average so now what do we actually want to say i said that gt is the random variable for denoting the gradient  w hat do i actually want  i want the expected value of mt should be equal to what the true expected value of gt  t his is what i want because i want to i do n o t want my upd ates to move in the extreme  i t should be closer to the average to the mean of the distribution  d o you agree that this is my wish list thi s is what something that i should desire for ok now let see what is m t a ctually if i want to write it as a formul a refer slide time thirtyfivetwentyeight so i have mt is equal to one minus beta  i will call this as gt right so remember the gt is grad of w t ok  so now let u s try to write formula for this  so m zero i will set it to zero  so m one is going to be one minus beta in to g one ok  mtwo is going to be beta into one minus beta into g one plus one minus beta into g two and m three is going to be beta into one minus beta square g one plus beta into one minus s tudent b eta square s orry beta square s tudent m inus beta m inus beta wait is the first term correct s tudent yes n o beta ok  so wait what am i oh beta is getting multiplied to g two plus one minus beta into g three  so what is the general formula going to be it is m t is equal to so one minus beta can come out ok  s ummation i is equal t o one to t one minus beta s tudent b eta square beta raise to t minus i and gi right ok  so this is what my mt is ok  n ow let me take the expectation of this t his fine no w ok t his is b one minus beta  n ow this is going to be is that fine so what is this  t his is an ap gp  w hat i s the sum going to be so it is going to be one over one minus o h it i s actually sorry one minus beta r a ise to t over one minus beta  i s that fine so what will happen is this will get cancelled and what you are left with this one minus beta raise to t into e of gt ok  so what is the relation that you have e of mt ok e of mt is equal to one minus beta raise to t into e of gt  w hat did you actually want s tudent e of gt r ight so  now how will you ensure that divide by divide mt by one m inus beta raise to t and that exactly the bias correction that we have done ok  s orry about this messy derivation but i guess most of you get it  if not we will just type it properly and upload it in the slides  h ow many of you got this m ost of you got fine so th at is the similar derivation for vt also fine  so that is why we need the bias correction"}
{"audio_filepath": "Data/Preprocessed/The convolution operation_86.wav", "duration": 1064.0, "text": "so whylet us startso far in the coursewe havelooked atfeedforwardneural networkswe haveseen how to trainthem andwe haveseentwo specialcasesof feed forwardneuralnetworksonewasthe autoencodersfor learningrepresentationsor learning latent representations of inputs and the other thing that we had seen was how to use a feed forward neural network to learn word representations where we saw this word to wake algorithm and it is different variants it was continuous bag of words skip gram model graph a nd so on so those are all since some since applicati ons of the feed forward neural network and now wewill move on fromtherethoughwe willlook atdifferenttypeof neural networktodaywhichis convolutionalneuralnetworksand we look atsome specific architectures which have become popular over the past few years ok so with that i will start this lecture on convolutional neural networks so in the first module we will look at the convolution operation ok so let us see so suppose we are tracking the position of an aeroplane using a laser sensor at discrete time intervals right so you have this ok so you have this aeroplane suppose it is going from say chennai to delhi and at discrete time intervals you are seeing the tracking the position of the aeropl ane right how far it is from chennai at this point right may be it is fifty kilometers one hundred kilometers and so on and now your laser you think that it might be noisy it might not be giving you very accurate measurements so you would be taking these measu rements and say intervals of course it is not in practice you would not do that but just indulge me for the purpose of illustration that say you are taking these measurements every five seconds or ten seconds or something like that now since your sensor is noisy instead of relying on a single measurement you would probably want to take the average of the past few measurements that you have taken so that would give you a more accurate representation of what the current position is does that make sense like you are taking multiple measurements and taking averages of those right and of course more recent measurements are more important as compared to the previous measurements right so this is suppose at time step t say this was t minus five seconds and this was t minus five minutes suppose so obviously you would not want to take give a very large weightage to the measurement that you are taken t five minutes back right because the plane would have moved by a lot by that time so it rely more on the rece nt measurements and less on the previous measurements right so now the mathematical way of lighting this is that you know the positions or the readings that you have taken at time steps one two three up to time step t you are interested in the revise estima tion of this measurement right so you have taken some measurement at time step t and you want a revised measurement of that and the way you are going to compute that is you are going to take a weighted average so w is the weight of all the previous me asurements that you have taken right so the measurement that you take a t one t two t three all the way up to t minus infinity and for each of them would have a weight associated with this so this operation right this thing  you can write it as the following operation that you have a vector of measurements or an array of measurements which is x and you have an array of weights associated with these measurements the farther the measurement from the current time step hopeful ly smaller is the weight assigned to that and this operation is known as the convolution operation right so you have x which is the input w is known as the filter and the operation that is defined as this equation is known as a convolution operation right look but of course in practice you would not do this from infinity right you would probably keep a window you will say i will rely on the previous six measurements that means whatever i took a t t one second t two second up to t six seconds right beyond that it does not really make sense so let us see how this computation happens so this is weight array so now what would be the dimension of this weight array how many entries would it have student seven seven right zero to six so seven entries ok and this is what my situation looks like right so this is the x the measurements which i have taken using the laser ok so i have taken some measurements now i am at a particular time step and i want to make a revised estimate so i have this x t and from that i want to compute s t and the way i am going to do that is by taking a weighted average of all these previous measurements is the setup clear to everyone ok and now this is what my formula is g oing to be so the revised estimate of s six is going to be whatever was x six into w zero  that means the weight assigned to the current time step x five into w minus one that means weight assigned to the time step one x four into two and so on so you get this ok so  i have these seven weights and i will multiply with them with the seven previous readings one is to one multiplication and i will get the weighted average and using that i get a revised estimate now i want to get a revised estimate fo r the next entry how will i get it i will just slide this weight matrix right so i will just slide it by one i will again do the same computation and get the revised measurements again for the next entry i will slide it by one slide it by one slide it b y one and i will keep getting these entries ok so everyone gets the setup how do you do the convolution operation it is basically a weighted average of the previous entries fine so here the input as well as the kernel is kind of one dimensional right yo u so you have it is so you do not have a twod input here you just have a single dimensional input here can you use a convolution operation on a twod input also do you know of any twod inputs images right so we can think of images as twod inputs now again i am trying to do the same thing the setup is the same the story just changes from laser to a camera now so i have taken an image maybe the image was captured and i am not very confident about all the pixels that i have c aptured ok so now for any given pixel i want to re estimate it using it is neighborhood that is what i want to do ok so this is the pixel i am going to look at some neighborhood around it right so every cell here is one pixel just assume that every cell here is one pixel so now i am going to re estimate this pixel by taking a weighted average of all its neighborhoods right so now can you tell me what is my filter going to look like in this particular case my f ilter would be just three  three right so whatever neighbors i want to average on for every neighbor i want a weight so if i am going to average on a neighborhood of three  three then for each of these i will want a weight so my filter would also b e of size three  three how many of you get this ok so we now like to use a twod filter which would be m cross n ok and in general it would be m  m so it would always be a square filter but i am just takin g the case now what this nasty looking formula is doing right so i have a particular pixel so this is an image so i will refer to this pixel as i ij right so it is the i th ij th entry in the image i want a revised estimate for that i want an sij for that so the way i am going to do that is i am going to look at m rows and n columns before it right so i am going to look at this neighborhood of m cross n ok and for each of these i would have a weight associated with it so if i am looking a t say for example this was four  four this pixel was four four then i will look at four one four one so that would be i three three so i will look at that neighbor and with that neighbor i would have some weight associated do you get that how this formula expands so this formula would have m cross n terms for every term you would have a have a weight and that weight you can just represented as this filter matrix so you get this what this formula is doing it looks a bit nasty but it is just the weighted average of all the neighborhood that you have and the neighborhood is a two dimensional neighborhood in this case how many if you get this properly ok now this in this formula actually i am looking at minus a and minus b that means i am looking at previous neighbors right now you should have these questions right why previous neighbors why not future neighbors so why am i not looking at this neighborhood so there is no correct answer here different convolution operations i mean different packages use different convolution operations but the most standard one i believe is when you look at the next neighborhood right that means you at this pixel and you will look at this neighborhood the neighborhood after it right not t he before it ok and in fact so this is the formula that i am going to look at plus j and plus p that means i am looking at pixels in the rows after this and in the columns after this pixel all of you get this instead of before now what is even more natural to do the names surrounding thing right so i will have this pixel and i will look at it is such a way that this pixel is the center of the neighborhood right so that is what i am going to go towards after a couple of slides and that is what i will use for all my convolution operations but in terms of textbook definitions these are the definitions that you will find in textbooks ok so let us let us apply this to a toy example so i have this input which is two d imensional input i have a kernel which is a two  two kernel so my m is equal to n is equal to two so i am going to place this kernel at this location ok and then what will i get as the output a into w plus b into x plus e into y plus f into z right and i will keep sliding this to get the other entries do you observe something about the input and the output student refer time tenseven size the output size has reduced why we will get back to this so right now i just you to notice it is obvious nothing great about it but i will just get back to it more formally later on so for the rest of the discussion we will use the following formula for convolution which is the centered formula right so two m to two n  that means i will be looking at a neighborhood which is centered on the pixel of interest that is why this two m to two m is that fine ok so this is how i am going to look at it so this is how i will place if this is the pixel of interest which i want to re estimate i will replace the kernel such that it this pixel lies at the center of the kernel ok so we will be looking at both preceding and succeeding neighbors ok so let us see some examples of twod convolutions applied to images so this is an ima ge i decide to apply the following convolution operation to edge ok tell me what the resulting image would be student blurred blurred why blurred student we are taking average we are taking the average right so it would be blurred you get the i ntuition so this kernel basically i have fitted at every pixel and i have computed the average around it and place at pixel by that average value and when we are going to take average things are going to get blurred right because all the sharpness is go ne ok now let us look at this kernel what will this do sharpen why because one was blurred the other has to be sharpened what is happening here student refer time eleventwentynine it is subtracting the neighbors right so you are taking five times the current pixel and subtracting the neighbors from it right so if the neighbors are similar those would get subtracted and this would stand out really right does that make sense this will result in a but this in my on my laptop this looks like a sharpened image i do not know why it is looking like this here ok it is a sharpened image just trust me you can so actually are common right so people who have used adobe or any of these photo shopping softwares right so you have this click button and where you say take an image sharpen and blur it so this is exactly what the tool is doing in the background it is applying this convolution operation throughout the image so when you say blur it is basically placing that convol ution operation throughout the image and computing the blurred image and same for sharpening and all these other spatial effects that you have most of them come out of some convolution operation ok so for example the next one  what would this do student refer time twelvetwentysix so i will give you a hint when will this result in a zero output student refer time twelvethirtytwo when all the neighbors are the same as this right so then when will it result in a nonzero output student  refer time twelvethirtyseven when there is a difference when there is a difference so looking at this image tell me one place where you know that it will result in nonzero output student refer time twelvefortyfive all the boundaries right so this is basically an edge detector in the slides it appears properly ok so this is basically an edge detector and you get the intuition that these boundaries whether neighbors are not the same as the current pixel you will not get a zero value in this case when all the neighbors are the same as the current pixel so you are taking the sum of the eight neighbors and subtracting the current value eight times so that would be zero right ok so enough of examples so now we will see a working example of a twod convolution so i just want to drill this idea of what happens when you do a twod convolution so what we are going to do is we have this three  three kernel and assume that everything here is a pixel ok everything here is a pixel so i am going to slide this three cross three kernel across this filter now when i place the filter once on the image how many outputs do i get student one one output so if i keep sliding it across the image i will keep getting one one pixel in the output ok so what the resulting thing that i get is known as a feature map ok because it is the original input that we have taken for every pixel you have tried to approximate it or whatever filter weights you have applied and it necessar ily does not mean that you are taking an average it could be some weird average of your neighborhood right so you have extracted some features from there so for example in the edge detector case you could think of it that you have extracted the fea ture that this pixel does not lie at a boundary right that is why you get the black pixel do you get that you see this way of interpreting a convolution operation that you are trying to extract some features from that neighborhood so in this earlier example whenever you got a black you are basically extracting the feature that this pixel does not l ay at a boundary is that ok fine so now you could get one such feature map by using a single three cross three convolution operation ok if i use multiple such convolution operations what would happen i will get multiple feature maps ok so let us try to understand this what is the dimension of my original image m cross n into three why is it into three student rgb channels rgb channels ok rgb is what we will h ave right so we will have this three  m  n so we will return back to this idea and from now this one image by using a single kernel so this in fact in for this figure right i am assuming that the input i s one cross m cross and i am not assuming there are three channels although it is a colored image but just bear with me so it is a one cross m cross an image and when i apply a filter i get a one feature map if i apply k such filters i will get k feature maps  so one feature map could be for the blurring one one could be the sharpening one one could be the edge detector and so on right there are various such filters that you could apply now in the oned case we slide a one dimensio nal filter over a one dimensional input in the twod case we slide a two dimensional filter on a two dimensional input what would happen in the threed case so now we are going to this rgb images right so we will have three cross m cross n as the input what would happen in the threeg threed case not threeg so what would a threed filter look like student box look like a cuboid a box basically and we will call it a volume why volume because it has a width it has a hei ght and it will have a depth so this is what a three d filter would look like i will assume that it is depth is the same as the depth of your input what is the depth of your input in this case student three three so i will assume that the depth of the filter is the same as a depth of the input and t he width and height could be three  three five  five seven  seven anything right so we will get into that in more details later on so once again we slid e this volume across the entire image ok what is the output going to be twod or threed student twod why so when i was oned i was getting oned output when i was twod i was getting twod output threed again twod output why because i have assumed that no not width stude nt refer time sixteenfortynine the depth of the filter is the same as the depth of the input so now you just imagine this if you can suppose the filter was of depth two instead of three then i would slide it horizontally first vertically and then across the depth also so then what would be output be in that case student three dimensional three dimensional and it would have depth of two student two everyone gets that right but for this lecture i am always going to assume that the depth of the filter is equal to the de pth of the input always right and that is how it is for all the convolution neural networks that we will see the depth of the input is going to be equal to the depth of the filter the rather the depth of the filter is going to be e qual to the depth of t he input so whenever i apply a threed filter i am actually doing a twod convolution because i am moving only along the width and the height i am not moving along the depth so the output is going to be twod so now can i have multiple such filters yes each filter will give me a twod output if i have k such filters i will have a student refer time seventeenfiftyfour k  twod output right k twod outputs fine"}
{"audio_filepath": "Data/Preprocessed/How LSTMs avoid the problem of vanishing gradients_110.wav", "duration": 446.0, "text": "so that was lstm and grus now the issue is that i have given you a very explanation that why you selectively read write and forget should work but you have not actually formally proven or even given an intuition for with these sets of equations how are we sure that the gradients will f low back right we introduced a bunch of equations remember in the case of lstms sorry in the case of rnns the problem was because of the recurrent connections right because you had these recurrent connections this w which was the recurrent parameter ri ght wh ich was connecting cell state s t one to cell state s t this was repeatedly appearing in your gradients right and that was causing the problem because when you had this multiplicative factor lambda into w and then if you compute the and this was some raise to t so then if you compute this magnitude then if the magnitude of w blows up then the whole thing will explode if the magnitude of w vanishes then the whole thing will vanish right that is the problem that we had so  this was because of the recurrent connections do we have recurrent connections in lstms or grus for that matter do you have recurrent connections yes or no student yes yes so then that problem could still occur right i mean if you had that the cr ux of the problem for the vanishing gradient was this recurrent connection which is getting multiplied and hence reading to problem but we still have recurrent connections the case of lstms also and why should things become any easier in this case how m any if you get the question how many if you can give me the answer selectively that is a good answer so can you think of what is happening here so first thing that we going to do now so i will go on to the next module whe n i going to give you intuition for what is happening and then we will do slightly in fact a rigorous proof of why it actually solve the vanishing and exploding gradient problem ok so let us look at the intuition first how lstms avoid the problem of v anishing gradients i am only focusing on vanishing gradients exploding gradients are actually easier to deal with why student refer time twotwentysix what can you do what are we interested in when we compute a gradient direction so if the magnitude i s very large what can we do just normalize it and restricted to be a certain magnitude so that is known as gradient clipping so exploding gradients in that sense is still not a big problem but vanishing gradients is because if it vanishes you cannot do anything because you could think of it that you already have a learning rate which is getting multiplied with the vector the gradient now in addition to the learning rate which was anyways clipping the norm of the gradient right so you are doing an expressive clipping also so it just like a additional learning rate inductions right ok so here the intuition and then will go to the more rigorous stuff not in this class probably so during forward propagation the gates control the flow of information right the gate decides how much of s t one should be pass to s t ok and they prevent any relevant information from being returned to the next state similarly during back propagation the gates will regulate the flow of infor mation so what i mean by that is that if at a cer tain state you have computed s t f t s t one  i t s t right so this gate is actually deciding how much information flows in the positive direction ok and suppose this gate value was zerofive so the zerofive of this information from s t has been carried on s t one ok now during back propagation what is the derivative of s t with respect to s t one going to be partial derivative i s going to be a f t  th ink of s t and s t one as single variables like you know n dimen sion variables then the just f t  of course you are forgetting that what kind of a network is this ordered network right so you cannot read as till de t as a constant where s tilde t also somewhere depends on s t minus ok but just th is assume that maybe this vanishes and that is the worst case assumption right because i do not want it to vanish but i am assuming that the second term vanishes but even then with the first term i will have a gradient which is proportional to the gat e why is that fine so remember that i am not making a easy assumption i am making a worst case assumption this is not favourable to me i am saying that the second term vanishes and i dont want it to vanish but i am just trying to prove that even in the worst case by the second term vanishes you still have this gradient f t from the first term right and why is that good why is that ok because f t decides how much flew in the forward direction and it is also deciding how much goes back in the bac kward direction so it is a fair regulator with says that if i passed on only this much information in the forward direction then during backward pass also i should only make a responsible by this much ok now let us look at a situation where you had f one f two f three upto f t and all of these gates were zerofive now zerofive seems a reasonable value but when we have zerofive raise to t and t is a large value what is going to happen this quantity is going to vanish so what is happening is that sone contribution to st i n the forward direction itself had sones contribution to st in the forward direction itself was had already vanished right because it was continuously getting multiplied by zerofive zerofive zerofive so it is like this chinese vespring problems right so this guy sai d something whereas next guy added noise the next guy again added noise and so on till the time it reach the t th guy this information was completely lost so in the forward pass if sone did not contribute to s t in the backward pass should i make it respo nsible for the crimes of s t no so what is happening in the backward pass again the gradients are getting regulated by the same forget gates so again in the backward pass will have a situation that by the time the gradient reach is sone it would be zero five raise to t and that is fine it is going to vanish but that is because even in the forward pass it vanished so let it vanish in the backward pass also so this kind of vanishing is ok so this is just the same thing writte n in words so if the stated time t minus one did not contribute much to the state at time t because f t was tending to zero right then during backpropagati on the gradients flowing into s t one minus one will also vanish because again during backpropagation the gra dients will get multiplied by f t and they will vanish but this kind of vanishing gradient is fine this is fair because if we did not contribute in the forward direction why should i help you hold your responsible in the backward direction right so that is fair so the key difference from rnns is that the flow of gradients is now controlled by gates which give the same regulation in the forward pass as well as the backward pass right so only if you contributed to something you will be held respo nsible if your contribution vanished your responsibility in the backward pass will also vanish right so that is the intuition and will next see an proof for this a proof actually it s as based on the intuition but i just make it more formal in terms of introducing the notations and so on so that problem we will do it in the next class ok"}
{"audio_filepath": "Data/Preprocessed/Evaluating word representations_84.wav", "duration": 486.0, "text": "n ow we come to this important part about how do you evaluate word representat ions so there are different tasks that are set up i hope some of you have read that paper and i can see that none of you have read that paper so semantic relatedness is one way of evaluating word representations refer sli de time zerothirty so ask humans to judge the relatedness between a pair of words so i construct some pairs of words and i show them to a human and ask them how related do you think they are on a scale of one zero to one so it is likely for cat and dog so meone would say zeroeight or at least you would expect values greater than zerosix now you have learned the representations using your model it could be any of the models that we have seen so far continuous bag of words skip gram or glove vectors so these are the three things right continuous bag of words skip gram which is known as wordtwo v ec and the glove representations and within them yo u could have this hierarchical s oftmax and other things and so on so you could if i asked you what is the similarity be tween cat and dog according to your word representations you could just use the cosine similarity and tell me that this is the representation right so now i will have many search words w one w two for which i have the human judgment and i have the model ju dgment right so i will have w one one w two one then w two one sorry one two and so on i will have many such word pairs for each of these word pairs i would have the human judgments and i would have the model judgments right how close do the humans think they are an d how close do the think they are now i can compute the correlation between these two decisions or these two random variables and i would want that for a good model this correlation should be high so whenever humans said that the two words are actually simi lar the models word vectors should also predict a high cosine similarity and whenever humans said that the two words are not similar the models word vector should also result in a low cosine similarity how many forget this so that is one way of evaluating how good your word representations are right so as i was saying earlier how do you tune those parameters so you could have such a set once you have learned some word representations and you want to see whether parameter k one was better than sorry rather hyper parameter k one was better than hyper parameter k two you could just take those two word representations learned by these two different hyper parameter settings evaluate them on this corpus and whichever gives a higher correlation you can keep that hyper parameter how many of you get that other task is synonym detection so from a resource known as word net or from other dictionaries you could get all the synonyms of a word so then peop le create a corpus where you give us in sin a word and give four candidates or some k candidates out of which one of these is the correct synonym the others are just distraction words right and distracting words now what would you expect your word represe ntations to do you have word representations for all of these what would you want how would you pick up the synonym based on word representations students refer time threethirteen the one which has the highest cosine similarity so  again you will compute the cosine similarity you will rank these and you will pick up the synonym right and now again i gave you one hundred such instances i gave you a word for candidates and i gave y ou one hundred such different word comma candidate pairs and you pick the synonym for ever yone and see for sixty of them you got it right then your accuracy sixty percent so that tells you how good your word representations are again if you are given two different hyper parameter settings one gives you sixty percent accurate the other gives you seventy p ercent accurate you will probably go with the one which gives you seventy percent accurate they are find how you can use this the third is analogy task where you find the nearest neighbor of this opera tion what should it be g randdaughter this is this analogy teller brother is to sister as grandson is to something right so now the idea here is that if i mea n it is like pretty weird right so if i take brother minus sister i get something now if i a dd grandson to that then i should get granddaughter it is intuitive in a way right i mean this is what you expec t your word vectors to do right so that is how the analogy task works so you could set up an analogy task you could have and you could ge t several such an analogy tasks from online tests and so on and you would want your word representations to exhibi t this kind of a behavior right so again you have these one hundred analogy tasks for each of these you know the true answer and from each of thes e you predict the answer from your word representations and first see for how many of them you get it correct then you could also have a syntactic analogy so you can tell me what this would be right in fact here again it should be the other way round  we works minus we work thus we speak would be we speaks right so th at is the syntactic thing right so you are getting a different form of the world so your word representations should also have this kind of properties that is what you desire so just evaluating whether your word representations show this kind of a property or not so we have seen three tasks one is semantic relatedness whether a pair of words how do humans rank it and how do the model how does the model rank it then the synony mous detection and the analogy tasks in each of these you do something with the word representations in the first two you use the dot product and this last one you use some arithmetic operation over the word representations so you would want v brother minus v sister is equal to v grand son minus v granddaughter right so v granddaughter right s o  that is there is a plus minus error there so now which algorithm gives the best result right so whenever we see a bunch of a lgorithms same as we did with adam and refer time fivefiftynine and so on we always want to answer this question which of these gives the best result so there was this study done by boroni et al in two thousand and fourteen that show that the predict based models right which are either which are the predict based models actually skip gram continuous bag of words and even glove for that matter right because it is also a predict based model these continuously or consistently outperform count based models that is what they sa id but a year later there was a separate study done by someone and in my opinion this was a more thorough analysis because the earlier study right they did not really give svd a chance to win in my opinion this is all on camera but the later the second set of guy right they gave svd a chance to win so i will tell you one example of how they gave is really a chance to win so remember in wordtwovec you had this weird three by four which you are using to raise the probability right now what they did is they s aid even in the co occurrence matrix actually these counts that you have if here you are using them by three by four in the case of wordtwovec and getting better results why not do the same thing in the co occurrence matrix also at the end of the day you are rai sing the count to three by four right so whatever counts you have here based on that you will compute ppmi or pmi or whatever but first why not try to adjust these counts so why not have a parameter k such that you can raise the counts to this parameter and then do all those computation and that is fair because the wordtwovec has a parameter hyper parameter so why not give a similar hyper parameter to svd similarly they did something to take care of the k negative s amples which wordtwovec has why not giv e sv d also similar chance right so when they did these kind of adjustments they found that after these modifications svd does as well as or even better than wordtwovec models for the similarity tasks but not for the analogy task but the analogy task was th e last task right brothers to sisters grandsons to grandmother right so in most cases we care about similarity and in very few cases we care about analogy if you are doing nlp application so that means in most cases svd would just be fine so that is what i just said at the beginning"}
{"audio_filepath": "Data/Preprocessed/Batch Normalization_73.wav", "duration": 899.0, "text": "nowwe wil l end with something known a s batc h normalization whic h is again almost a defac to standa rd at least in convolutional neural networksso if y ou are dealing with convolutional neural networ ks you will use something known as batch normalization so let us see what it is so this is again something which is some method which allows us to be less care ful about initialization  so let us see why that happens refer slide time zerothree six so to understand the intuition behind this let us consider a deep neural network ok and let us focus on the last two layers h four and h three n ow typically will use some mini batch algorithm for training right  so we will use mini batch version of gradient descent or mini batch version of adam or any of these algorithms right now what would happen if there is a constant change i n the distribution of h three no just think about the question that i am trying to ask you  so as far as these two layers are concerned h three is the input and h four is the output it does n o t matter what has happened so far or in particular does not matter what x was whether it came from a normal distribution or whatever distribution right a t this point my input is h three and my output is h four now i am training it in mini batches what if across batches my distribution of h three looks very different what would happen is it a good thing or a bad thing i t is a bad thing  right so if you have training data right just think of this as i said just focus on this layer if you have an input which is not following a fixed distribution is constantly changing during your tra ining then that i s always a bad thing right because you try to adjust to one distribution and now again the distribution is completely changing s o that always makes our training very very difficult  right so if you have a very fluctu ant distribution t hen a training is going to be hard ok  so that i s the intuition that i want to build refer slide time twosix so now this could actually happen so it would help if the pre activations at every layer are you need gaussians because for the input we ma de a case that will make the input as unit gaussian right so that things are very nice they are all coming all the inputs are coming from the same distribution but we now realize that at every layer we have an input right it is not that the original in put the only input even h three is an input even h four is an input and so on so why not ensure that at every layer your inputs or your h one h two h three also is something which looks like a gaussian distribution which comes from a gaussian distribution  w hy not e nsure that that i s the basic idea behind batch normalization and how do you do that is the following that you had computed this s i k just as we had done in the derivation earlier right  so s i k is one of these guys now if you do this what are you act ually doing you just normalizing it right you a re subtracting the mean and dividing by the variance so that means you a re making it zero mean unit variance and that i s the intuition which i was trying to build that why not at every layer have this good di stribution which is zero mean unit variance b y even i f you are feeding it multiple batches for that batch you will ensure that by this subtraction and division or the normalization process the data will become unit variance and zero mean ok  so now at ev ery batch the data is coming from the same distribution even if it was originally from a distant different distribution b ut how do we compute this mean and variance so did you understand the question that i am asking i am focusing on this s i k i want t o subtract the mean of that s i k how do i do that so the name gives it away batch normalization it can not be more explicit than that so compute the mean for the current batch and the variance for the current batch and normalize your inputs or normali ze the s i k according to that you get this  so now end up with a situation where all your inputs at every layer across different mini batches seem to come from the same distribution is it fine the current batch  so you take the average value from the cu rrent batch so then it will become zero mean for that batch and unit variance for that batch and this you are ensuring for every batch  so every independently every batch you are ensuring that it comes from a zero mean unit variance distribution  right so overall the effect is that all the batches are coming from the same distribution no  so at validation time you will compute the mean and variance from your entire data entire training data once after the training is done right so now we will comput ed from a mini batch and this is ensure that across mini batches now your input always comes from a zero mean unit variance distribution across all the layers refer slide time fivetwo t his is what a deep network will look like with batch normalization right  so what will happen is you passed an input you computed this tan h then you wi ll have this batch normalization layer watch is what is the operation that the batch normalization is going to do this is the operation that it is going to talk ok e ver yone gets that and now it gives me a unit normalized distribution sorry it gives me a input coming from a zero mean unit variance distribution and then i pass it to the next layer again at a batch normalization layer so after every layer you will actuall y add a batch normalization here now my question is is this legal w hat is legal in this course anything that is differentiable  right so you have to make sure that if we have added this operation it should be a differentiable operation  so that you c an come so now the gradients have to flow all the way here right so that means i should be able to compute the gradients with respect to this  so now this is one of my a i and i should be able to compute dou a i with respect to something or rather the lo ss dou of the loss function with respect to a i by turns out that the operation  t hat you have done is actually differentiable refer slide time sixfour y ou ca n actually work that out and it is not important i am no t going to derive it because it is ju st yet another derivative that you wi ll take but it is a you should get the intuition from here right what you a re doing is this simple operation and this just looks differentiable right so the operation that you a re doing is differentiable  so that i s why you can add these batch normalization layers and you can back propagate through this layer but now what i s the catch here it somehow ties to the question that he was trying to ask you are actually enforcing that all your are zero mean and unit varia nce  right so this is again som e sort of a constraint that you a re enforcing right what if that i s not the best situation in which the network can learn what if to distinguish between some classes it was ok i f the distribution was not same across all the batches they get this they a re enforcing a certain consider they are enforcing a certain condition on all the layers and all of them have to be zero mean and unit variance but that may not always be good refer slide time seventwo so they do someth ing which is counterproductive let us see what that is why not let the network decide what is best for it so after the batch normalization layer so this is what a normalized s i k was after you have done that you compute a y k and this is not the fina l output this is the output at the k t h layer this is equal to this w hy do they do this and remember that gamma and beta are going to be learnable parameters what are you doing actually you a re again scaling it and shifting it this is the same as adjusti ng the variance and the mean right so now what happens if the network learns the following you get back the s i k  so you had taken s i k and you had normalized it  b ut now if you allow these gammas and betas to be there in the network then the network can decide that maybe at this layer i do n o t want this normalization i just want to stick to whatever output i was getting so it could learn the gammas and betas in this way and ensure that you get back the unnormalized s how many of you get this fine l ot of you do n o t seem to get this but i am pretty sure if you go back and look at it you will get it  right so what is happening here is that i s why i said it is coun ter productive that you first forced it to make at unit mean and zero variance and now you added no zero mean and unit variance and now you added this operation which is again a scaling and shifting operation so remember that when you make the data zero mean and unit variance that i s exactly what you do you shift it  so that it become zero mea n and you scale it  so that it becomes unit variance so you are again introducin g parameters which again introduce the same flexibility that you could learn gamma and beta in such a way that you could get back the original data which was not normalized ok  so i f the network wants to learn that and if the network fees that i s the right thing to do then it has the flexibility to learn those parameters and you can recover si i think the rationale is that your first making is something which is more stand ard right and then from there trying to learn it instead of just trying to let it learn in the way d o you get the difference between the two the first bringing it to all of these things to some standard value which is between i mean which is the normal d istribution and then from there allowing it to learn wherever it has to learn right that i s the idea  but it could be the case that the other thing also works here refer slide time ninethirtythree so now what we will do is we wi ll compare the performance with and without batch normalization on mnist data using two layers refer slide time nineforty so here in this figure what i a m going to draw is the validation loss am i no the training loss as i keep increasing the number of epochs and here i am showing you the histogram of the activation functions at layer one  so i have trained a deep feed forward neural network and i a m showing you what do the activations look like at layer one with and without batch normalization so remember that we started with this intu ition that without batch normalization there would be this constant fluctuation and the data would seem to come from different distribution at every training instance w hereas with batch normalization you are ensuring at your data comes from zero mean uni t variance distribution right and so that is one thing which i want to see another thing i want to see is that how does i t affect training  right so that i s the animation that i am going to show you so focus on all these three things i do n o t know how you will do it  but focus on this focus on this and focus on this with tw o eyes refer slide time tenfortynine so let us see to see what happened right  so this so now look at the focus on the leftmost figure  so that does not seem to change much with respect to it is mean and variance right  but if you look at the middle figure that i s constantly changing it is mean and variance  right a nd you see the effect on the training loss that the first one which was with batch normalization that converges faster as compared to the second one right again an empirical result i am not really proving that this will always happen t his is what empirically we observed so this was the story that we covered from one thousand nine hundred and eightysix to two thousand and six where back propagation was already it was alread y discovered but was not working well and there was this s park in two thousand and six that showed that we could do some things to make training really work for deep neural networks  but maybe that something is not sacrosanct w e could try different things what we tried at that time was unsupervised free training which is almost non existent now but that lead that led to these thoughts that maybe this is because of optimization generalization regularization activation functions and so on  right so there was a lot of r esearch in these different areas and that led to a lot of developments which was better optimization algorithms better regularization better activation functions better initializations and batch normalization right so these a few concepts that you ha ve seen in the past few lectures one being dropout and the other being weight initialization using this xavier initialization or h e initialization and this batch normalization  right t his is something which is all prevalent right  so this is something tha t you will see in all deep neural networks that get trained definitely in convolutional neural networks and more often than not even in recurrent neural networks so these are the two most popular types of neural networks so in both of these you will se e that these ideas are regularly applied and they always lead to more stable training or better generalization refer slide time twelvefortyseven so now this was all which happened till two thousand and sixteen or seventeen what has happened still since then so there is still continuou s research in designing better optimization methods  so as i said after adam there was this eve which did no t become very popular  but there i s still people looking at better optimization methods and there is something which has been developed on adam and came out in d ecember last year now people have also started looking at data driven initialization methods right  so instead of having this fixed initialization which is drawn from a unit or just which is drawn from a normal distribution and then just div ided by the square root of n w hy not think of data driven initialization methods that  so there are some works on that again not very popular because most of the shelf things that you will try will not really do any data driven initialization b ut if you really think that you are stuck at some point then you could look at some of these works and see how they try to come up with initializations based on the data that you are dealing with and now after batch normalization there have been some other types of normalizations which have been proposed which seem to work better than batch normalization b ut largely the stable configuration which has kind of prevalent is adam in terms of optimization xavier or he initialization in terms of initialization relu in te rms of activation functions w hat else is there batch normalization in terms of again regularization plus initialization and dropouts in terms of regularization r ight so these are roughly the key terms that you wi ll almost see in all the deeply living d eep neural network people that you see right you will always see when they describe the hyper parameters they will say that this is how we initialized is this is the drop out that we use this is the batch normalization and the training algorithm more oft en than not is going to be adam so they have seen some very crucial elements of training deep neural networks over the past two to three lectures right and now we will build on these and we will assume that this is what you are going to do so now when i talk about neural networks like convolutional neural networks and so on i not go back a nd tell you use adam or use batch normalization or assume that you already know these things and you will try to train your networks using these tricks that we have your lea rned t he last couple of lectures have been about tips and tricks for deep neural networks a nd from here on in the next lecture will move on to what wordtwovec because that i s what you need for your assignment  so in the next lecture we will do a word repr esentations so that i s essentiall y seeing an application of feed forward neural networks and from there on we will m ove on to convolutional neural network"}
{"audio_filepath": "Data/Preprocessed/Contractive Autoencoders_56.wav", "duration": 474.0, "text": "so with that we will move on to something kno wn as contract ive autoencoderso this is yet another type of auto encode rs aga in with the sa me aim that you want to do some kind of a regularization so it again tries to prevent and over complete auto encoder or even an under complete auto encoder for that p oint from learn ing the identity function so it does not allow you to simply copy the inputs to the outputs that i s what it i s trying to learn a nd it does so by adding the following the regularization term to last function and the way it does this is b y defining the following regularization term ok w hat is this term  o k let u s see some things which we already know what is this f robenius norm of some matrix what is this matrix s tudent j acobean j acobean what is the j acobean s tudent refer time onezero w hat are the two variables here that you see s tudent h h and s tudent x h is a scalar matrix vector s t udent v ector v ector x s tudent v ector v ector right so it is some function between two vectors ok and it i s a matrix so take a gues s how many entries would n o t you have  if x is r n and h is r k s tudent n cross k n cross k even if you do n o t know what the entries are you are able to guess that it i s going to be a n cross k matrix right refer slide time onethirtyfour now let u s se e what this n cross k matrix looks like ok refer slide time onethirtyseven so it has the input has n dimensions and the hidden layer has k dimensions  so this is what the j acobean looks like w hat is the first column i f the partial derivative of every ne uron in the first hidden layer with respect to the first input right and now you can see what the other col umns would be  t his is what the j acobean is this basically the derivative of h with respect to the vector x answer is just you are taking a derivat ive of a vector with respect to another vector you will get a matrix as the output ok n ow what does the j l th entry here capture actually s tudent refer time twotwelve w hat does a derivative capture s tudent refer time twofourteen h ow much does h l chan ge with a small change in s tudent x k x k right that i s what a derivative captures is that fine and then what does the f robenius norm capture it i s just the square of sum of the square of all the elements of the matrix right so it i s basically how m uch each of these elements vary with respect to the input and we are just taking the square of that so you see what is the term that we have added refer slide time twofortyfour now tell me what is intuition behind this ok so when would this term so r emember this term is added to the loss function and you are trying to minimize the loss function so that means you want this term to go to s tudent refer time threeone y ou want the f robenius norm to be s tudent zero refer time threethree zero right ideally of course that will not happen because there is always a tradeoff between l theta and omega theta if you make it zero then l theta would be very high right refer slide time threeseventeen so now what would happen if one of these guys say dou h one by dou x one actually goes to zero w hat does that mean h one is not sensitive to variations in x one right fine b ut was our original mandate what did we want these neurons to capture w e wanted the neurons to capture these important characteristics right so if x one cha nges we want h one to change do you get that  h ow many of you get that w e wanted the neurons to capture the important characteristics of the data right b ut now we have added a contradictory condition which says that we do n o t want the neuron to capture a variations in the data do you see this so what is happening here l theta says that i should be able to capture these variations right o ther wise i will not be able to reconstruct i f all my h i s are not sensitive to variances x one that means i give it any x one it will produces the same h i is that clear is that with ok everyone right that means so see this is this  so i have these training examples occurs all these training examples my bold x which is vector x is going to change  t hat means x i s which are the elements of this vectors are going to change now what this condition is saying is that if i change x i i do n o t want the h l s to change i do n o t want the values of the hidden representations to change s o that means it is changing t he respective of what is the input fed to it try to produce the same output do you get this argument  o k t hat means it i s not capturing any important characteristics of the data is that fine is that valid argument but that i s not what we wanted we wa nted it to capture the important characteristic of the data  so what are we trying to do now refer slide time fiveone so just i it i s hard for me to do evaluate what you have said but just pay attention and see if that is correct you can judge it on your own right so that i s the actually the idea right we have put these two contradictory conditions with each other right l theta says capture the important variances of the data o mega theta says do not capture variations in the data watch the tr adeoff capture only very important variations in the data do not capture the variations which are not important c an you relate this to something that you have seen before s tudent b ias variance n o the other answer there are only two answers bias varian ce and pca when i say the other answer s tudent p ca w hat am i trying to force it to do capture only the important variation i t i s if it is not clear right now we will come back to this ok refer slide time fivefiftytwo so let us try to understand with this with the help of an illustration right how many of you get the argument which i made on this slide ok most of all refer slide time sixthree now this is the situation i have u one and u two as my dimensions fine which of this is important u one th e variations in the data across u one is something that i should care about b ecause i can see that brings in some difference what about the variations in u two s tudent n ot important n ot important they seem like noises because these variations are there th ey are not all lying on the central line they are slightly away from the line here are some variations b ut should i go out of my way to capture these variations does it make sense to do that n o right so it makes sense to maximize a neuron to be sens itive to variations along u one b ut it does not make sense to make the neuron sensitive to variations along this other dimension which is u two ok  by doing so we can balance the two conditions  so one condition was trying to capture all the important varia tions ok do this but do it only for the dimensions which really matter t he other conditions says that do n o t capture important variation s ok do this  but do it only for those dimensions which do not matter w hat is this remind you of a t least the diagra m should have it away right s tudent refer time sevenseventeen i t i s same as p rinciple component analysis right  so that is exactly what you try to do in pca you try to capture the variations across the important dimensions but not across the non important d imensions  h ow many of you get the concept of contractive auto encoders  o k good so i think that i s a where we will end lecture seven refer slide time seventhirtyeight a nd just a quick summary so we showed that under certain conditions auto encoders are equiva lent to pca a nd we use this result very crucially there that svd theorem i will not state it refer slide time sevenfiftyone a nd then we looked at different types of regularizations for auto encoders where we looked at weight decaying  t hat means t he stan dard l two norm we looked at the sparse auto encoder the contractive auto encoder and we also looked at these denoising auto encoders right so that i s the summary of this lecture"}
{"audio_filepath": "Data/Preprocessed/PCA : Interpretation 3 (Contd.)_48.wav", "duration": 86.0, "text": "refer slide time zeroeleven a quick summary we have seen three different interpretation s of pca and eigen vectors played a very crucial role in that and the other thing which played a crucial role was the covariance matrix of the original data  a nd with these three different interpretations what we realize is that the solution that we get or the transform data that we get projecting the original data on to the on to a basis consisting of eigen vectors ensures that there is high variance across the new dimensions and we can ignore of the bottom top n sorry bottom n minus k dimensions along with these variance is not high  t his also ensures that the error in reconstructing the data by ignoring this dimensions is minimized right it i s a lowest possible error a nd it also ensures that the covariance between your retained dimensions is zero becaus e we are able to diag onalize the covariance matrix of the transform ed data so that is what we had so now if you think of it right just to connect it t w o things that we need later on for auto encoder right w e are trying to learn a new representation for the data right and we are trying to also compress the data and we want this compression to be such that it i s as lossless as possible right w e are going from n dimensions to k dimensions and still we want to retain the essence of the data and do n o t wan t to lose out much of the information in the data ok  so that i s essentially what pca is doing n ow let u s see this in practice"}
{"audio_filepath": "Data/Preprocessed/ From Spring to Winter of AI_2.wav", "duration": 779.0, "text": "we will start talking about artificial intelligence and this is titled as from the spring to the winter of ai so i am going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what eventually happened to the initial boom and so soletus startwithone thousand nine hundred and fortythreewhereasisayingthattherewasalotofinterestin understanding how does a human brain work and then come up with a computational oramathematicalmodelofthatsomccullochandpittsoneofthemwasa neuroscientist and the other one was a logician no computer scientists or anything at that point of time and they came up with this extremely simplified model that just as a brain takes a input from lot of factors so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be our factor xone you could think about is a weather good to go out is it raining would it be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on so being kind of presses all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that i want to probably go for a movie so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with the binary decision right so this is what they proposed in one thousand nine hundred and fortythree so now we have come to an artificial neuron so this is not a biological neuron this is how you would implement it as a machine right so that was in one thousand nine hundred and fortythree then along and then thiskind of ledto a lot of boom in our interestin artificial intelligence and so on and i guess around one thousand nine hundred and fiftysix in a conference the term artificial intelligencewasaformallycoinedandwithinaoneortwoyearsfromtherefrank rosenbergcameupwiththisperceptronmodelofdoingcomputationsorwhat perceptron model of what an artificial neuron could be and we will talk about this in detail later on the course and not tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages do you find anything odd about this statement yeah so learn and make decisions make sense but why translate languages why is so specific why such a specific interest in languages so that you have to connect back to history so this is also the period of the cold war and there was always always a lot of interest there was lot of research and translation was actually fuelled by the world war and evens that happened after that w here these countries which were at loggerheads with each other they wanted to understand what the other country is doing but they did not speak each others language that is why there was a lot of interest from espionage point of view or from spying and so on to be able to translate languages and hence that specific require and lot of this research would have been funded from agencies which are interested in these things right and the defence or war or something so and this work was largely done for the navy and this is an this is an extract from the article written in new york times way back in one thousand nine hundred and fiftyseven or fiftyeight where it was mentioned that the embryo often this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of it is existence so i am not quoting something from two thousand and seventeen or eighteen this is way back in one thousand nine hundred and fiftyseven fiftyeight why i am that is why i like the history part of it so recently there is a lot of boom or a lot of hype around ai that ai will take over a lot of things will take our jobs it might eventually we might be colonized by ai agents and so on so i just want to emphasize that i do not know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one thousand nine hundred and fiftyseven one thousand nine hundred and fiftyeight right this not something new that people are talking about now it is always been there and to what extent this promise will be fulfilled is yet to be seen and of course as compared to one thousand nine hundred and fiftysevenfiftyeight we have made a lot of progress in other fields which have enabled ai to be much more successful than it was earlier for example we have much better compute power now we have lots of data now and all thanks to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from it so we have made a lot of progress in other aspects where which ai is now at a position where it can really make a difference but just wanted to say that these are not things which i have not been said in the past it has always been the it has always been considered to be very promising and perhaps a bit hyped also so that is about one thousand nine hundred and fiftysevenfiftyeight then now what we talk about what is all the for the past eight to ten years at least when we talk about ai talking about deep learning and that is what this course i s about largely about deep learning i am not saying that other and what deep learning is largely about if i want to tell you in a very layman nutshell term is it is about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal so this is like a schematic of what a deep neural network or a feed forward neural network would look like now this is again not something new which is up in the last eight to ten years although people have started discussing it a lot in the last eight to ten years look at it way back in one thousand nine hundred and sixtyfive sixtyeight opposed something which looked very much like a modern deep neural network or a modern feed forward neural network and in many circles he is considered to be one of the founding fathers of modern deep learning so that is about the springtime for ai and what i mean by that that everyone was showing interest in that the government was funding a lot of research in ai and people really various applications health care defence and so on and then around one thousand nine hundred and sixtynine an interesting paper came out by these two gentlemen minsky and papert which essentially outlined some limitations of the perceptron model and we will talk about these limitations later on in the course in the second or third lecture but for now i will not get into a details of that but what it is said that it is possible that a perceptron cannot handle some very simple functions also so you are trying to make the perceptron learn some very complex functions b ecause the way we decide how to watch a movie is a very complex function of the inputs that we considered but even a simple function like xor o r is something which a perceptron cannot be used to model that is what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and lot of government funding actually subsided after one thousand nine hundred and sixtynine all the way to one thousand nine hundred and eightysix actually this was the ai winter of connectionism so there was very little interest in connectionist ai so there are two types of ai one is symbolic ai and the other is connectionist ai so whatever we are going to study in this course about neural networks and all that probably falls in connectionist ai paradigm and there was no interest in this and people i mean hard to get funding and so on for these seventeen to eighteen years and that was largely triggered by this study that was done by minsky and papert and interestingly they were also often misquoted and what they had actually said in that papers so they had said a single perceptron cannot do it t hey in fact said that a multi layer network of perceptrons can do it but no one focused on the second part that a multilayer network of perceptron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on right so that is what happened for a long time and this known as the winter the first winter then around one thousand nine hundred and eightysix actually came this algorithm which is known as back propagation again this is an algorithm which we are going to cover in a lot of detail in the course in the fourth or fiveth lecture and this algorithm actually enables to train a deep neural network right so deep network of neurons is something that you can train using this algorithm now this algorithm was actually popularized by at rumelhart and others in one thousand nine hundred and eightysix but it is not completely discovered by them this was also around in various other fields so it was there in i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rumelhart other and others in one thousand nine hundred and eightysix were the first to kind of popularize it in the context of deep neural networks and this was a very important discovery because even today all the neural network so most of them are trained using back propagation right and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network right so something this was discovered almost thirty years back is still primarily used for training deep neural networks that is why this was a very important paper or breakthrough at that time and aroundthesametime so againinterestingly so backpropagationisusedin conjunction with something known as gradient descent which was again discovered way back in one thousand eight hundred and fortyseven by cauchy and he was interested in using this to compute the orbit of heavenly bodies that is something that people care about at that time today of course we use it for variousotherpurposesone of thembeing discoveringcatsandvideosor evenfor medical imaging or for describing whether certain have of cancer is being depicted in a xray or things like that there is a lot of other purposes for which deep neural networks enhance and hence back propagation gradient descent and other things are being used for it but again these are not very modern discoveries these are dated way back thirty years and even gradient descent is almost one hundred and fifty years and so on so that is what i wanted to emphasize and around the same time in one thousand nine hundred and ninety or one thousand nine hundred and eightynine there is this another interesting theorem which was proved which is known as the universal approximation theorem and this is again something that we will cover in the course in the third lecture or something like where we will talk about the power of a deep neural network so again the importance of this or why this theorem was important will become clear later and when we cover it in detail but for now it is important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision so what it means in very layman terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many laymen terms that is what it means and if i have to hype it up a bit or i have to say it in a very enthused and excited manner i would say that basically it says that deed neural networks can be used for solving all kinds of machine learning problems and that is roughly what it says but with a pinch of salt and a lot of caveats but that is what it means at least in the context of this course sothisisallaroundone thousand nine hundred and eightynineanddespitethishappeningsomeimportantdiscoveries towardsthelateendofeightyswhichwasbackpropagationuniversalapproximation theorem people were still not being able to use deep neural networks for really solving large practical problems and a few challenges there was of course the compute power at that time was not at a level where it could support deep neural networks we do not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural network so when people try it training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation off the shelf at that time because of which it was not very successful so again despite these slight boom around eightysix to ninety where some important discoveries were made and even follow up in ninetytwo ninetythree and so on t here is still n ot a real big hype around deep neural networks or artificial neural networks and at time again a slump a slow winter right up till two thousand and six"}
{"audio_filepath": "Data/Preprocessed/PCA : Interpretation 3_47.wav", "duration": 197.0, "text": "now y ou go to the third interpretation where we will try to say something ab out the variance so we started off with the following the wish list that we wanted low covariance and we wanted high variance  so far we have paid attention to the covariance because everything was revolving around this covarian ce matrix in both the solutions  but what about variance have we achieved the goal with respect to high variance refer slide time zeroforty so let us see  so what is the i th dimension of the transform ed data it i s this you take your data and project it onto the i th dimension right  so x hat is equal to x into pi now what is the variance along this dimension how do you compute the variance so this is my projected data and let me just call it x hat i so this is the i th column after projection i s that fine everyone is ok with this  n ow for this i t h column i want to compute the variance how will i do that  r emember that the data is zero mean what is the formula actually  i t i s going to be x hat i minus mu i into x hat i minus mu i right  but mu i is zero  so it just turns out to be the dot product dot product of x i hat with itself ok and of course divided by m is this fine refer slide time onethirtyfour i can write this as x p i and then when i take the transpose i will get this ok n ow what is th is quantity  t his is exactly the moment where i feel like saying fl what is this quantity s tudent refer time two zero n o look at the circle what is x transpose x times p i s tudent refer time two eight what is p i with respec t to x transpose x s tud ent e igenvector eigenvector e igenvector s o what is this product going to be s tudent l ambda refer time two fourteen l ambda i p i is that fine what is p i transpose p i s tudent one one ok so what is actually the variance alo ng the i th dimension s tudent l a mbda refer time two twentysix w hat is lambda i s tudent e igenvalue so what will happen if i retain the highest eigenvalues student refer time twothirtythree i wi ll get the highest variance dimensions right fine  so all roads lead to s tudent refer time two thirtynine e igen vectors eigen values right so andrew ng in one of his lecture says that there are ten different interpretations of pca i only know three of these i do n o t know the remaining seven m aybe he was bluffing s o that people like us can keep busy oh this is getting recorded so yeah so you get this  so we have satisfied everything in our wish list variance covariance and also did this detour where we saw that it actually amounts to minimizing the error in reconstruction where we are throwing away the dime nsions along which reconstruction did not add much to our knowledge about the data  so these are the three different interpretations that i have right  so hence we did the right thing by throwing away those dimensions which correspond to the lowest eig envalues because lowest eigenvalues is nothing the lowest variance also refer slide time threetwentyfour so this is the quick summary the covariance between the new dimensions you can leave actually those you can just read it later on"}
{"audio_filepath": "Data/Preprocessed/A quick recap of training deep neural networks_69.wav", "duration": 336.0, "text": "welcome to lecture nine of csseven thousand and fifteen today we wil l talk about greedy layer wise pr e training bette r activati on functi ons bette r weig ht initializati on methods and batch normalization s o today s lecture is more like ti ps and tric ks to make dee p learning work so whenyou are actuallyexperimentingwith deeplearningin practice whatare some of the things that you need to take keep in mind and it is also my way of connecting the history that we saw towhere we are todayright so there werecertain things which we saw in thehistoryand now i willtry to bringthose backand connectto wherewe are heade d from here rightwhere we have reache d today a nd where we are heade d from here so that is with that i n module one i will do a very quick reca p of traini ng neura l networks and not take more than five minutes a nd i n eed it for a specific purpose so we already saw how to train such a very shallow neural network what was the learnin g algorithm gradient descent and this was the update rule right in particular i wanted you to notice that the gradient actually depends on the input so when you compute the gradient formula you have this multiplication by x so it is proportional to t he input and this is one fact that we will use it at least a couple of c ases in the lecture today so this was a very shallow single neuron network what if we have a wider network still which algorithm student gradient descent refer time oneforty g radient descent ok a nd we just have these three different formulae and for each of these formulae note that the gradient or rather this gradient depends on the input that you are feeding in ok i did not keep this in mind and what if you have a deeper network so we saw a very shallow network we saw a wide network and i am showing you a deep network what will you do again gradient descent but you will apply the chain rule for computing the gradients and again here in general y ou will notice that for any of these weights wone wtwo w three the gr adient formula will have this h i minus one what is h i minus one student refer time twoseventeen input from the previous layer right and h zero is the actual input so the gradient at any layer is actually proportional to the input from the previous layer and this could either be the input from the hidden layer or the actual input and finally we saw this thin so we saw a wide network we saw a thin network now we wil l see a wide network and a deep network right sorry we saw earlier we saw a wide network and a deep network now we see a wide and deep network and here again you have compute the gradient by applying this chain rule across multiple paths and that is wha t we use and we call it back propagation and remember again they are the same thing holds that the gradients at some point are proportional to the input at that layer everyone remembers that ok so this is important so what we have is things to remember from what we have seen so far is that so  training neural networks is basically a game of gradients right so you compute the gradients and everything depends on those how will you update the weights and everything from the re on is about the gradients and these gradients actually tell you the responsibility of the parameters towards the loss and you appropriately update them and we saw a variant way different sorry various variants of how to use the gradient so we saw t he gradient descent we saw nag momentum and all but in all of these the underlying core thing was to compute the gradient and then do some manipulations based on that and the other key thing is that the gradient at a particular layer depends on the inp ut to that layer ok so now let us go back and just retrospect a better and see what is it that we have learned so far so so far what i have taught you gradient descent oh sorry back propagation is something which was propos ed way back in one thousand nine hundred and eightysix right so in fact it was existing before that but it was popularized by this work of rumelhart and others in one thousand nine hundred and eightysix right so but then in the one thousand nine hundred and ninetys or early two thousand if back propagation already existed and we could train deep neural netwo rks then why did not we here so much about deep learning at that time of course you guys were busy with school and all at that time but why did the others or older people like me not hear about it student computational power computational power is t hat the only thing student refer time four thirtyfive computation and memory is are the only thing student convergence who said convergence ok good so actually what happened right in the late eightys and early ninetys and even early two thousand when you used back propa gation to train really deep networks it was not very successful and what do i mean by not successful actually what are the two things that could happen someone gave the answer already student refer time fivezero it does not converge right that means you do not reach the optimum solution right in fact till two thousand and six it was very hard to train very deep networks and typically even a after a large number of epochs these networks did not converge that means they were still at a very high loss and although in principle everything is fine you have a deep neural network you have an algorithm that can train it but you are still not being able to train it properly and you are not being able to make any practical use of that so that was the story till two thousand and six so today is about what happened in two thousand and six what it led to in the next few years and th en where we are currently right so that is the journey that we need to make ok and that is why we started off with this quick recap of back propagation because that is what i want to tell you that why did it not work earlier and where are we today"}
{"audio_filepath": "Data/Preprocessed/Deep Learning(CS7015): Learning Parameters: Gradient Descent_21.wav", "duration": 1836.0, "text": "in this module we will talk about gradient descent refer slide t ime zerotwentytwo so what we want to do is find a more efficient and principled way of navigating the error surface refer slide time zerothirty a nd the goal is to find a better way of doing this refer slide time zerothirtyfive so let u s start by setting up t hings we will define some notations and some parameters and so on and from there on we will try to come to the algorithm ok  so my parameters in this case were w co mma b what i am going to do is i am going to put them into an array or a vector right an d call that vector as theta  so theta is the vector of parameters and theta belongs to r r what r two right there are two parameters here  so it i s a two dimensional vector refer slide time oneeight now what i want is again what i will do is i do n ot know what the value of w comma b is  so i started with a random guess  so that is always going to be my starting point i will always start with a random guess and from there on move on to good values n ow once i have started with a random guess i wan t you to tell me some changes that i can make to w and b  so that i land up in better situations right that means i land up in situations where the error is less is that fine so that change in w and b i am going to call it as delta w and delta b and that again is a vector which is storing these two values  so this is the picture right i want to take theta and i want to add a small change to it  so this is my theta this vector is actually theta right this is the theta vector i want to add a small c hange to i t which is again a vector this is delta w comma delta b such that i will get a new value for theta new  so theta new would be what actually theta new is equal to w new comma b new is that fine that is wha t theta new means refer slide time zero twoseventeen now what has happened is actually when i have added delta theta to theta i have moved in the direction of delta t heta i have come from here to here n ow i am going to be a bit conservative and i am going to say that while i am ok in moving in th e direction of delta theta i do not want to make a giant stride what i will do is i will just move by a small quantity in that direction so this delta theta is this large magnitude  so all i am saying is that i will not i move in that direction i am fin e with that  but i do not want to make a giant stride i will just take a small stride in that direction  so eta is a scalar which actually scales down delta theta so now if i am going to take only a small step in that direction instead of this large ch ange i will just get a smaller change theta new so red the red vector is actually going to be the movement which i make that is the new value of theta  so theta new is equal to the original theta plus a small step in the direction of delta theta so e verything is clear y ou are done we are done with gradient descent w hat is missing w hat is delta theta right i am telling you i want to move in a certain direction but what is the right delta theta to use how many of you know the answer to this w ha t is the answer m ove in the direc tion opposite to the gradient why w here does that answer come from n ot the ml class folks how many of you know why we need to move in the direction opposite to the gradient why ok w e will see ok  so that is the q uestion that we need to answer i f i give you an answer to this question then what is it i am doing i am giving you a principled way such that you start from a random value of theta move in certain direction and you will ensure that your loss has decreased and then you have to keep doing this right  so that is the set up and the answer t o this comes from taylor series refer slide time foureleven so now what i am going to do is i am going to give you the right direction delta theta fine and for ease of n otation i am going to call it as u  so remember what this delta theta is what is it c hange in w comma change in b  so it i s a vector in r two remember that ok i am just going to call it as u n ow this is what taylor series tells me what it tells me is that if i am at a certain value of theta and if i want to change that value a bit then what is going to be the new value of the loss function or any function for that point and this is the formula for that ok n ow what is let u s see what are some quanti ties here what is this quantity scalar vector matrix s calar  t his v ector we just did that right it i s a vector w hat about this w hat is this quantity  actually gradient  w hat is the gradient w hat is the gradient no you are telling me how to use t he gradient i am asking you what is the gradient you are giving me absolutely correct and absolutely useless definitions that is a very good answer ok so now what i am going to do is i am going to digress a bit and i am going to tell you something ab out derivatives partial derivatives and gradients and then we will come back to this ok  so now suppose you have a function l this is l in my handwriting this function of w and say this function is w square ok now what is what is this called a der ivative of the function with respect to w this is the derivative and you know this is two w ok n ow sup pose i have a function b square now what is this quantity is a partial derivative of the function with respect to w w h y partial because it i s consid ering b as a constant and taking the derivative with respect to only one of the variables right this happens to be and what is this quantity oh sorry  so is w comma b right this is the partial derivative with respect to b ok now can you tell me what is a gradient t he gradient is nothing but it i s just these two partial derivatives taken together and put into a vector right now suppose i had a function which depended on hundred variables what would the gradient be size of the gradient r one hundred it wo uld lie it would be a hundred dimensional case ok  so n ow can you tell me with this evidence in knowledge  but this primer can you tell me what this is t his is a gradient vector which is right there in front of you in a red ink this is what it is r ight fine everyone ok with that so actually the right way to write this and probably we need to correct in the slides would be theta  so remember that theta is equal to w comma b  so this is the derivative of l theta with respect to theta which is no thing  but the collection of the partial derivatives with respect to the components of theta is it fine  so everybody understands what is a derivative partial derivative and gradient ok fine  so now the gradient is a vector in this case fine ok re fer slide time eighttwentyfive so now what is this quantity i t is a n o it i s what is this t he dot product between these two vectors ok fine n ow one last thing and many more things actually  so what is this square of the gradient t his is not the square of the gradient w hat is this hessian fine everyone knows the textbook what said can you tell me what does it i s a scalar vector matrix m atrix  w hat is the size of this matrix two by two  w hat are the elements of this matrix s econd order partial deriv atives right  so it i s the gradient of the gradient right is that fine  so what does that mean you had thi s gradient this is the gradient now you want to take the gradient of this again with respect to w comma b right that is what this means it i s a gradient of the gradient right  so what that means is we will take the gradient of the first quantity again with respect to w  so that would be dou square by dou w what would this quantity be w hat would this be i s that fine and you can fill in th is quantity right so now it i s clear what the hessian is i t i s the derivative of the derivative and it would be a matrix ok is that clear to everyone  so i have a habit of doing a lot of these basic stuff i know that the top twenty percent of the clas s gets really pissed off when i do this  but as a philosophy i teach for the bottom thirty percent of the class so i do not mind that and the other thing is i use slides  so i do not write a lot of math so i can cover a lot of material despite doing all th is basic stuff right  so i am going to stick to that what i am trying to say is that write this in the feedback that you do not like this basic stuff  but it i s just that i am going to ignore that feedback i mean just being honest right  so i like doi ng this because it just takes me ten minutes to do this and for the rest of the class i do not have to look at blank faces afterwards right  so it really helps me a lot fine  so is that all clear all the quantities here are clear so now so this is th e gradient this is the hessian and now eta remember what did we say about eta i t i s a small quantity and what do we do with small quantities always in math s w e ignore them  so once we take their powers you are always ignore them w hether it i s correct or not who cares i mean someone has told it  it is good to ignore  so we will ignore it right  so now all these higher order terms we can ignore right  that means i will only consider this fine so let u s again look at what the setup is t he setup i s that i have some value of theta i want to move away from that value such that what do you say about this loss compared to this loss i will call this the new loss and i will call this the old loss what is the relation between them t he new loss should b e l ess than so if i or someone gives you a u i am not getting ok someone gives you this u then what does what when would you say it is a good u refer slide time twelvetwentyeight i f this condition holds everyone agrees with that right  so i have f ound a goo d direction to move in if this condition holds now this condition actually implies that this condition should hold right this is l theta plus eta u right so if i just do minus here i get this right  so this quantity which should be less than equal t o zero implies that this quantity should be less than equal to zero and remember eta is a positive constant ok why cannot it be negative w hy b ecause you wanted to take a small step in that direction if we make it negative we will do what w e will reverse t he direction we do not want that as of now right so eta is that for a positive quantity s o that means this quantity should be less than zero is it fine with everyone refer slide time thirteentwentynine so so far after all this story what we are left with is this condition should hold for the u that i am trying to choose so that i can be sure that i have chosen the correct u right and the definition of correct u is that the loss at the previous step the loss of the new step should be less than the loss at t he previous step is that fine so that is what we have arrived at now what is the range of this quantity that is why i asked you what is this t his is a d ot product i will leave it at that  so now you tell me what is the range of this people from the ml class cannot answer d id i cover this in the m l class n o o k fine what is the range of this n ot a very hard question p lus or minus s tudent  refer time fourteentwentyfive mod of u  refer time fourteentwentysix v ery good  h ow many of you understood that answer h e said plus or minus mod of u into mod of gradient the gradient vector right why is it so easy l et beta be the angle between u t and this between sorry it should not be u transpose between u and the gradient then we know that this condition holds cos b eta is given by this quantity and we know that cos beta lies between minus one and one ok n ow if i just say that this quantity is equal to k then i can just get this condition n ow let u s see what are we trying to do w e are trying to find a u such that th is quantity is negative  w e are trying to find the use such that this quantity is negative now i just stop at negative we would like to make it as negative as possible right because the more than negative it is the more will be the decrease in my loss fu nction r ight because this quantity tell me tells me how much my loss decreases  so the more the negative it is the more the loss will decrease  so let me make it as negative as possible now what is that value w hen will that happen when alpha is you k now the answer you started with the answer s tudent  refer time fifteenfiftyfour n o what is that one phrase which you have marked up move in the direction s tudent  refer time sixteenseven ok n ow think of that s tudent  refer time sixteennine w hat would happen when this is the most negative i t can be what would the angle be s tudent refer time sixteennineteen one hundred and eighty degrees  h ow many of you get that because when this is the most negative  that means the cos beta is actually minus one and when is cos beta minus one when the an gle is one hundred and eighty degrees  that means u should be such that it is at one hundred and eighty degrees to the gradient hence repeat the phrase s tudent  refer time sixteenfortytwo m ove in a direction opposite to the gradient i s that fine e veryone gets it now w hy you need to move in the direction opposite to the gradient refer slide time sixteenfiftythree so this is what the gradient descent rule is you are at a particular value of theta you want to move to a new value of theta such that your new loss is less than the current loss what grad ient descent tells you is move in a direction opposite to the gradient  so are you fine with this now with gradients i have come to scalars  but i will just explain what i have written here so this quantity is nothing  but theta t plus one right is eq ual to theta t right and what is this right  so the new theta is equal to the current theta minus why b ecause we want to move in the direction opposite  so it is basically theta t plus one is equal to theta t plus eta into a negative directi on right th e direction negative to the gradient hence you get that minus one now what are these quantities let me just take that carefully  so this quantity is gradient of the loss function with respect to w sorry the partial derivative of the loss function with re spect to w evaluated at w is equal to w t and b equal to b t what does that mean so remember when you are dealing with derivatives as always a formula and then a value add that at a particular value  so what is the derivative of x square with respect to what does not matter two x so derivative of x square with respect to x is two x what is the value of this derivative at x equal to one two right  so you see the difference you have a formula which is two x now you substitute in a particular value and you get the value at that particular value ok  so that is what this means because you are already at w t comma b t now you cannot subtract a formula from here r ight you have to put subtract a value  so you know what the formula is you plug in the values of w t com ma bt get that value and subtract it from your current wt is that fine so everyone completely understands what is the gradient descent rule is fine refer slide time nineteenfourteen so now we have a more principled way of moving in the w b plane w hat do i mean by that remember this was our w b plane this was our error this is something what our error surface looked like it was this flying carpet i was randomly moving on the w b plane earlier right and trying to guess what the errors or trying to comp ute the error and then settle for a particular value n ow i have a more principled way of moving in the w b plane i know what is the next step based on the current step i just need to move in the direction opposite to the gradient so let u s try to  so this is what it tells me for one step  but i need to keep doing this till what is that golden word s tudent  refer time nineteen fiftyeight c onvergence right i have to keep doing this till convergence ok refer slide time twentytwo so let u s create an algorit hm out of this rule i will start a time step zero i will do this for some max iterations instead of saying till convergence i will do it for some iterations a t every iteration i will this is how i will update my weights i will take the current weights su btract the gradient from that and get the new weights i mean not subtract the gradient subtract this quantity and get the new weights  so now is everything clear i s the gradient descent algorithm done can you do it for the toy net work which i had i s t here something still missing s tudent  refer time twentythirtyeight eta is fine we will take a small value zeroone or something a ctually not told you what these are right i means to write it you know these are derivatives  but what is this actually ok  so let u s see that now  so that is what we are going to see next refer slide time twentyonezero so now we want to find out we are in the car quest is for this delta sorry the partial derivative with respect to w and partial derivative with respect to b that is the thing which we had plugged in the formula  but we do not know what that is right  so we need to find that out  so now for simplicity let u s assume there is only one point of it which is x comma y  so earlier we had this x one y one and x two y two n ow i am jus t assuming there is only one point which is x comma y refer slide time twentyonefifty so  n ow what is a loss function earlier i had this summation over i equal to one to two  but i have just one x comma y  so i will just use that this is what my loss function a nd what are the quantities that i am interested in finding o ne is this the partial derivative of this loss function with respect to w refer slide time twentytwoeleven so let u s do this lets actually derive this so this is what it looks like now you have to help me in deriving this what will i do first s tudent  refer time twentytwoeighteen t ell me the next step s tudent  refer time twentytwotwentythree two into f of x minus y and push the gradient inside of course the derivative is that fine a nyone who has a problem with this next y is a constant this is the true i remember so that is why this is a constant is not the predicted y now this quantity what is f of x actually s tudent  refer time twentytwofiftyone s igmoid function right  so i will just write it n ow this is the qu antity that i need the derivative for  so i will just write it here what is the next step t his is of the form one over x  so what will it be s tudent  refer time twentythreefour m inus one over x square and then you put the derivative and say it i s that fine n o w the quantity inside is of the form e raise to x  so the derivative is e raise to x and you push it inside is this fine so this on slide should come both these are coming together  so is that fine n ow what is this actually s tudent f of x f of x  w hat is this s tudent  refer time twentythreethirtytwo t his is actually one minus f of x just take my word for it for now you can go home and work it out right  so this actually if you do one minus this and do some trickery you will get to this quantity right  so what you get is a very simple formula f of x into one minus f of x into x i am going to substitute back here so now i exactly know what the partial derivative of w is refer slide time twentythreefiftyseven so there is only one point then this is what the partial derivative with respect to w is going to be of the loss function right if there were two points what would happen i f there were two points my loss function was this is a sum of two elements a nd i am taking some derivative of a sum i will get a sum of derivatives right refer slide time twentyfourtwentythree so how many of you will not cringe if i say this is the answer a nyone who has a problem with this y ou get this how many if you do not get this h ow many of you get this g ood fine n ow can you do a sim ilar thing for b c an you tell me the answer without actually deriving it s tudent refer time twentyfourfortyone i can perfectly understand what you are saying s tudent  refer time  twentyfourfortyeight x would not be there right because this last x that you see here came be cause w into x was there  but b we are not multiplying x  so what we will get is this you can go home and check refer slide time twentyfivetwelve so now we have everything that we need n ow we actually have everything t hat we need ok no more trick questio ns  so now we will write code to do this ok we will actually implement the code and see what happens  so these are the two data points that i had zerofive comma zerotwo and twofive comma zeronine the first thing which i need is something which can implement the sigmoi d function  so this is one over one plus e raise to minus w x plus b is that fine now i need something which can compute the error  so this is summation of half into f of x minus y the whole square i go over all the data points summation of half into f of x minus y the whole square is that fine n ow what i will do is i will take this try out a lot of values of w comma b and plot the error surface ok  but this is only for illustration in practice i will not do this w e just kno w that this error surface exist i jus t want to verify that whatever algorithm i come up with does not efficient navigation of this error surface that is what i want to verify that is why i am plotting this n ext time you need a function which can compute grad of b w e just saw t his on the previous slide this is f of x minus y into f of x into one minus f of x right simple e veryone is fine with this t hen i need a function which can compute the grad with respect to w same thing except that i have this x at the end so i have all the ingredients in place now what would i do what is the next thing that i will write t he main loop right i will write the main loop now so this is what the main loop look like looks like i start with some random initialize for w comma b r emember that our initial theta which is composed of w comma b is going to be some random guess  so i started with the random guess which is minus two comma two i have chosen eta to be one  that means i am not going to be conservative i am going to move in the direct ion of the gradient i f i chosen at zeroone and zeroone i would have been conservative and i am going to run this till one thousand epo chs which is my notion of conversions now in each epoch what i am doing is for every data point so remember that this gradient wit h respect to w was a summation of i equal to one to two and that formula right so for each data point i am computing the grad adding it right  so that is the summation part similar thing i am doing for b once i have computed the gradient which is the sum mation quantity i am just moving in the direction of the gradient is that fine everyone understands the code it is simple python code and it does exactly what i had shown in the pseudo code now let u s execute this code and see what happens  so i will start with my random point which was minus two comma two and now i am going to actually run this code and keep plotting what happens on the figure  so just pay attention fine  so now here i s how the code is running see what is happening what is happenin g actually so at every point i am changing my w so that i am moving in the direction of the gradient i keep doing that as i keep doing that my error keeps decreasing why b ecause that is exactly what we got from taylor series that if we do this the err or is bound to decrease right and then we keep doing this and after a few iterations we will actually reach almost the value which is the zero error right and this same thing would happen if you start from anywhere else it will keep moving in a principle d way and reach the low error configuration now some of you would say that maybe this was the shortest path right it could have just rolled over from there  but that is not a principled way of doing that right we the principled way of doing it is to mo ve in the direction of the gradient you might take a longer route  but reach your destination taking shortcuts is always risky in life as well as here  so so  do not please this is an advice for error assignments and so on  so this is the more princip led way and we will reach the solution  so that is what is happening here  so we have actually derived everything that we needed and this is all you need to write for gradient descent fo r this toy example that you had now answer this question n ow sup pose i had hundred such variables instead of w comma b i had hundred such variables what would happen y ou do not have to visualize it s tudent  refer time twentyninefortyeight i n terms of the code s tudent  refer time twentyninefiftytwo i will just need to have these func tions for all of those i will have to calculate it by hand but still doable it is just a lot of tedious work of course later on we will see a more refined way of doing this where we can do a lot of these computations at one go  so we can directly start operating in vectors as opposed to scalars here i am treating w and b separately here i could have actually had a function which tells me grad of theta directly right a nd later on we will see something like this ok  but for now the code is still runnin g here refer slide time thirtytwentyfour now it suffices  so later on we will see gradient descent in more detail in the course and we will also see a lot of variants of gradient descent  but for now it suffices that we have an algorithm which can learn the p arameters of a sigmoid neuron  so just as we had the perceptron learning algorithm we have the gradient descent learning algorithm which can help us learn the parameters of the sigmoid neurons starting from random values a nd it gives a principled approa ch for doing that"}
{"audio_filepath": "Data/Preprocessed/Deep Learning(CS7015): Representation Power of a Network of Perceptrons_17.wav", "duration": 765.0, "text": "we will go to the next module where we talk about a network of perceptrons and then we talk abou t the representatio n power of a networ k of perceptronsso this module shoul d have beentitled as networkof perceptronsso nowin particular whatwe are going to see is how any booleanfunctio n whether linearl y separabl e or not can be represented using a network of perceptrons now what do i mean by represented during a network of perceptrons what it means is that i will give you a network of perceptrons you take any boolean function feed any value of xone to xn and the network will give you the same y as it is expected from the truth table ok that is what representation means just to put it out clearly and now i am going to again do a setup i am not giving you the solution i am just making some set up and then we will discuss the solution so for this discussion we will assume that true equal to plus one and false equal to minus one so instead of zero and one we will assume minus one and plus one and these are your inputs xone and xtwo we are taking the two input case and i will have four perceptrons first i will have four perceptrons and i will also have very specific weights connected to form the inputs to these perceptrons so red means minus one and blue means plus one right so the first two inputs are connected with a weight of minus one the next two inputs with minus one plus one plus one minus one and the last would be plus one now once i have this i will set the bias of all these perceptrons to minus two so that will that means they will fire only if they are weighted sum of the inputs is greater than two now after this i will have one more perceptron so i had two inputs i converted that to four values these four values are now going to feed into one more perceptron and these weights i will not fix them these are the weights that i am going to learn ok these and the final output of this perceptron which is the green perceptron is the output of the network right so now coming back to what i said that it can represent any function what i mean is that you take any function feed in any combination of xone xtwo this network will give you y and i am telling you that it will match the truth table of that function now let us define some terminology this will also stay with us for the rest of the course so this network contains three layersthe layer containing the inputs it is called the input layer very creative the middle layer containing the four perceptrons is called the hidden layer and the output layer which gives you the output is called the output layer output perceptron which gives you the output is called the output layer right so you have a input layer a hidden layer and an output layer and the outputs of the four perceptrons i am going to call them as hone htwo hthree and hfour and the red and blue edges are called the weights for the layer one which we have not learned we have actually set them by hand and the weights for wone wtwo wthree wfour are called the weights for the second layer other the layer two weights and these are the weights that we want to learn now i make this claim that thisnetwork it can take any boolean function it can implement any boolean function so this same network can implement any boolean function that means if i take this network and if i try to learn the values of wone wtwo wthree wfour for any boolean function whether it was originally linearly separable or not i will be able to implement it so isnt thisan astonishing claimany boolean function do you thinkthis isan astonishing claim well not really if you really understand what is happening here right so let us see what exactly is happening here so when will perceptron one fire when the input is false false zero zero will it fire for any other input when will perceptron two fire any other input same for the next perceptron same for the next perceptron so you start getting an intuition of what is happening here you do ok let us see so now for this particular network now that i have given you some intuition of what is happening basically every node or every neuron in the hidden layer is catering to one of the inputs and it will fire only for that input it will not fire for anyone else so now let us try to implement the xor function and see what are the so now let us try to implement the xor function and see what are the set of inequalities that result from this earlier when we try to look at the set of inequalities we ended up with a contradiction let us see if that happens now so this is xone xtwo this is your xor function so that is just like any truth table then i am noting down the intermediate values and then my final input to the green perceptron is going to be summation of these and it will fire if this summation is greater than equal to zero or else it will not fire now for the first case when the input is zero comma zero what is hone going to be one and everything else is going to be zero that is exactly what we saw in the previous slide so what is the summation going to be just wone right so it is wone hone plus wtwo htwo so on but htwo to hfour are zero so only thing that remains is wone for the second case wtwo for the third case wthree for the fourth case wfour so is it clear now what is happening let us go a bit more into detail right so now for the xor condition what are the conditions that we need wone should be less than wzero because this should not fire wtwo should be greater than equal to zero wthree should be greater than equal to sorry w naught not w is not zero and wfour should not fire so wfour should be less than wzero are there any contradictions here by design no right so we have made sure that for the final layer only one of these guys feeds to it so it does not matter what the remaining outputs are they do not interfere with each other unlike earlier where we had conditions like wone should be something wtwo should be something and then wone plus wtwo should be something there are no such contradictions here because we have made sure that every neuron in the middle layer actually caters to one specific input and now the weights in the final layer can be adjusted so that we get the desired output for that input so i can set whatever value of wone i need to set so that i can fire the neuron in fact i could just fix wzero as zero and then i can adjust the weights of wone wtwo wthree wfour and i can implement the xor function so are you convinced that this can be used to implement any boolean function how many if you are not convinced so the negative question never works how many if you are convinced sure now what if we had three inputs before that it should be clear that the same network can be used for any of the remaining fifteen functions and for each of these functions we will end up with a different value of wone wtwo wthree wfour but you will be able to satisfy the truth table right and you can go home and try it which i am sure you will do ah so what if we have a function of three inputs two hundred and fiftysix what is two hundred and fiftysix no eight fine sure so this is what it will look like and anything specific about the weights of the initial layer can you tell me what the weights would be just tell me red red red red blue blue whatever colours you like this thing first perceptron what would the weight colours be red red red then enough so this is how it will look right right and now this same thing will work with the same logic for any boolean function of three inputs you will get these eight inequalities and they will not interfere with each other and you can set the values of wone to weight so that you can satisfy it ok fine so what if we have n inputs two power n perceptrons in the middle layer right ok so now here is the theorem any boolean function of n inputs can be represented exactly by a network of perceptrons containing one hidden layer with two raised to n perceptrons and one output layer containing one perceptron we just saw an informal proof of this we just constructed i just gave you the answer it this is how you will get it now note that a network of two raised to n plusone perceptronisitsufficientor necessary or both sufficient yes that is what it says is it necessary no we already saw the andfunctionwhich we can just representusing a single perceptron right so it is not necessary but it is sufficient so this is a very powerful theorem if you think of it right so now this whole objection right remember this history and when we have the c i winter when people showed that perceptron cannot handle the xor function that is for a single perceptron if you have a network of perceptrons you can actually have any boolean function but whatisthecatchasthevalueofnincreasesthenumberofneuronsincreases exponentially right but still in theory you could have a solution now again why do we care about boolean functions i keep coming back to this why do we care about boolean functions because you took this and so the question that i the question that i want to answer is how does this relate back to our original problem right we know any boolean function can be implemented how do we go back to our original problem which is whether we like a movie or not right and you could see that there is a whole family of problems there right whether we like a movie or not whether we like a product or not whether i want to go home today or not yes no any kind of a yes no problem it is a whole family of problems there so let us see so we are given this data from our past experience right so we are told that this is what the movie looks like these are the actors directors joiners everything we also know whether we like these or not so we have a set of positive points and we have a set of negative points right and now we want to have a computational model which can satisfy this data that means once the model is trained once whatever algorithm i algorithm i use has converged it should be able to give me the correct output for a given input that is what we are interested in and that is a real classification problem that we are interested in now for each movie we are given these factors as well as the decision and i said pis and nis are positive and negative points the data may or may not be linearly separable it is not necessary that the data is linearly separable those were the goody cases it but in general that may not happen but do we worry about it now no what the previous theorem told us is that irrespective of whether your data is linearly separable or not i can give you a network which will be able to solve this problem modulo that it might be very expensive in the number of neurons in the middle layer but if you keep that aside i have a solution for this and that is why we care about boolean functions because many problems we could actually cast to it in a simplistic way if we ignore the real inputs and if you even think of the real inputs suppose it could take all values between zero to one you can always make it binary you could say that is the value between zero and zeroone is the value between zeroone and zerotwo and you could make it as small the scale as small as possible right so that is why we care about this so the story so far has been that the network of networks of the form that we just saw it which contain one input layer output layer and one or more hidden layers these are known as multilayer perceptrons but a more appropriate terminology would be multi layered network of perceptrons because the perceptron is not multilayered you have a network of perceptrons and that network has many layers right but generally there is abuse of notation we always call it mlp which is multilayered perceptrons and the theorem that we just saw gives us the representation power of an mlp and basically tells us that it can represent any boolean function that we want to deal with so that is where we will end this class and in the next class we will talk about sigmoid neurons"}
{"audio_filepath": "Data/Preprocessed/Attention over images_116.wav", "duration": 646.0, "text": "and so now in thi s lecture we will g o on to the nex t modu le whic h is talking about attention over image s so let us fir st motivate why is it so differe nt and wha t could be done there right so the question is how do we model an attention mechanism for images so in the case of text we have a representation for every location of the input sequence right so every location in the input sequence in the case of text was a word and then you are looking at this problem of transliteration every location was a character and whether it is a charac ter or a word for everything it was discrete so we could just know that this is the important time step t and then we know that along all the inputs at different time steps you want to pay attention to certain time steps right so that the definition they was very straightforward right now for images what do we do so for images we typically take the representation from a cnn right it could be fc seven or any of the convolution layers or max pooling layers right now there is no concept of time step there right because the entire image is given to you at one go so now how do you decide where to pay attention to but if you think about it does makes sense at least the motivation is very clear so for example for this figu re if i am trying to generate the description as man throwing a frisbee in a garden or in a park or something like that right so when i am generating a word man i would want to focus only on the man and not focus on any other part of the image simila rly when i am generating the word frisbee i would like to focus on this may be when i am saying throwing i would like to focus on his hand action or something like that and in a park i would like to focus on the background and so on so it does make se nse that each word in the description is complete covering from coming from a different space in the image or different position in the image but the representation that we use say the fc seven representation that doesnot contain any location information it just a flat and vector that we had so now how do we do this how do we get attention on locations is is a motivation and the problem clear and motivation is straight forward the problem is that we are using fc seven representation is just a flat vector remember that was the fully connected vector and does not have any location based encoding so if for example if the fully connected vector is of size five hundred and twelve i cannot say that the first twelve or first twentyfour of these five hundred and twelve dimensions correspond to this set of pixels the next twentyfour corresponds to this set of pixels and so on right so that is the problem how do i what do i attend to how do i decide where to attend to because that is what i am saying that the vector the elements of the vector or the dimension of the vectors do not have any semantic right that is not that the first dimension corresponds to first location what you want an attention on this locations in the image right but the first dimension in the vector fc seven vector does not correspond to any specif ic location in the image you know that was a fully connected vector right so it corresponds to everything in the image so what something simpler than that why do i say something simpler than that object detection is itself is a in itself is a another convolution neural network which does this and so on right and we saw this past or seen in past class seen in problems now let us solve the problem at hand the problem at hand is that i want i will just rephrase a problem definition so that the answe r becomes obvious i want a representation which allows to give which allows me to get some location information no but the fully connected layer if you back track it was fully connected by definition right the answer is really straight forward the pro blem only arise at the fully connected layer right because that is fully connected but what about the outputs on the convolution layers do they have position information student yeah sir yes we saw that suppose this is vg g whatever it is sixteen i guess and this is what i am saying as a problem because this was fully connected so you do not know that this dimension corresponds to location one location two and so on but if you look at the convolution layers we know that everyth ing here actually goes back to some location in the image and if i learn to pay attention to this guy maybe i am paying attention to some equivalent portion in the image does that make sense right now can you build on this intuition and tell me i want this as a solution right so in the case of words these are the word vectors that i had at every location and i was learning to pay attention to them let us learning these alphas for each of these now what is the corresponding diagram for images what is each of these box is going to be so that we learn the attention weights what do you mean by pass student refer time fourfortynine no so maybe i am not understanding your answer what is the equivalent of this this box which i have highlighted there between the image and some attention weight so you are directly going to operate on the image remember attention weights are never given to us no one will mark that man is this frisbee is this and park is this or whatever there is no supervision s ame size as a convolution what now what is the size of a convolution let us take the last one right so what do you mean by size you mean five hundred and twelve or you mean fourteen or you mean the other fourteen that is the size of the output five hundred and twelve cross fourteen cross fourteen so what do y ou mean by the size five hundred and twelve or fourteen or fourteen or five hundred and twelve cross fourteen cross fourteen channel does not make sense because channels capture i mean you do not want to focus on the red part or the green part or the blue part in some cases you might that is correct ok student refer time fivethirtynine these are all partially correct answers going in the right direction let just think a bit more and see so probably between these two they gave some part of the answer now can you think of it so you want a representation for the i mage first of all of us are clear that we do not want to work with the raw image right that is all of us are clear with that the second part is we are going to work with the convolution neural network we want to pick up a representation for the convoluti on neural network which gives us location information right and we agree that the fully connected layer does not give us the convolution layers give this ok now i am asking you to focus on one of the convolution layers which is five hundred and twelve cross fourteen cross fourteen so  let us see from there how we will be try to get these attentions ok so the output of the fiveth convolution layer or five c this is i think this whole thing is five and this is five a five b and five c right these this is how the code or the general architecture is nu mbered so this guy has fourteen cross fourteen locations right the five hundred and twelve cross fourteen cross fourteen output so it is five hundred and twelve channels but the number of locations is fourteen cross fourteen and we have seen that each of these fourteen cross fourteen locations corresponds to certain portion in the imag e right now for each of these fourteen cross fourteen locations that means i have one hundred and ninetysix such locations and for each of this how many dimensional representation do i have i want everyone to say this student five hundred and twelve five hundred and twelve because we are rea ding it from the figure no what you are taking is the one taking this pixel can you see what i am highlighting i am taking it across the depth right so that is why five hundred and twelve dimensional representation of one pixel in your output volume and how many such p ixels do you have student five hundred and twelve one hundred and ninetysix and each of these pixels corresponds to some real location in your image that means it has space information right ok so now these are the one hundred and ninetysix locations that you have now this looks very much similar to that diagr am that we had for words so now you can think about that you have one hundred and ninetysix items in your sequence and now what will you do we will learn to pay student refer time sevenfortynine so what would that look like so at every time step we will have an equation for alphas right let us try to write an abstract equation right first of all give me the indices of alpha what does alpha compute the importance of the of the j th location at the p th time step fine is that ok what should this b e a function of second part is kind of obvious so let us c all these as h one to h one hundred and ninetysix  so these are the js or the ts js or the ts student js js so what would the second parameter be student h j h j and what is the first parameter be what is the decoder in this case we are trying to generate a caption that means we are trying to generate a sequence that means what will be the decoder be student rnn rn n so what should i depend on s t or s t one student s t one s t one  s t is the current thing right that we do not know yet so it will depend on st minus one comma h j of course you can make it depended on several other things also but at the minimum you will see these two things right because you are trying to understand the importance of the se guys so these better participate in the function and you are trying to compute the importance at current time step so you better know what has happened till time step t one it is not very different from the attention equation that we had written in fact it is a same actually and what was one form of this attention that we had seen does anyone remember that we had carefully analysed the parameters and the dimensions of that form what should the output of this function be scalar vector matrix scalar right so what is the form that we had seen for this function v t tan h of something you should get comfortable in writing these equations right because that is what you will do if you are proposing your models and so on right so you will see ok ay in the previous model this depended on the following two quantities i think it should depend on four more quantities so i will write a new equation just think about it there are two inputs s t minus one and h j what will be doing with each of these inp uts do a introduce some student parameters parameters so what will you do student w s t one w s t one plus student plus u v into sorry u into student h j plus some bias right so you just comfortable with this right i mean that is all i mean whatever we have seen so first of all remember that the attention is a feed forward neural network the moment i tell you that you should know that it will have a linear transformation followed by a non linearity right so that should have been very cle ar that it would have a linear transformation followed by a non linearity ok and this is the non linearity and then you have this other constraint that you want the output to be a scalar that is why you had this guy which was a vector multiplied by a vec tor which gives you a scalar everyone is comfortable with this right so we see at the attention over images is not very different from attention over sequences it is more or less the same once you figure out what is the correct representation to you s o that you get the space information after that it is straightforward right ok so that ends the module"}
{"audio_filepath": "Data/Preprocessed/The problem of Exploding and Vanishing Gradients_106.wav", "duration": 619.0, "text": "and that takes us to the problem of vanishing and exploding gradients ok s o you want to see wha t is a problem with thi s bac k propagat ion throug h time which c ould lea d to certain interesting situations so we will focus on this   s t   s k and l et me just go back so remember that this formula had this   s t   s k  right where s t co uld be the last time step and s k could also be the first time step because you are summing over all the time steps r ight so y ou could have a term which is s t capital t which is the last time step the first time step and the derivative of the last time step with respect to the first time step right so that is a situation that we are deali ng with so we will consider one such generic element which is  s t  s k and we will just try to expand it so remember i have done this short circuiting so i am now just going to expand it again so th is is goi ng to be tt one t one t two and so on up to k one s k  ok and i can write it as this generic formula everyone find with this i have just replace this as a product and written it more compactly now let us look at one such term here  s j  s j one  now just to confuse you guys from next slide i will go over to  s j  s j one or not confuse you i just did not pay attention to this so instead of s plus one and j and i am going to do j and j minus one right it remains the same does not matter so we are interested in this particular quantity so let us see what this derivative is and remember that in the final formula we have a product of these quantities so i am looking at one such term in my final product so just to jog a memory a j is the pre activation which is given by this and then s j is the hidden representation after activation after the nonlinearity which is given by so let me just write it down as s j by s j minus one can be written as this chain rule which is first compute s j with respect to a j and then a j with respect to s j minus one everyone is fine that so far at this point please raise your hands if you f ind ok now let me just write down a j and s j explicitly so remember that a j is this d dimensional vector which are the entries a j one a j two up to a j d and s j is the corresponding activation applied vector which has these entries sigma a j one a j two and so on ok now first question what is this quantity scalar vector matrix tensor numerator is a student refer time twofortyfour denomitor is a student refer time twentyfivefortyfive that is why it is a matrix ok so that is the matrix that i am interest ed in if i can give you that matrix and we are kind of done so it help me filling in this matrix tell me what this matrix is going to look like even before we start filling it ok you are right but it does not matter because you will have u x and then you are taking the derivative with respect to s j minus one right so this does not matter ok so everyone gets that you will have a u x j here right but that does not matter because you are taking a derivative with respect to s j so that is a constant so  s j  a j is what what does this matrix look like how many of you see a diagonal matrix ok good so it is straightforward right what is the first entry it is going to be  s jone  a jone  what is that going to be it will be something but let us look at the second entry  s jone  a jone what is this going to be what this going to be student zero zero because i t does not depend on that right so now you can see how the full matrix will look like all the of diagonal elements are going to be zeros and diagonal elements are going to be sigma primes everyone fine with this ok so this matrix i am going to just ca ll it as diagonal sigma prime a j this is a diagonal matrix which i have and what is dou a j by dou s j minus one scalar vector matrix scalar student refer time fourseventeen matrix which matrix student w refer time foureighteen w right ok so now for some reason i am interested in the magnitude of this why i am interested in the magnitude of this for some reason i am interested let us see why we will become clear that for some reason i am interested in and here i will write how i will write the magnitude of this right so this is the norm that i am interested in so i have already said that this is actually equal to whatever is inside this norm so i can just write it as this norm so i have norm of c is equal to no rm of a b which is less than equal to student norm a norm b norm a norm b ok this is fine ok now let us look at the norm of this now going to say that sigma a j is actually a bounded function because we are using sigmoid or tan h or something so  it is a bounded function ok so that mean sigma dash a j is also going to be bounded actually can you tell me what is the bound for the logistic function for sigma dash a j if sigma is logistic function what sigma dash what is the bound for sigma da sh if i say one by four how many of you will agree with that how many of you have a problem with that if you do not understand this you not understand anything after that ok still do not have a problem so for the logistic fun ction the bound is actually one four the maximum derivative that you can get if you have this curve so then that would be one four ok what about the tan h function and that actually happens at this point right zerofive so zerofive into zerofive is one four what about the tan h function the bound is one righ t so this is this clearly an upper bond on these things the derivative is going to be an upper bounded thing that means this magnitude is actually going to be upper bounded by something and i will just call it as lambda sorry as gamma so this quantit y is bounded and i am going to call that bound as gamma what about our weight matrix it is again bounded right we have real weights we do not have like blowing we do not have very large weights it is all bounded so it is still going to be some upper b ound on this and i will call this magnitude as gamma right so this quantity on the left hand side i can say that it is less than equal to some gamma into lambda now let us look at the product so this is a quantity that i was interested in and this is actually a product of various such quantities so what is it going to be now can you go to the next step it will be gamma into lambda raise to t minus t minus k right t minus it basically as t minus this product as t minus k terms right so it w ill be gamma lambda raise to t minus k now if gamma or lambda or rather gamma into lambda if it is greater than one what will happen what will happen to the series explore if it is less than one student refer time seventwenty it will vanish right so y ou get that so that is why you have this vanishing an exploding gradients problem ok but why what if this vanishes what vanishes let us go back so i have shown you that this quantity could vanish right if this vanishes the entire gradient could vanis h and if the gradient vanishes what would happen student no updates no updates and you just stuck where you are if the gradient explodes what happens think in terms of the wb plane you suddenly have a very large gradient what will happen is just gone way far from where you are right now because your update is w is equal to w minus eta into this gradient and this you have got a very large value now it just going to move somewhere very far from where you are and that is never go where your suddenly jump to a different universe ok so that is the problem in training recurring neural networks you could have this problem of exploding or vanishing gradients and we have done a mathematical derivation of why you have this pr oblem ok so one trick to do that is to avoid this is remember these are t minus k terms and the problem appears when your t minus k is or rather you are t is close to capital t and k is closed to one right in those cases you will have many terms in the product you will have as many as t terms in the product so even if your product is even if this product is slightly less than one if you raise it to capital t it is going to vanish right so can you think of solution for this and the last module in the title of this lecture was truncated back propagation can you think of a solution for this so you do not back propagate through all the time steps yes use an approximation that if you are at time step n we are just going to look at n minus k time steps and we are not going to look all the way back right that is the common trick used to avoid exploding and vanishing gradients what is the other thing that you could do to avoid exploding gradients so remember that you have some gradient right to think in terms of vectors we have some gradient vector w whose magnitude is very large what will you do to avoid exploding gradients in gradient descent your always interested in the direction so what can i do student refer time ninethirtyfive just normalize it right so you can just do this so typically what is done is that you can it is a normalizing it you can just say that you will clip the gradient so that it is magnitude is less than a certain k right so normalize it i n such a way that it is grade it is magnitude becomes k so this is something typical that you will see when you use tensor flow where you have something with says clip the gradients to a certain magnitude and there are different ways of doing this so i just give you an intuition that this is what is used for magnitude but there are other things that you can use for magnitude so just go back and look at that ok so that is a back propagation through time with exploding and vanishing gradients and then the solution for that or a part for that is truncated back propagation ok we have we have not yet done with this problem we will again look at other solutions for handling this which will lead us to lstms which is long short term memory cells and gated recurrent units so that we will do in the next lecture"}
{"audio_filepath": "Data/Preprocessed/ SVD for learning word representations (Contd.)_77.wav", "duration": 56.0, "text": "so we will start from where we left yesterday ah so we did this whole story on starting from co occurrence matrices we learnt how to get better word representations and the key thing there was we used svd as a dimensionality comp reduction tool and we came up with this neat result that you could use w word as the representation of the mate of the words it has m rows and k columns where k is very less than the size of the vocabulary  so you have achieved lot of compression and you are still able to learn very meaningful representations which you could use for several downstream tasks what to use these for and how to use these for you will see that later maybe four lectures from now i mean i say four lectures i mean four two hour lectures right so it might be more in terms of actual lectures so we will get to that but for now we have a way of learning representations for words"}
{"audio_filepath": "Data/Preprocessed/Finding influence of input pixels using backpropagation_96.wav", "duration": 328.0, "text": "so now so farwhatwe havedoneis wehaveseen the influenceof neuronson theor whatimagepatchescausea neuronto firethenwe havevisualizedthe weightsso neuronshavebeenvisualizedweightshavebeenvisualizedthen wehavedonesome occlusionexperimentson the inputimagenowwe willtake this furtherand we are interestedin seeingthat whatpixelsin the imageactuallyhelpin the outputor in any neuron in the intermediate layers and we will find out some principal way of finding this influence rig ht and we are going to use bac k propagation that means we are going to use gradients ok so we can think of an image as an m cross n inputs going from x zero x one all the way up to x m   n nothing great about that and we are interested in finding the influence of each of these inputs on a given neuron ok now what is one way of computing influence that you have learned in this course what is the hero of this course gradients right so gradients tell you the influence so now can you tell me if i want to compute what is the influence of this neuron or this input on this neuron what would you do student refer time onetwentyseven  h j  x i  but can you compute that how will you compute that how d o you compute the gradient with respect to the input we have always stopped at the last hidden layer and the weights before that so how will we do that how will you do this ok this is a trick question just a hint is there a restriction on the chain r ule or can you do it you can just keep adding links to the chain right so what is so difficult about that you already know how to compute the gradients till this point and in fact you will also know how to compute the gradients till this point and w hat is stopping you from doing it up to this point what if i just call this h instead of x then you would not have a problem right and actually we call it h right we call it h zero  we can do it right it is straightforward so let us see refer slide tim e twoeleven if i want to compute  h j  x i  i can see that if the if  h j  x i is zero that means this pixel has zero input on the neuron if it is large then it has a high influence if it is small then it has a low influence so this is how i will see whether a pixel has an influence of the certain neurons in the in some of the hidden layers and this is not restricted to convolut ional neural networks as you can see i am just actually treating it like a feed forward neural network with parse connections ok so we could just compute these partial derivatives and visualize this gradient itself as an image so what do i mean by tha t is i am going to compute  h i  x zero  h i  x one all the way up to  h i  x mn right so i am going to co mpute this m cross n entities and i can just visualize this as an image now what do you expect this image to look like if zero represents gray colour what do you expect this image to largely look like what would you actually hope for this image should be largely gray because most of the input pixels should not be influencing a given pixel in the hidden layer right that pixel should influence by only a small number of pixels so that we can say that this is the patch which causes it to fire and not tha t every pixel in the input is causing it to fire because that is meaningless so that does not that is not something that we care about how many if you get this please raise your hands so i will just repeat it if a pixel fires for every pixel if a ne uron in the hidden layer is influenced by all the pixels in the input that means it is not really discriminating it is not really specialized right we want neurons which fire only for certain patches in the input so that we know that this neuron is re sponsible for this kind of a pattern ok so if i plot this as a image i would want most of these entries to be close to zero right because i want the influence to be zero ok now the question is how do we compute these gradients so we will just treat them as a free forward neural network we already know how to do back propagation across these roots and we just need to add one more term to the chain right so i will just show you what we will do here so i am interested in  h three two  x two  so i will observe that th ere are four paths which go from h three two to x two or rather from x two to h three two  so i will just sum up the gradients along these four paths right and if i solve it i will just be left with this ok so that is how i will visualize so this is very simple we have done a lot of gradients in class so you can just go back and check this and it should work out well ok so you can just see this and this way we can just compute the gradients for all the input pixels and now i am going to plot it as a image and this is what my image looks like do you see what is happening here its all very murky right most of it is great that is fine we expected it but there is not hing really standing out right even in this patch where you have some non gray pixels it is almost like the entire cat region is appearing as non gray the influences are not coming out to be very sharp we would have wanted something like only the eye p ixels cause some neuron to fire or only the ear pixels cause some neuron to fire and that is not really happening ok so it does not produce very sharp influences so someone proposed something known as guided back propagation which we are going to see next and that helps you to better understand the influence of the input pixels"}
{"audio_filepath": "Data/Preprocessed/Tips for Adjusting Learning Rate and Momentum_38.wav", "duration": 756.0, "text": "t ips for a djusting the l earning r ate and the m omentum ref er slide time zeroseventeen so before moving on to these slightly advanced optimization algorithms we will revisit the problem of learning rate in gradient descent refer slide time zerotwentyfour so one could have argued that we could have solved this problem of this slow movement on the gentle slope by increasing the learning rate remember that we have this eta and we deliberately chose to be conservative that we will take a small value for the eta  but what if i just blow up the eta  i could just take a ver y large eta what would happen it will overshoot right so what will happen is i will see what happens when i take eta equal to ten ok  so so i will see what happens when i take eta equal to ten refer slide time zerofiftyseven so this is step one step two st ep three its moving very fast on the regions where the slope is gentle  but it also moves very fast much faster on the regions where t he slope was already steep so when the gradient was actually high you ended up blowing it further by multiplying it with the eta which is ten so it i s again going to have this effect that you will move much faster in the steeper regions and again you will see these oscillations because you will overshoot your objective does that make sense right so it i s not that you ca n always choose a high eta and get away with it so what do you actually want what is your wish list regulate theta you want a adaptive eta right that it somehow figures out that i am on a gentle slope  so i should move slowly i should move fast and i a m now on a very fast loop  so i should move slow  so this having this one eta is not working for every point on the error surface right for everywhere on the error surface is that clear ok s o ok  so we will see such algorithms soon where we try to adjus t this learning rate refer slide time twosix now here are some tips for the learning rate  so how do you if you are just going to deal with this gradient descent or nag or momentum how do you adjust these learning rate  so how do you fix a learni ng rate so a learning rate is typically something known as a hyper parameter  so why is it called a hyper parameter so what are your parameters s tudent which i learned w hich i learned using the objective function eta is not a part of the objective function you are not computing radians with the respective to it i s a hyper parameter  so you will try to tune this hyper parameter  so what you will do is in practice you could try these different values on a log scale next what will you do run th is all these for a few epochs note down the dash just note down the loss function so run all of these with different learning rates for say five epochs you will get some loss right now which one will you pick the one which led to the maximum decrease in the loss i will keep that learning rate and now what you will do you just stick to that i started off with a dash scale s tudent log scale l og scale now what will you do ok  so now run it for a few epochs figure out which of these learning rates on the log scale works well now do a finer search around the best learning rate that you discovered right so say zeroone was the best on the log scale  so now look at zerotwo zerothree zerofour zerofive look at values around it and see which one works better so this is how you will tune the hyper parameters otherwise there is a very wide range right if you put tune from zeroone to zeroone there are just too many values to consider so we will have to do this log scale and then a linear scale will that make sense these are just heuristics there is no guarantee that will always work or which of these are clear winner strategy  but you have to try this  so tuning a learning rate is an important part when you are working in deep learning  so at least when you are workin g with gradient descent or nag or momentum based gradient descent refer slide time fourtwo now here some tips for annealing the learning rate  so there is something known as step decay  so what you can do is halve the learning rate after every five ep ochs can you tell me the intuition for this what do you expect after five epochs that you have moved enough and now you are closer somewhere to the solution  so if i closer to the solution if i closer to phoenix market city you want to move fast or slow s tudent slow w hat will you do s tudent refer time fourthirtytwo d ecrease the learning rate right  so after every five now this is again what is so sacrosanct about five it i s just a magic memory  so this is again hyper parameter  so you could fix some number of epoch and after these i will just halve the learning rate ok now this second one is what my favourite is and i typically use this what i do is i compute the loss after epoch t i run epoch t plus one i compute the loss again if the loss has increase d what will i do i will just throw away all the updates that i have made in this epoch i will decrease the learning rate and again learn again start this epoch what do i mean by throw away all the updates s tudent refer time fiveseventeen so after epoch t i will save my model i will save all the w values that i have computed and i will let it run for one more epoch after this epoch if my loss function actually increases i reload this model which i had saved half the learning rate and then run this epo ch again does that make sense  so i have run till epoch t i have some values of w s and b s i will save this values i will just save it as a numpy array now i will with the same learning rate that i have been using so far i will run the epoch t plus one ok and i get some new values of w comma b right i will plug this into the loss function i will plug this into the loss function i will get two loss values if this loss value is greater than what i was at the previous time step  that means things did not work out well in this particular epoch so i will throw away all these updates i will just reload the model which i had saved i will just start from where i was at epoch t i will decrease the learning rate i will make it half and run this epoch ag ain right and hopefully now i should do better because there is something i am just making a hypothesis that the reason i did not get to a better loss function was because my learning rate was not adapting to it so i will just halve the learning rate because this solution was good this was a low loss function i just want to be something around it i do not want to make any drastic steps so i will just half the learning rate from there  so then you not see this drastic change that your loss funct ion should not improve  so first of all local minima is known problem in deep neural networks so what happens is that in deep neural networks you do not have something which is like a neat convex function as your loss function right i t i s a non convex function which means there is no one unique minima there could be several minima and there are several analysis which show that a lot of these minima are equivalent  so in practice these are the things that you do either once you reach a minima you jus t stay there the second thing that you could do is you have trained your algorithm trained your parameters for say one hundred epochs and you have stopped now now again go back and start with a different initialization you started with some w naught b naught a nd you have reached to some solution keep this solution now start with a different initialization  that means if you look at your wb plane you have started from some other point  that means you started from some other error location right and run this algorithm again and see if you reach a different minima so the only thing you the way you counter this is you just try different stochastic things right should try to start with ten different initializations every time reach a minima and then at the e nd select the lowest possible of these did this make sense to most of you how many of you got this oh cool i thought i was just rambling  but yeah fine does that make sense to you at least ok does it fine y es a local minima is a severe problem in lot of deep learning optimization and typically people get away by that by just picking up one of these minimum fine now the other thing is you could use exponential decay where with each time step you just keep decreasing your learning rate and if this ca se two  that means at every time step you are halving the learning rate  so you just get with something like this but the reason i do not like this is that you have one hyper parameter which is eta which you are trying to tune and now to tackle that proble m you have introduced one more parameter which is k hyper parameter which is k so it becomes harder to tune that now and there is a similar thing which is one by t d k where you try to use this formula to decay or learning rate  so both of these i typ ically do not use in practice i use the second one i prefer the second one refer slide time nineeleven now tips for the momentum can you make sense of this you just stare at it it looki n g just come back ok let us see what happens at t equal to zero this becomes zero s tudent l og one l og one is zero this is two raise to minus one minus zero which is just two raise to minus one which is zerofive so what is your mu t at t equal to zerofive does that make sense is it fine with everyone or is it confusing no ok mu max is typically this let us assume mu max now what happens at time step two hundred and fifty this is two hundred and fifty by two hundred and fifty  so this becomes one one plus one is two the best thing that you learn in this course log of two is one  so this become two raise to s tudent two raise to minus two m inus two which is zerotwentyfive so what is this s tudent zeroseventyfive zeroseventyfive let us do one more i had t equal to seven hundred and fifty one minus one by eight  so that is what is going to be right ok  so then what is happening as my time steps are increasing what is happening to what is happening i am having more an d more faith in the history or the current gradient what am i increasing actually i have made a mistake actually this is mu is gamma there is not we did not use mu anyway what you guys just went along  so this is gamma actually right that was a momentu m term that we had  so as a number of time steps is increasing my gamma is increasing  that means i am having more and more faith in my s tudent refer time elevenfortynine n o history learning rate is eta momentum is gamma  so its gamma into update t minus one and eta into gradient at the current time step right and here gamma is actually equal to mu is there any more confusion that i can a d d so when i say gamma i mean mu and so that is how it is  so as i am increasing the number of time steps i have more and more faith in the history  that means i do not want to now get distracted by this one update which i am making right i want to go by the history and i am not increasing this gamma or mu indefinitely i am capping it by a max right m ax i will have this much faith which is zeronine hundred and ninetynine in the history does that make sense this is again just a heuristic do not worry too much about it  so that is how it is"}
{"audio_filepath": "Data/Preprocessed/Visualizing filters of a CNN_94.wav", "duration": 346.0, "text": "so now this was visualizing the neurons inside the convolutional neural network so neuronsrememberare the outputsrighttheseare not theseare the featuremaps what about the weights itself what are the weights in a convolutional neural network student refer time zerotwentyfive the filter s the filter s themselve s are weig ht have y ou eve r trie d to visualize weights beforewhen student auto encoders auto encode rs and wha t was a trick there how did we student refer time zerothirtysix visualize what was the optimization problem that we solved student refer time zeroforty how many o f you went and looked a t the prerequisites h ow many of yo u looked a t the prerequisites ok so we had done something similar while discussing auto encoders so because that we had done something similar while discussing auto enc oders right so we were interested in knowing that there is a particular hidden neuron inside the auto encoder and we wanted to see that what does this neuron capture so if i give it emnist digits then what kind of patterns does it fire for and if you remember we had solved this optimization problem and realize that this neuron will fire for an input which looks like this where wone or all the weights which are connecting to this neuron ok w hat was the dimension of the input if you are dealing with emn ist digits student refer time onetwentytwo seven hundred and eightyfour what is the dimension of this a one thing which i have circled here student refer time onetwentyseven seven hundred and eightyfour right it is written x equal to so it has to be seven hundred and eightyfour why is it seven hundred and eightyfour because there are seven hundred and eightyfour weights connect ing each of the input pixels to that neuron right so that means this weight matrix itself we can visualize it as an image and thats exactly what we had done if you remember we had this grid of images that we were analyzing and in some images we saw that some dark element fires here and each we were arguing that this is the curve which exists in two or nine or eight and that is the one which is capturing a nd in some cases there was a cusp here which was firing and we were arguing that this could be for the three or f or a nine or for a eight or something like that right so we were trying to visualize these things and the way we had plotted it was just treating this weight matrix or weight vector as an image and seeing what causes the neuron to fire right refer slide time  twotwentyone so we can do something similar for convolutional neural networks i want you to think how would you do that i will give you some hintsthe answer is there on the next slide but i just want you to think about it right so remember here you h ave dense connections ok that means your weight vector was the same dimension as the input vector w hat about filters in the case of cnn t hey are smaller they are three  three five  five or seven  seven much smaller than your original image so then what do these filters correspond to j ust think of the animation that we had seen right we had this image and we were taking a filter and applying it at different places so what does the filter cor respond to w hat is the filter overlapwith patches in the image right so now what kind of analysis can you do student dense what kind of patches does this filter fire for or what kind of patches does the neuron connected to this filter fire for d oes that make sense everyone gets the intuition h ow many if you get the intuition please raise your hands thank you so now recall that we can think of a cnn as a feed forward neural network and in particular when you have a filter it actually interacts only with few pixels right so interacts with say pixel one two five and six so that is the patch that it interacts with and now i want to see when does this neuron fire so that is the same as asking what do i put in one two five six for this neuron to fire or similarly what do i put in three i do not know this was one two five six i guess so three four seven eight for the same different neuron to fire right but all these neurons fire because they are connected to the same filter so that means i am interested in these patches which will cause the neuron to fire and those patches can appear anywhere in their image is that fine that is the whole point of convolution neural networks wherever there is a nose whether it is at the top corner of the ima ge or the center or the bottom it should be able to detect right that is the whole point of weight sharing and sparse connectivity ok so we are going to do exactly the same thing we will have a three  three filters or five  five filters or seven  seven filters were just going to visualize as them as images but unlike the earlier case where the image a ctually correspond to the full mnist image here these images are j ust corresponding to those three cross three or five cross five patches and you want to see what kind of patches causes the neurons to fire ok and the solution is still the same we will have this w by w the normalized weight filter weight which is causing the input to fire h ow many if you are fine with everything at this point please raise your hands high up so this is what we get right and we observe certain things which like we had earlier made a case for that these filters try to detec t certain types of patterns or textures or edges so you can see that right this is capturing these slanting edges this is trying to capture some horizontal sorry vertical edges then some edges oriented differently and also some colored patterns some tex ture right so you see something like a checkbox here or chess box here and so on so these filters are actually firing for different kinds of patches so they are trying to detect different things from the images so you could visualize this and unles s you see a lot of variety in this that means something is wrong right because your filters are not being trained to be discriminative with terms of different patterns that they can detect and so on right so you want these variety of patterns to occur ok and i am going to make a claim that this is only interpretable for the first layers in the convolutional neural network why is it so i am seeing some half complete answers so i will ask this as a quiz question"}
{"audio_filepath": "Data/Preprocessed/(Need for) Sanity_9.wav", "duration": 241.0, "text": "so lot of fields have adopted deep learning now and lot of state of the art ai systems are based on deep neuralnetworksbut now what i s needed is after all thi s madnes s were deep learning has taken over a lot of research areas can we now bring in some sanity to the proceeding so this is really a need for sanity and why i say that is that because there is this paradox of deep learningso there is this interesting question that why does deep learning works so well despite having a high capacity so the deep neural networks have a very high capacity which means that susceptible to over fitting so most of you would have done some course on machine learningso there you know that over fitting is bad because you are just memorizing the training data and then you might not be able to do so well and at tested and over fitting happens when yourmodelhasahighcapacitysoeventhoughdeepneuralnetworkshavehigh capacity why are they doing so wellwe will focus on this high capacity but when we talk about the universal approximation theorem and give some arguments for why deep neural networks have such a high capacity the other thing is they have this numerical instability right so we spoke about these vanishing and exploding gradients and again we will talk about this later on in the course so despite this training difficulties why is it that deep neural networks performs so well and of course they have this sharp minima which is again it could lead to over fitting so if you look at there is an optimization problem it is not a neat convex optimization problem so it is a non convex optimization problem so why does it still do so well so it is also not very robust so here is an example on the right hand side the figure that you show so the first figure is actually of a panda and the machine is able to detect this panda with some fiftyseven percent confidence  right we have trained a machine for a lot of animal images w e have shown it a lot of animal images at test time we show at this image the first image that you see on the right hand side and is able to classify this is a panda with fiftyseven percent confidence but now what i do is i add some very random noise so that second image that you see with some very random pixels if i add it to this image i will get a new image so every pixel in this image is added to this new noise image and i get the image which is see on the third the third image that you see right to you and me or to any average human this still looks like a panda t here is hardly any difference between this image and the original image but now if you pass this to the machine all of a sudden instead of recognizing this is a panda it starts to recognize it as a gibbon and that too with ninetynine percent confidence  so why is it that they are not very robust and despite this not being very robust why are deep neural networks so successful  so people are interested in these questions and people have started asking these questions there are no clear answers yet but slowly and steadily there is an increasing emphasis on explainability and theoretical justifications  so it is not enough to say that your deep neural network works and gives you ninetynine percent accuracy it is also good to have an explanation for why that happens is it that some components of the networks are really able to discriminate between certain patterns and so on so what is going on inside the network which is actually making it work so well right and hopefully this will bring in some sanity to the proceedings so instead of just saying that i apply deep learning to problem x and got ninety percent success  we will also make some kind of more sane arguments just to why this works and what is the further promise of this and thinks like that so that is roughly a quick historical recap of where deep learning started and where it is today st arting all the way back from advances in biology in one thousand eight hundred and seventyone to recent advances till two thousand and seventeen and so on deep learning right and h ere are few url so you could take a look at this for a lot of interesting applications of recurrent neural networks bunch of startups which have come up in this space is working on very varied and interesting problems and he re are all the references that i have used for this particular presentation so that is where we end lecture one and i will see you again soon for lecture two"}
{"audio_filepath": "Data/Preprocessed/Distributed Representations of words_75.wav", "duration": 681.0, "text": "so whatwe saw now wassparserepresentationsor one hot representationsparse because onlyone bit is on fromthere wewillmove on to somethingknown as distributedrepresentationsof words andyou havealreadyseen that sparseis in theorybut itgivesa very simplerecipeof convertingwordsto vectorsbut it does not serve much purpose s o let us see what distributed representations of words are s o this around one thousand nine hundred and fiftyseven j r f irth made this very profound statement that you shall know a word by the company i t keeps and this of course a pla y on some other similar code  but what does this actually mean  s o it means that you want to know what does the word bank convey  o r what is the essence of the word bank right w hat this code says is that if you want to know about bank you should say you should see the company that it keeps  that means what are the other words w hich appear typically in it is neighborhood a nd of course when you have a large amount of corpus given say the entire wikipedia o f course at that time wikipedia did not exi st  but any large corpus a nd does this led to something known as distributional similarity based representations s o to understand t his we first have to understand the idea of a co occurrence matrix refer slide time onethirtyeight s o the basic idea is to u se the accompanying words which in this example happen to be financial f inancial deposits credit etcetera to represent bank and to do that we will construct something known as a co occurrence matrix which looks like this right s o a co occurrence matri x is a terms cross terms matrix  that means every row in the matrix corresponds to a term or a word and every column in the matrix also corresponds to a term or a word c an you guess what how many rows would there be s ize of the vocabulary h ow many colu mns would there be size of the vocabulary right o kay  so here is how we construct a co occurrence matrix s o we take a word we are interest interested in constructing the row for that word t he number of columns is the same as the size of the vocabulary n ow for every column we will make an entry which tells us whether or how many times this this word appeared in the context of the target word right f or example i f i look at machine i am looking at the row for machine i am trying to construct the en tries in that row i know that the number of columns is equal to the all the unique words in my vocabulary s o i look at the first word which is human and in that cell i enter the value which is the number of times human appeared in a window of k words a round machine is that fine is that straightforward a nd that is how i will construct this co occurrence matrix where i have taken the window size as two  that means in any given cell my entry would be the number of times human appeared within a two word w indow of machine is  right the ig t h cell clear the definition of the ig th cell clear s o this tells me that user actually appeared two times around the word system in a window of two words around it is that clear everyone gets this how i construct the co occurrence matrix ok n ow you could use the same  so this is known as words and this is known as context  that means the rows we refer to them as words and the columns they refer to them as context n ow as you said that the number of rows and the numbe r of columns can be same that we can consider the same words in the context as well as the same word as the target words right b ut you could also do something different you could say that i do not want to consider all words that is context words b ecau se for example the word for appearing with any other word does not really give me much information because it is just a stop word right or the word the or an d or a these are known as stop words in the language these do not really give me much informatio n s o if you go back to the bank example financial credit deposit are the words which i really care about and these are the financial bank or with deposits and also these words do not really matter a lot ok do you get the intuition s o you could choos e to have fewer columns which are only the important words that you consider and you ignore the sto p words ah i n this discussion i will alternately switch between considering the columns as the same as the rows and sometimes are restricting the columns to fewer number of entries refer slide time fourthirtyeight s o now each row gives us the vectorial representation of the word s o we have seen how we have moved from sparse representations to distributed representations s o now take a guess now would this ve ctor be sparse s o we saw the extreme sparse right w hich was one hot  n ow the vectors which you get here are they going to be dense or still sparse sparse right b ecause every word does not appear with every other word right you still have these v dim ensional vector and there are some words which will appear with very few words right s o you expect to have non zero entries in very few columns right so these representations are also sparse refer slide time fivenineteen s o there are some problems some o f which are fixable s o we look at the fixable problems first t he first thing as the stop words are very frequent s o these counts should be very large s o if you take the entire wikipedia corpus and you take the word machine or system t hen the words the and for and so on would have appear ed like more than one thousand times in the context of the word machine right a nd as compared to the other words like system or user they would have appeared much fewer times s o this kind of skews your counts right it is like highly biased in the favor of stop words s o how do you deal with this refer slide time fivefiftyfive s o there are two ways one is ignore frequent words right s o that is the solution which i suggested earlier t hat your number of columns would be l ess than the number of words is that fine s o you do not actually consider frequent words at all t he other is user threshold t so  that means in these columns like for and with and so on w hatever be the entry if that crosses a certain threshold then i will just replace it by that threshold is that clear s o i am just saying that this means that the word has appeared more than one hundred times a nd i am not interested in keeping the actual count which was more than one hundred i just saying that more than one hundred is enough for me right b ecause i know that all the other entries are going to be much less than this s o just like replacing it by a very large number instead of actually counting that number refer slide time sixfortythree t he other solution is instead of c ount you can use something known as pmi s o this is how you compute pmi e ven if you do not know it does not made a lot of difference because you know that it will always be there on the slide that is why you guys do not read anything s o pmi is comput ed like this s o intuitively tell me what does pmi capture l ook i would say focus on this formula rather than the upper one when would it be high t he easier question to answer is when would it be low remember you are dealing with a fraction s o if in dependently the two words appear a lot of times b ut together they appear very rarely then the pmi is going to be low is that clear n ow if both the words appear one hundred times and together also they appear one hundred times that is the best case scenario  that means t hese are very tightly tight words right they always tend to appear together s o the pmi would be high for words which are very frequently co occurring n ow  so this is what would happen if you replace the counts the co occurrence counts by the correspo nding pmi ok n ow if the count of two words is zero we have a problem because then the pmi tends to be minus infinity right s o how do you deal with this situation e psilon or some we will use some hack right as usual s o instead of pmi use something know n as pmi zero which works like this i f the count is greater than zero then you use pmi i f the count is equal to zero then you just put the entry zero in the set make sense t here is also something known as positive pmi which is slightly more extreme it says th at use the pmi only if the pmi is greater than zero otherwise use zero c an anyone tell me the rationale for doing this y ou see the subtle difference between the three things right o ne is of course doomed because you cannot handle zero counts t he other one is sayin g that if the count is zero then i will just substitute zero the last one is saying that if the pmi is negative right t hen i will replace it by zero  that means in the last case all the cells in your or in your pmi matrix would be positive right non negative rather s o can you tell me the rough intuition for using this and there is only a rough intuition but can you tell me so the very rough intuition for this is that what does it even mean to say that two words are negatively correlated i mean either the y occur together right w hich means there is some relation between them but a negative relation between words does not make sense that is the intuition behind this n ow of course i could argue that what about antonyms and things like that  but that is a lso not the same right because you could have good and bad in the same sentence right b ut that is the roughly the intuition that negative values do not mean much s o just replace them by zero s ok t here is no again a formal reasoning behind this  but just the intuition s o we have looked at the co occurrence matrix where we started with counts t hese counts were very sparse and there are also some other problems with counts in the terms of some frequent words taking a lot of limelight and so on s o we ha ve fixed although then we have done some very minor and simple fixes a nd i just very rush quickly rush through them because they are very simple b ut these were all fixable problems what a non fixable severe problem with this w hat is the problem with t he one hot representations l arge refer slide time tentwentyseven w hat about these representations s till large it is still of size v i t is still very high dimensional still very sparse not as parse as the one hot encoding  but still sparse and it grows wi th the size of the vocabulary s o now remember that p enn treebank at fifty k words g oogle one t corpus at tha thirteen million yeah s o it keeps going with the size of the corpus s o now how do you know h ow do you fix this i wish i h ad that harry p otter thing a n yone remembers that spell to wipe out your memory h ow would you deal with it s o you now see how it connects s o now again you have ended up in a situation where you have a very high dimensional matrix right a nd you are looking for ways to reduce the dimensions so we w i ll go back and rely on things that you have learned and one of those was svd right s o you can use singular value decomposition w hy did i say svd and not pca b ecause this is n ot necessarily a square matrix a t his could be a rectang ular matrix and for all practical purposes svd is just a generalization of pca"}
{"audio_filepath": "Data/Preprocessed/Denoising Autoencoders_54.wav", "duration": 1533.0, "text": "in this module we will learn about denoising autoencoders refer slide time zeroseventeen so the idea behind the denoising autoencoder is very simple what you do is you have your original x i  now for the minute for a minute just consider the discussion when your x i s are binary inputs ok so each of these red guys can be between c an be zero or one now what i do is before feeding it this input to my autoencoder the box is the autoencoder w hat i do is i do a corruption so the corruption is as follows with probability q i will set x ij that means one of these guys to zero right and w ith probability one minus q i will keep it as it is ok so with some probability q i am actually corrupting the data otherwise i a m retaining the data as it is and then feeding that data to the autoencoder why would this work binary input case as i sai d just assume that the inputs are binary we will also see the other case why would this work what was our problem earlier that was completely able to reconstruct the training data right but at test time i had issues now what i have done to the trai ning data corrupted it just think for a minute what will happen now i want someone to ask me a question in return oh that is the corruption that i am choosing or you could flip it is what you are saying yeah if it is zero change it to one so that is also fi ne that is the question i was expecting what is the loss function now what is the loss function x hat my i minus x tilde i or x hat i minus x i which choice makes sense student first tilde first let us the case take the case when i do x tilde i what happens in that case from this networks perspective it is still learning to memorize the training data right it just this is what it thinks as the training data and just trying to learn that transformation right so it is not really helping my case do you understand that i just corrupted the training data that is fine but from the networks point of view it still gets away by memorizing this data and that is not what i want so what should i do can anyone tell me the i mean can everyone tell me th e answers student minimize minimize the error between student an x i an x i how many if you understand why that should help all of you gave the answer but only few of your raised your hands why so  hard to deal with this inconsistency refer sli de time threefour besides because i am still going to minimize my original objective function ok now can the network get away by copying the input to the output so input remember the input to the network is this and what i am trying to minimize this if i just copy x tilde i to the output will my objective function be minimized no right so it does not have incentive to copy now so what will it have to rely on say a reasonable probably twenty percent is the standard right so even if i reconstruct i w ill not get zero error i will at least get some twenty percentage so let us let me give you an example and then let me know if you can figure out what happens this example will contradict something else that we have done before but just play along suppose my input features were height weight and bmi and we all know that bmi depends on height and weight i hope all of us know now can you think what is happening i am corrupting one of these inputs and i still want everything to be reconstructed back so wha t will the network now have to rely on it will have a now rely on this relations between these inputs also so again if i take my example of digit three i have corrupted some of these pixels right but i still want to be able to reconstruct three so it will h ave to be smart enough to learn that if i have seen this and i have seen this then it has to be something in between which gives me a three do you get the intuition so now i am making it is job harder so that it is robust to changes at test time that mean s a test time if my digit looked something like this it should still be able to predict it as a three or it will still be able to learn the same representation as three do you get the intuition right so that is what i am trying to do i am trying to somehow br ing in the corruptions that i would expect a test case and trying to make the model more robust it can no longer get away by memorizing the training data because i am not feeding it the correct training data it has to do something smarter than that ever yone gets this i will come back to your question everyone gets this please raise your hands yes yes this is all under regularization no this is regularization no so at that case i have already made that overfitting can happen in an over complete as well as under complete autoencoder everyone gets that right i show that example where it could happen in both the cases so my figure maybe over compete but it can just happen in any of these is that fine it no longer makes sens e for the network to just start copying the input data different kinds of noises means yeah so let me try to answer that right so what probably you are trying to say is that all my input images were three vertically written i added some noise and managed it but now at test time suddenly you show me a three of this kind like that will not work also that is what were your question was a different types mean different values of the noise twenty percent twentyfive percent and so on so we will first see practical application in which autoencoders are used and then compare it to denoising at autoencoders so this the next few slides for those of you may care is also a small answer to the difference between machine learning and dp refer slide time sixthirtysix so suppose you are given this task which is handwritten digit recognition i see everyone paying attention now i should say this before every slide ok so this is the task handwritten digit recognition you are given some data where you w ant to classify the digits into one of these ten glasses the traditional machine learning approach to this is we just construct a feature vector this is a twentyeight cross twentyeight image so i guess twentyeight cross twentyeight pixels which is seven hundred and eightyfour i treat this as a feature vector and feed it to any of my machine learning algorithms say svm or multi class svm or logistic regression or any of these right and do a classification based on them this is what you would have done in your machine learning course if i had given you this assignm ent right now the autoencoder approach or in general the deep learning approach would be you take this data which is the original feature representation that you had there is no engineering feature engineering happening here right ideally i want to have features of the form that if pixel twentyfive comma thirty was black and if pixel thirty comma twenty was also black then probably i am drawing a curve somewhere so it could be one of these curvy digits and not one or any of these seven or any of t hese things right so you want to do some feature engineering so typically in machine learning what you do is you start with these seven hundred and eightyfour features you observe a few things and you have these handcrafted features added on top of these right so you will a dd some more features to the data now the deep learning approach is that you let you also learn the features on their own so how did we learn these features we took this original input we passed it through the an autoencoder which captured some of thes e relevant characteristics the differences we do not really know what these relevant characteristics are that means you and i cannot read them and make sense of them i cannot say that this pixel is actually capturing the interaction oh sorry this neur on is actually capturing the interaction between my seven hundred pixel and seven hundred and ten pixel i cannot do that i could have handcrafted those features if i believe that all my data is around the center i could have handcrafted some features which say that capture the in teractions between those that is what you do in machine learning here you are trying to learn the features also on their own right what would happen if i add one more layer to this autoencoder i would learn even more complex interactions between these f eatures so this neuron is actually learning interactions between all the input neurons ok i add one more layer here again this neuron will learn all the interactions between these abstract representations right so i could learn more and more abstract representations of the input so i am not doing feature engineering i am just throwing data at the network and i am assuming that it will learn better and better representations now i am doing this in the autoencoder setup where actually i am trying to optimize the objective function of minimizing this loss and of course the squared of this loss is just fine so first what i will do is i am not happy with my original seven hundred and eightyfour dimensions so i train a autoencoder to learn some k dimensions which are good i know these are good because they are able to reconstruct the data perfectly to a certain extent right of course because you add regularization it may not be perfect but it captures the essence of the data you get that so i h ave better dash represent ations now feature representations right m y original feature representation was seven hundred and eightyfour i have come up with some better representations now what will i do was my task to learn feature representations what was it classification right so what will i do now is i will i have learned this much from the autoencoder i will throw away the last layer i do not care about the last layer what i care at the last layer is a classification problem right so i will construct a new neural n etwork where the first two layers of the network are the same as what i learned from the autoencoder and on top of that i will add an output layer and now i will try to train this network how many of you get what is happening here those of you do not get it can you ask me some questions let me just try to answer on my own it is like playing chess with yourself so this is my original input seven hundred and eightyfour dimensions what i have learned with autoencoders is a smarter representation of this data ok now one simple so lution that i have is i have this one hundred dimensional data suppose this is the representation so for all the training examples instead of using that seven hundred and eightyfour dimension data and feeding it to a multi class svm what i can do is i can first compute this one hundred dimens ional representation and feed that to a multi class svm is that fine and you see that should work better in practice because i have reduced the dimensions i have reduced the dimensions smartly a nd now i can train this network is this fine all i am sayi ng is instead of a multi class svm i could also have a neural network right i could feed that representation to a neural network so what would that neural network look like one hundred what are the parameters here w belonging to one hundred cross ten how many if get it now ok so this is what i could have done so i have learned a better feature representation and now i am using that representation to learn my classifier if i do this in an end to end manner that means my feature representation is also came out of a neural network and my classifier is also a neural network then i have a complete end to end solution for this you get this now we will see a way of visualizing this and then we will make some observations from the visualiz ations so first let me tell you what the visualizations is so i am returning to the autoencoder setup so i had this input and i had this h dimensional or k dimensional hidden layer now i can think of each of these neurons as something which gets activated for a particular type of input is that fine what do i mean by activated it is output would be student one remember this is a logistic neurons that we are talking abo ut or even tanh neurons the output would be one so it is the maximum output that you could gain fine now so for example h one is equal to sigmoid of this when would this fire when where w one transpose x i is very high right when you are in that regime where the sigmoid flattens right this regime ok when it is very high so i want to be able to maximize w one transpose x i do you get this i want to be able to maximize this i want to find my w one transpose is fixed now because i have trained the autoencoder i have got these weights this is all post mortem r ight i have trained the autoencoder i have got these weights now i want to find an input which will cause this particular neuron to fire so what is my max what is my optimization problem maximize just help me out maximize w one transpose x let me just call it x and the optimization is with respect to x right because i want to find the x which maximizes this quantity my training is done i do not no longer care about changing ws my training has been done i am interested in finding x s w hich will maximal ly fire this so and i am going to assume that all my inputs are normalized this just makes some analysis easier and remember that normalization is always ok you always do that so this is the optimization problem that i am i nterested in solving what is the solution to this how many if you can solve this no i want to find the x i student refer time fourteentwentynine now i have trained the autoencoder now i have known all these the one i am considering one column of the matrix w one i want to see what is the input that i should give so that i am sure that this neuron will get activated and i know that this neuron will get activated if i maximize this quantity right so i want to maximize that quantity and find an x such that i t will get maximized i was just hoping that no one brings in eigenvectors w one is a column it is not a matrix just try to work it out what is this this is a dash between w and transpose and x i dot product when would the dot product be maximized when t hey are both in the same direction right that means you know the direction is going to be x i is equal to and w hat did i want the norm to be n ow do you get it fine so the solution is going to be this is fine w one by the norm of w one so just remember that this quantity is going to get maximized when the dot product is maximized the dot product is maximized when both x i and w one transpose are in the same direction right so that means x i should be in the same direction as w one and i also wanted this constraint that x i should be the norm of x i should be one so i am just dividing w one by the norm of w one so i know now what is the input i should feed to the network so that one of these neurons fires now what i am going to do is i am going to plot the xi s which maximize each of these neurons i am going to consider some one hundred neurons in the hidden layer and i am trying to find out the input image which is going to maximize or which is going to cause each of these neurons to fire  do you get what i am trying to do even though you do not get why i am doing it but do you get what i am trying to do ok so what am i going to do is this is a vector right so i am just going to try to plot this as an image of the appropriate dime nsion and this is what i get with a vanilla autoencoder there was no noise this is what i get and this is for the mnist digit data set right so my data is two three one and so on digits this is what happens when i get twentyfive percent nic e and this is what happens when i get fifty percent what do you understand from these figures remember that each of this is the figure which caused one particular neuron to fire is that clear each of these is a trigger which caused one neuron to fire one image yeah one box corresponds to one column yeah so it is just that the dimension of the column is again twentyeight cross twentyeight so i am just plotting it as a twentyeight by twentyeight image so i will just let me just clarify that is i think that is what i said yet so what i s the dimension of this in fact you just know this right this dimension of this is twentyeight cross twentyeight so i can just take that vector and again plotted as a twentyeight cross twentyeight image so what i mean is this is seven hundred and eightyfour right so x i is a seven hundred and eightyfour dimensional vector i am just taking it as a twentyeight cross twentyeight image and plotting it because my inputs were actually images so i am just plotting those images fine so at least you see what i am doing here and what i am telling you is that each of these boxes that you see corresponds to one of these images so i had images x one x two up to x k such t hat each of these caused the k th neuron to fire ok now what are you seeing here i mean what how do you make sense of what you are seeing and remember in the mnist ok sorry so let us try t o forget all this neural network and everything and let us just try to see yes the weights would be student more distinct no why do you say the weights are more distinct yeah but on average you would be still reducing it right ok so let me just expla in what is happening then we can come back to this so now we have this set up we had some input we had a certain number of neurons here and then we had the output ok this is what our neural network was trying to do student refer time eighteenfiftythree now l et us take this task of recognizing a digit now how do i actually recognize a digit if i want to distinguish between a nine and a three i would try to see if there is a curve in these positions and it is not there in this hence this is a three this is a nine that is something roughly like that right so in other words i am now i have given delegated so that means what i do is i think of three as a combination of you get the idea as a combination of these images with these strokes right so this is actually this strok e this is actually this stroke this is roughly this stroke and so on you get the idea so i think of three as a combination of many of these strokes right now what i would like is if this guy could detect one of these strokes right the other guy could de tect one of these other strokes right now you see that some of these strokes are shared across digits for example all these strokes here look at the digit nine these strokes gives common to three and nine both right but some strokes would be missing for three some st rokes would be missing for nine and you would have extra strokes in both of these so now each of these neurons could actually recognize these strokes then a combination of the information that each of these neurons is capturing could help me decide whether it is a three or a nine how many of you get that intuition student refer time twentytwentythree so i would like each of these neurons to detect certain strokes ok that means i would like this neuron the first neuron to fire for an input like this where there is a stroke at the bottom i would like some other neuron to fire for a different input whether there is stroke here now can you relate this to what you are seeing in the picture in the second and third picture this neuron is firing for inputs which would h ave a stroke at the corner right and you see different neurons are firing four different strokes so each neuron is trying to capture something relevant and together now i could combine them to get the final output how many of you see this how many of you do not get this so to ask questions otherwise i cannot really help it how many of you want me to go over this again w hich part yeah so let me just repeat what each of these boxes is right so each of these boxes is the image which causes the k th neu ron to fire right so remember i decide i came up with this that this is the input which causes the second neuron to fire what was the dimension of this input twentyeight cross twentyeight so i am just plotting that twentyeight cross twentyeight input right and i am realizing that thi s input seems to be something which has a dark spot here right so now just related to the analogy that i am trying to give at the bottom that this neuron fires for inputs which have a stroke here that is that is capturing and there are other neurons whi ch are trying to fire for other strokes a nd i would want these neurons to capture different strokes so that together they captured all the information in the image and helped me decide that a combination of these strokes gives me a nine a combination of th ese other strokes gives me a three is that clear now you also student yes sir so yeah so now the thing is this right the again the same thing you could learn to reconstruct the output but you may not capture the important characteristics in the input right so now as you keep making it is job harder it has to rely on capturing these important characteristics in the input right and actually if you look at the difference between the second figure and the third figure right let us look at the same guy here so you see that this is actually thicker and wider the stroke that you see here is thicker and wider so now it is actually relying on more neighborhood information to fire it is not firing just for this stroke but it is fighting for a larger str oke it is also requires more neighborhood information because you are corrupting the pitch so it has to rely on information from the other guys the same example that i gave for height weight and body mass index right the same thing holds here i have co rrupted a lot of inputs so now it will fire only if it gets a lot of information from the neighboring inputs also is that fine ok and i now coming back to your question yeah i do realize now what you are saying that the weights are actually becoming la rger yeah it makes it more robust but again so regularization just does not always mean that your weights have to be small right that is one way of constraining or regularizing but this is another way of regularizing where you are making it more robus t but it does not necessarily need to lead to the same solution where your smaller weights does that make sense it is ok for most of you any pleas e raise your hands if this a nd this is same thing that i have written here r efer slide time twentyfourtwo now we saw one form of this function ok which was just flip the input if the output is just corrupt the input right you could also add a gaussian noise so you could take the input add a gaussian noise to it with zero mean and th en again try to reconstruct the original input back is that fine so you could just use different noise functions to do this so we will now see such a denoising autoencoder where we have actually added a gaussian noise instead of the zero one noise or the c orruption that we were doing yeah so the purpose of this particular example that i am giving is to compare an autoencoder which is regularized by adding this gaussian noise with an autoencoder which is regularize by using weig ht decaying right the ltwo regularization so l two regularization is also known as weight decaying because you kind of decay the weights right you force the weights to be small so what they showed is that with denoising autoencoder using a gaussian noise you actually learn something known as edge detectors right so you see all of these are trying to detect edge again the same thing is happening i am plotting the images which will maximally cause a particular neuron to fire and it looks like all these ne urons fire for different edge patterns in your original data so now they are capturing all the edges in the data and the combination of these edges should tell you what your final class is ok and this seems to work much better than the weight decay fil ter which is not really capturing any regular pattern ok so this is just an empirical evidence that an autoencoder with a gaussian noise seems to do better than autoencoder with the ltwo regularization"}
{"audio_filepath": "Data/Preprocessed/Create images from embeddings_99.wav", "duration": 335.0, "text": "the next thing that we will see is how do you create images from embedding so let me see what that means so remember that each of these things can be thought of as an embedding of the image right because you had this original image which was two hundred and twentyseven   two hundred and twentyseven dimensional and now you have a four thousand and ninetysix representation for that or a two hundred and fiftysix   seven   seven representation for that so you could just flatten it as an out as a vector and you could treat that as a embedding for the original image right now for any kind of embedding or hidden representation what do we always want from that representation think auto encoders it should capture all the important characteristics of the original image and in particular i should be able to dash the original image from it s tudent refer time onethree we construct the original image from it right so thats what i would want from a good embedding so let us see if we can do this right so find an image this is the optimization problem that i am interested in find an image such that its embedding a similar to a given embedding what do i mean by that is suppose i take a monkey image and pass it through all these layers and compute all these embeddings right now again i start with a blank image and my optimization probl em is such that for this blank image i want to modify it so again this blank image is my parameter matrix and i want to modify it such that the embedding that it produces should be similar to the embeddings that the monkey image produced so how can yo u set this as a optimization problem what would your loss function be so lets call the original monkey images ione and let us call this as embedding of ione now can you tell me what the objective function would be for the new image that you are trying t o create this entry the first entry in its output that let me call that e i two ok so e i two one and ei one one that means the first dimension of the embedding they should both be student very very close so in such cases what is the error function that we w ill choose student refer time twotwentynine refer time twothirty right so you have to get comfortable with designing these loss functions right so you have seen you have seen this loss function before we just have to be able to related to the problem tha t you are trying to work on so let phi zero be the embedding of the image of interest let x be a random image and we will report the repeat this forward pass using x and compute phi of x right that means were computing the em bedding of this random image that we have started with then we compute this loss function and add appropriate regularization for that and that propagate and update what what will you update student refer time threesix image right you will update your x matrix right and you will keep doing this till convergence and let us see what happens so its suppose so now what i am trying to do is this is my original image and i have the convolution one embedding of it so in this i am using convolution one as the embedding and then i am trying to solve this optimization problem to recreate x such that its very close to the original image so let us see what are the different outputs that i get so this is the original image and on the right hand side you have the reconstructed image such that the conv one embedding of both the images is the same so you can see that when i am trying to do a reconstruction from the conv one layer i get almost the same image back now if i keep do ing it from different layers what do you expect it to be if i do it from conv two conv three conv four and so on student refer time threefiftynine it wont be so accurate right so let us see what happens if i try to reconstruct it from conv two refer slide time four five ah relu one max pooling norm one conv two relu two i am keep i am going deeper and deeper into the network so what i am trying to do here is remember that i have different choices for these embeddings so the first thing which i showed you was when i was trying to the first thing was when i was trying to set my objective function such that i am trying to map this embedding the second imag e that i showed you was when i was trying to map this embedding and the last image that i will show is when i was trying to map these to embedding so my objective function was to create an image such that this embedding of the created image is the same as this embedding of the original monkey image right so thats what i am progressively trying to do as you can see as i keep going ahead i get more and more abstracter reconstructions and i dont really get the monkey back refer slide time fourfiftyseven and once i go to the last fc six or f seven layers i get very weird looking reconstructions and thats expected right because by that layer they have completely abstracted it out right you have just proba bly captured there is something like a nose something like eyes or some for here and there but you have loss the entire shape and other characteristics of the original image right from the deeper layer the construction would not be that good and thats k ind of expected right in spite of having the maximum no you could right a maximal operation is just another embedding which is that the compression there is much more because you have ignored the four entries and just taken the max value so it becomes harder and harder to do that but mean you i wouldnt call this as a reconstruction right what you see here is not except for the conv one and conv two layers the rest of the things were not really such accurate reconstruction so just says that you are losing a lot of information in that abstraction or maybe not i will do it next time"}
{"audio_filepath": "Data/Preprocessed/Motivation from Biological Neurons_10.wav", "duration": 407.0, "text": "so welcome to lecture two of cs seven thousand and fifteen which is the course on deep learningso we will talk about mcculloch pitts neuron thresholding logic perceptrons and a learning algorithm for perceptrons and talk about the convergence of this algorithm and then we will talk about multilayer network of perceptrons and finally the representation power of perceptrons solet us start module one which is on biological neuronssoremember during the history we had started all the way backin the one thousand eight hundred and eightys when we spoke about biological neuronssowe will just start there spend a few minutes on it and then go on to the computational models which is mcculloch pitts neuron so now this is a course on deep learningso we are going to talk about deep neural networksnow the most fundamental unit of a deep neural network is something known as an artificial neuron and the question is why is it called a neuron where does the inspiration come from  so we already know that the inspiration comes from biology and more specifically it comes from the brain because we saw that way back in the one thousand eight hundred and ninetys this term neuron was coined for neural processing units or the cells in our brain so now before we move on to the computational neurons or the artificial neurons we will just see the biological neurons in a bit more detail and then we will move on from there so this is what a typical biological neuron looks like so here actually there are two neurons this portion is called the dendrite so it is used to receive inputs from all the other neurons so that is the place where the input comes in then remember we said that i n one thousand nine hundred and fiftys we discovered that these neurons are actually discrete cells and there is something which connects them so that connection is called a synapse and it decides the strength of the connection between these two neurons so t here is an input there is some strength to the connection then once this neuron receives inputs from various other neurons it starts processing it so that is the central processing unit which is called the soma and once it is done this processing it will it is ready to send its output to other set of neurons so that output is carried on by the axon so we have inputs we have some weights attached to the input we have some processing and then an output so that is what a typical biological neuron looks like and let us see a very cartoonish illustration of how this works right how the neuron works so our sense organs interact with the outside world and then they pass on this information to the neuron and then the neuron decides whether i need to take some action in this case the action could be whether i it should laugh or not right whether the input is really funny enough to evoke laughter so if that happens this is known as something that the neuron has fired now of course in reality it is not just a single neuron which does all this there is a massively parallel interconnected network of neurons so you see a massive network here now the neurons in the lower level site so these neurons they actually interact with the sensory organs they do some processing based on the inputs so they decide whether i should fire or not and if they fire they transmit this information to the next set of neurons and this process continues till the information is relayed all the way back and then finally you decide whether you need to take any action or not again in which this case it should be laughter so that is how it works and when i say massively parallel interconnected network i really mean it because there are ten raise to eleven which is roughly one hundred billion neurons in the brain now this massively parallel network also ensures that there is some division of work now what do you mean by that is not that every neuron is responsible for taking care of whether i should laugh or not or not every neuron is responsible for processing visual data some neurons may possess visual data some neurons may possess speeds data and so on so there is this division of work every neuron has a certain role to play so for example in this cartoonish example that we took so there might be this one neuron which fires if the visuals are funny right whatever you are seeing is funny there will be one neuron which finds sheldons speech to be funny the way he speaks so that might be funny and there might be another neuron which actuallyfindsthedialoguecontenttobefunny andnowallofthispassonthe information to the next level and this guy would fire if at least two of these three inputs are funny so that means i have some threshold based on which i decide whether to react or not if it is really funny then only i laugh it otherwise i will not laugh so the neurons in the brain as was obvious in the previous slide are arranged in a hierarchy and i will take a more realistic example where we look at the visual cortex soisthisistheportionofthebrainwhichisresponsibleforprocessingvisual information right so as you see here you have our retina from where the information starts flowing and it goes through various levels so you see you follow the arrows and you will see there are several levels there is one level here then another here another here and so on right so it is again as i was trying to illustrate in that cartoon the information is relayed through multiple layers and then it goes all the way back to the spinal cord which decides that in this case i need to move the muscle right sothatiswhatisbeingdecidedhererightsotheinformationflowsthrougha hierarchy of layers and in this particular case i am going to focus on these three circled layers which are vone vtwo and ait right so these actually form a hierarchy and let us see what this hierarchy does right so at layer one you detect edges and corners so i am looking at you all i just see some dots and some shapes so that is what layer one recognizes i just recognize some edges and some dots and so on now layer two tries to group all of these together and come up with some meaningful feature groups right so it realizes oh these two edges actually form the nose these two dots actually form the eyes and these two edges actually form the mouth right so that is slightly higher level of processing that it is doing and then layer three further collects all this and leads to higher level objects right so now it is realizing all these things put together is actually a human face right so you add edges and circles or dots then you had some feature groups and then the feature groups combine into objects right so that is how this hierarchy processes so here is a disclaimer i understand very little about how the human brain works right and what you saw is a very oversimplified explanation of how the brain works right what i told you is there is an input a layer of networks which does a network which has many layers which does some processing and then you have an output right that is the very simplistic view that i gave you this is an oversimplified version but this version suffices for everything that we need for this course right this is not a biology or a neural processing course right so it is enough for this course so that is where we will end module one"}
{"audio_filepath": "Data/Preprocessed/Deep Learning(CS7015): Learning Parameters: (Infeasible) guess work_20.wav", "duration": 669.0, "text": "in this module we will try to learn these parameters and initially we will try to learn them by guesswork and i will show that that is actually infeasible that is w hy we need a more principled approach so we will keep the supervised machine learning setup in mind and now we will focus on this model and discuss an algorithm for learning the parameters which are w and b given some data using a giving appropriate function objective function so that is what we are going to focus on now sigma here stands for the sigmoid function the logistic function in this case when this sigma is actually the logistic function and now i am going to simplify this further so that it helps us to do a better analysis i am just going to consider the case where i am just in one input and the bias ok and also following the normal terminology in the literature this w naught from now on i am going to call it b because that is the normal convention b stands for bias so i have two parameters w and b which i need to estimate ok and this is my model for the movie example and the other change which i am going to make is instead of deciding whether i like or dislike which is one zero the setup that i am going to work with is that i am giving the critics rating and i want to predict the imdb rating so i am given a real value and i also want to predict a real value for no particular reason this just makes life easier for me for explaining a few things but the same thing or the same algorithm would also hold if you add a binary output right and you will see that later on in the course so here is a setup clear we just have two parameters w and b and we are going to assume that y belongs to real numbers it is a imdb rating and x also belongs to real number it is a critics rating now let us see what we are given as training is a set of points we are given some n training pairs and now we understand what this means that means for a lot of movie i am giving the critics rating and i am also given the true imdb rating for them of course in the two variable case this does not make much sense but just bear with me and now the training objective is such that whatever my function predicts which is a function of w x and b that should be very close to the true output that i know this is the function that i want to optimize now let me ask you this i am trying to tell you that i am going to give you an algorithm for training this network suppose i have trained this with two data points zerofive comma zerotwo and twofive comma zeronine right at the end of training i will give you some values of w and b let us call them w star and b star these are the final values of w which i have given w and b what do you expect from these values what do you expect at the end of training if i say now the network has learned what do you expect you are still going to the test case i am just talking about the training still we expect such that what happens if i plug in at the end of training if i plug in the value zerofive here what should happen zeronine so this is what you expect at the end of training if you plug in the value zerofive it should be very close to zerotwo the output and if you plug in the value twofive it should be very close to zeronine this is exactly what you expect and this is what training means ok fine in other words we hope to find a sigmoid function such that these two points lie on that function can you imagine a geometric picture for this what would happen actually how many if we can imagine it ok how many of you get it now this is what will happen right so you will get a sigmoid function such that these two points lie on that fair ok and that exactly means that when i plug in this value i will get this value and when i plug in this value i will get this value right so that is what it means so let us see this in more detail and now what we will do is our quest is for this w star and b star i will try to find this manually i will do some random guesswork and try to find this because i do not have any clear principle algorithm for finding it as of now so i will just use some guesswork so i will give my initial guesswork as w equal to zerofive b equal to zero for no reason i just picked up some values right and this is what the function that i got what does this mean this function an error so the sigmoid formula should be here we should have this sigmoid formula here so is this a good are you happy with the solution if i give you are you happy with this solution is this good bad ugly has to be something bad we will not call it ugly ok so why is it bad it is not passing through those points i will ask you a question how bad is it can you assign a number to it we are always good at qualitative stuff but quantitatively can you tell me a number how bad is this can you tell me a way of finding how bad this is i already told you in detail how to find that how bad it is the loss function right we have the loss function let us see that again and see if we can find out how bad this is so this is what my loss function is ok and i have two data points i will just expand it out fine now i will plug in the values i know this is zeronine and i will compute the value of f twofive i will plug in this and i will plug in this ok and this is what i get so this is how bad it is what did we actually expect it to be in the good case zero so this is not zero this is zeroseventythree so now we have a quantitative handle on how bad this is ok so let us keep this in mind and let us try to continue guessing so we want the loss function to be asked close to zero as possible we are not there yet so then i make a different guess i say let me try minus zeroten zerozero what happened now is it now good bad ugly now let us call it ugly right so it is worse and how do i know it is worse because i plugged it in to the loss function and i got a value which is greater than the value at which i was so i clearly know this is bad so now this is how my mind is working right oh i as far as w was positive things looked at least i was close to zero in the first decimal now when i made it negative that does not look good so let me just keep it positive and keep increasing it right so i saw zeroninetyfour and i also tweaked the b of it i have done complete random guesswork right now what happened good bad ugly better ok now what will you do what would your next case would be make w even more positive perhaps that would help and be even more negative and so on i can continue in this manner and actually get close very close to the solution so i can do this guesswork and find these values but it is still an educated guess right i am not guessing in the dark this is what is helping me drive towards those guesses and i am just looking at these values and making an educated guess right and that is the educated guess which i took that probably making w even more positive would help but this is still brute force in a sense right this is not something that you would want to do when you have one hundred one thousand parameters and so on right and one million data points and so on so let us look at something which is better than our guesswork algorithm ah so we are not there yet actually on the next slide i am still going to talk about the guesswork algorithm and eventually we will get to something which is better than the guesswork that ok so since we have only two points and two parameters what i can do is i can take all possible values of w and b right that is what i was trying i was picking up some values of w and b why just pick some values of w and b i will pick all possible values of w comma b right and i will fix the range i cannot fix pick it from minus infinity to infinity but i will pick a range i will say from minus six to six let me try all values of w comma b compute the loss and plot it right let me tell something about this error function because this is going to stay with us for quite some time so what you see here is something like a flying carpet this is colour coded red is bad red are the places where the error is high blue is good blue are the places where the error is low darker the shade of blue lower the error darker the shade of red higher the error so in particular if i look at this point what has happened is i have taken the corresponding value of w comma b right which is say minus four comma minus one right something like that i have plugged that value into my loss function and i got this as the loss function this has the loss value and that is what i have plotted for all values between minus six to plus six and minus six to plus six for w and b so everyone understands how i have constructed this error surface now this of course becomes and now what i can do is once i see this error surface i know how good this is the point where i need to be this is the darkest ah shade and this is where the error is the lowest so i can just pick a w comma b value which lies there this is fine for this toy example where you just have two parameters but this becomes untractable once you have more data points and many more parameters and that is what happens in most real world applications right so this is not a feasible way of going about things right and here again note that i have only taken the range from minus six to six i do not even know what will happen if i have to look at all values of w comma b right maybe there was something outside here right which was even more lower error or something right so i do not really know that so i cannot really use this so i need something better than this plotting the error everywhere and finding it order that is pure brute force or surrogate to this was the guesswork algorithm but which is again something we cannot do for if you have large number of parameters so everyone gets this that this is a way of finding the solution but this is not feasible right that is the only point i am trying to make and we look at the geometric interpretation of what was actually happening in the case of the guesswork algorithm with respect to the error surface so i had chosen some values of w comma b the first value that i chose actually gave me an error of if you remember it was some zeroseventythree or something like that right so that is the point then i decided to take a very random guess and my error actually increased so you see that i am actually climbing up on this error surface i have gone from a slightly darker shade of blue to a lighter shade of blue right and then i corrected myself and then kept moving in a direction where i was going towards the darker and darker shades of blue so what i was actually doing is i was trying to traverse the error surface and land up in the good regions which were the dark blue regions now what i want to do is i want an algorithm which will allow me to do this in a principled manner which is neither brute force nor guesswork so that is what we learn in that module"}
{"audio_filepath": "Data/Preprocessed/Hierarchical Attention_117.wav", "duration": 1187.0, "text": "so we will g o on to the next one whic h is hierarchica l attention so again something very popula r in nowadays become very commo n for va rious thing s so aga in not very difficult idea to understand so let us first look at the motivation for this and then we will look at the solution so consider a dialog right today everyone is interested building chatbots every second start up wants to build their own chatbot and every second startup out of that they wants to build for the agriculture domain or the banking domain or the healthcare domain so here is what a typic al dialog looks like right this is of course not for any profound purpose this is but you can see this is an important dialog right very relevant and very important so this is what a dialog looks like so let us try to break it down into the kind of entities that we deal with so can you tell me about a dialog what is a dialog it is a dash think in terms of things that we have discussed so far student refer time oneseven sequence good right again the safest answer is sequence from now on no  it is only for one lecture it is a is it a just a sequence or sequence of sequences right so it contains a sequence of utterances so each of these lines here is an utterance and each utterance in turn is a sequence of words right ok so what we hav e here is a sequence of sequence as input and this is very common in many many applications right so can you think of an encoder for such a sequence of sequence rnn of rnns good that is the answer right so we think of a tw o level hierarchical rnn encoder so first leveller will encode the utterances ok let me ask you few questions is there is a mistake in a diagram should this be connected yes no maybe do not care ok second question i will write some parameters he re right what is our notation w u this is u right and this is w right is it fine if i have a dialog which contains one hundred utterances what will happen that is a practical problem but more conceptually what is wrong here what is each rnn trying to do  i encode a sentence encode an utterance so why should it be different for the first utterance second utterance third utterance and so on right all these rnns should be the same does that make sense the u one is equal to u two is equal to u three and w one is equal to w two is equal to w three ok is that fine everyone agrees with that so now can you tell me if there is a correction should be there or not conceptually what is each rnn doing encoding student refer time twofortyseven one sentence right then why should it be connected to the previous sentence but then if you do not know all these sentences then how will you predict the utterance the next response rather what is missing here what kind of a what was the title of this module so what is miss ing where is the hierarchy right so what we will have is this right so each of these green guys is presentation for one utterance in your input in fact the red red guys sorry the red guys are the representations for the utterances in your dialog and then the green guy is a sequence of utterances right so remember you have the sequence of sequence of words so the red guys are the sequence of words and the green guys are the sequence of utterances does that make sense ok how many of you get this please raise your hands ok good so and now what would the decoder be you have a hierarchical encoder what could the decoder be what is the decoder have to do it has to produce a everyone student sequence of words sequence of words so what kind of a decoder will you use just an rnn not a hierarchical rnn the input is hierarchical why should not the output be hierarchical how it would look unbalanced right the diagram will not look very neat right if you refer time threefiftyseven do you need a hierarchical and decoder no right i mean just a simple decoder because the decoders has to produce a sequence of words so it will take something from the encoder what will it take from the encoder in the normal encoder dec oder paradigm not the attention based paradigm what will be the input to the decoder the last dash vector your options are state that is very safe answer here the last state vector what will that means your options are orange blue green and red st udent green green the last green vector that is what the input is going to be right so this is what is going to look like this is option one what is option two feed it to every time step that is what option two is going to be ok so that is you have yo ur hierarchical encoder decoder network what is missing here attention ok so let us look at another example consider the task of document classification or summarisation what is the difference between two in classificati on what the what were the decoder be feed forward neural network with the softmax what would be decoder be in the case of summarisation students rnn rnn good what is the document sequence of sequence not sequence of sequence of sequence it can be a sequence of sequence of sequence also right i did not think of that then it could be a sequence of sequence of sequence of sequence ok let us look at the not so funny case which is sequence of sequence what is the sequence of sequence of it is the se quence of sentences which in turn is a sequence of words which in turn is a sequence of character we will not go there we will just keep it till words so it is a sequence of sequence so again you need some kind of a hierarchical encoder here right so will encode each sentence then you will treat the sentence sequence as a sequence encode that and then you will pass it to the classifier now think of this problem right now if we want to do document classification how would you go about it actual ly you want to classify whether this is a politics or sports or health or whatever refer time fivefiftyseven i think in terms of attention what would you do actually first we will find the important words in the sentence to find the important words you will have to read all the students refer time sixten if you want to find the important words you will have to read all the sentences so what will you do first find the students word in sentences word in sentences and then important words within the sentence so what kind of an attention mechanism do you need student hierarchy hierarchical right so let us look at this so first let us look at the our data model paradigm so what is the data given to you i given a document and the class table for the document and your first thing is the word level encoder which looks like this can i will not explain what this is i will just expect you to know what this is why do i have two indices for h what is i and what is j  i is the dash id sentence id and so it is the j th word or the i th sentence right that is what i am encoding and how many sentences am i encoding the number of sentences in the document ok and what is the second encoder so diagram is absolutely clea r but the equations are not the diagram is absolutely clear right there is no just need to map the red blue orange green guys to the equations ok so let me ask you this what is h i j  no in the diagram blue red orange student refer time seven twentythree what is w i j  how did you write the rnn equation time step t is equal to rnn of t minus one and the input at time step t right what is the input at time step t here student word word right probably this was not a good choice maybe we should ma ke i t x i j  w i think we might get a confused with the weights we should no t but they are so fine so w i j is actually the input word what is h i j minus one now tell me what colour is w i j it is like an iq test at which colour map it w i j maps to which colour students orange orange good and h i j students blue blue but what about the red that is which colour i mean sorry not which variable students refer time eightfifteen hi t i that is the last state of every sequence right t i is a length of the i th sente nce right ok now what about h i two  the green guys right and what is this h two k students refer time eighttwentynine the last green guy this guy ok is it fine and then the decoder is just a softmax we do not need to go with that and loss and everything is fine so this again whatever it is we should always be comfortable and writing the end to end equations from x to y right and you can write it in this case now let us make it a bit inte resting how would you model attention in such a hierarchical encoder decoder model how many attention functions would you need two one for attention over sentences the other for attention over students works works ok can you think of these equati ons not a very big stretch from what we have done already right i mean at level one it should be straight forward at level two just ignore level one how many if you can imagine the equations it is not very hard i am not joking i am i mean just think about i t and the level one should be straight forward because that is just the same as ok so first we need to attend to the most important words in a sentence and then we need to attend to the most important sentence in a document ok let us have see how to model this so we have document again the same input then you have the word level rnn ok now what be the word level attention equation look like i am looking for the attention equation for words what are the indices going to be j i j t ok what is i j t that is i have put as superscript in w this is the wor d level attention so at the t th time step i want the importance of the student refer time tenfour j th good right what would that equat ion depend on students the word the word should be straightforward right it should depend on oh but oh ok sorry this is only for this guy right ok so you have focusing on one of these so you trying to find the importance of these three words rig ht which are there in the first sentence so you have computed h i j that means you have computed all the representation for each of these word that means you have computed the first three blue vectors that you see ok and then you are computing the a ttention no so this is oh so instead of having it here this is how we have been writing at right student refer time tenfortynine ok i so sorry i should have check this so read this as u i j is equal to or let me just explain it so remember this i s a vector and we wanted to do this operation to make it a scalar right so u w is that parameter which was getting multiplied earlier so we had this attention equation as u w transpose tan h of something right so now that u w has been removed from h ere in equation two and has been added as exponent to equation to the alpha equation does is that ok how many of you completely confused please raise your hands how many of you understand this completely once can the sum the one ok aa shit what did i d o ok let us just see if i can still salvage this ok let us go one by one so what is let me just delete some of these things let us try to write it on our own right so this is what we are trying to do so i will i am ignoring the sentence id rights so this is sentence one two and three so let us just focus on one sentence and the same equations we hold for the other sentences also ok so first of all the attention weight would depend on what it would depend what are we trying to pay attention to stude nt words words so it should have word in the input right what else can you at have in the input student previous previous state unfortunately for this problem do we have a previous state student refer time twelvetwentyseven no we just doing one predict ion right there is no rnn here we just the feed forward network do you have any s t minus one in the output that we have put here so t his was the importance of the j th word at the t th time step so j belongs to the input and t belongs to the decoder ri ght in this case does decoder have multiple time steps there is only one time step of the decoder right that is the problem which we have run into but let us assume instead of classification we are trying to do summarisation that means we are given th is document and we were trying to generate summary of this and let us say the summary was the following ok so this is the summary that you are trying to generate from this document ok and now this summary has three or four time steps if you count exclamation as the last time step now how is it going to be what is the decoder going to be in that case rnn right and the decoder will have some k time steps ok now at every time step at a given time step t what am i trying to do in next assignment try to de velop a better eraser for this ok so we want to compute when you compute the attention for a word the j th word in particular at that t th time step right and we have for a minute understood that we do not have a feed forwa rd network at the decoder we have a recurrent neural network at the decoder because we are trying to generate a summary ok we are trying to generate a seque nce at the output so at the t th time step we are interested in understanding which of the docum ent was to pay attention to ok so now that is going to be a function of what and i finded a bit irritating for the want of a better word that at least one input to this function should be straight forward right what is it student the word the word that you are trying to learn how much attention to pay to right so that should be very straightforward so that should be w one j because i am considering the first sentence right now ok is that fine for the first sentence i am trying to find out which a re the words which are important what is the second input that you could put in it should depend on the index t right so it should be the previous state of the decoder ok and then of course i will have a this is again actually not alpha but e right and then you get how do you get alpha from there how do you get the alphas student softmax softmax is that fine ok so alpha will be some softmax of the es is that ok fine so this is for the word level now the equations that are written on the slide are slightly so the equation has slices slightly differently written so let me just go back and write our own equation and so we want to write an equation for this what was our equation ignore the equations on the sl ide it was something like this v transpose tan h of w student s t s t one u wonej student b b ok now imagine that your decoder is a feed forward neural network what will be missing in that case there is no s t minus one there is no previous state o f a decoder because we just want to make a prediction once by paying attention to all the important words and sentences in your document right so which part will go away w s t one ok fine now the other thing that you are doing is alpha was actually or ot her alpha i j is actually exponent of e i j divided by summation of other k sorry alpha j t e j t e k t is that fine it is just the softmax equation is that ok right now the only thing that you see different and these two equations here is first yo u do not have the w s t minus one because the decoder only has one time step and second we have taken out this v transpose from here and instead we have added it here is that ok does that make sense this is just different way of writing it so again you write the attention equation for the words now ok now what about the sentence now first of all earlier what were we using for the green guy what was the representation for the green vector it was the what was the green vector in the absence of atte ntion what was the green vector h t i right the last time step of sentence one is it fine everyone with me please raise your hands if you are with me ok now what would it be it would be a dash sum of w vectors a weighted sum attention weighted sum r ight so that is exactly what this equation is capturing so what did you saying is there is representation of sentence i is a weighted sum of the representations of all the words in that sentence is it ok so that we will get a representation for s one s two up to s capital k all the sentences that you have now what do you want to do for the second level what do we want we want to compute the import ance of that sentence for the t th time step right so let us call that beta so i am interested in beta if we need to really read out this i said again alpha is being used in both the places right so you want to find out the importance of the j th sentence at the t th time step what is it going to be a function of one is a sentence representation what is th e sentence representation given by student s j s j and what else the decoder state at the previous time step right does the decoder have a previous time step state here no so it will just depend on s j and that is exactly what this equation is cap turing right and again the same trick that i have added this extra parameter to the exponent is that fine and the final representation being fed to the feed forward network is a weighted sum of the sentence representations ri ght so this again has to b e s i  s i ok i really sorry about this but i am pretty sure that once we correct the slides and then you go back and look at it should be clear right it just two sets of equation one set of equation sorry os sorry that is corre ct sorry so this the idea is there are two sets of attention mechanisms for each you will have your own set of equations the basic form if you can work out what the f attention would depend on the actual form would depend from would differ from paper to paper or the toolbox to toolbox that does not matter so much you just need to know that you have these as the input you are going to add some parameters to every input that means you are going to do linear transformation and then you just need to m ake sure that alphas eventually turn out to be scalars right that is why you will have this additional vector getting multiplied at one point"}
{"audio_filepath": "Data/Preprocessed/Momentum based Gradient Descent_35.wav", "duration": 1080.0, "text": "in this module we will look at momentum based gradi ent descent refer sl ide time zeronineteen so what were the observations about gradient descent that it takes a lot of time to navigate regions having a gentle slope so what is the practical impl ication of thi s in practice why it what does this need to what does this mean r ight it takes more time  so remember we had said this max iteration equal to one thousand now if you a re initialization happens to be such that you are stuck in this large flat region then those one thousand iterations just keep moving around that flat region right y ou wi ll not enter into one of the valleys and valleys is what you are interested in right because values is where you wi ll have some minima for your function right so if you have a very gentle slope then for one thousand iterations you will keep moving around that gentle slope right that is why this has a practical implication now this was because the gradient in these regions were small can we do something better that is the question right so yes we can and we will take a look at momentum based gradient descent refer slide time onefourteen so here i s the analogy which i give my tas have heard this at least ten times  so i will just repeat it the eleven time for them  so i hope that is the one which i want to use here yeah ok  so now suppose you are stan ding at the v elachery gate and you want to go to p hoenix market city something that all if you can relate to today  so you want to go to p hoenix market city and you ask the security guy at the gate that where do i go right so he will say take a left n o take a right so i am slightly dyslexic actually i have a left right dyslexia  so take take a right ok  so you will say he has told me to move right  but you would still be a bit cautious right we will just keep moving slowly in that direction th at is how we find ask for directions you keep moving slowly in that direction now one hundred steps later or one hundred meters later you find another guy and you ask him or her where is phoenix market city he again points to in the same direction keep moving left ri ght so now you wi ll what will happen you wi ll increase your space and then you ask again someone when you read the signal where it is and he a gain points in that direction what will happen mov e even fast so what is happening here if a lot of peop le are pointing you in the same direction you better start taking larger and larger steps in that direction does that make sense that is how we find directions and move around  so just like a ball gains momentum as it goes down a slope right it i s con stantly moving in that direction  so it starts moving faster  so now can you tell me a way of incorporating this  i have be en moving in a certain direction these directions are nothing  but the gradients and now at this point someone asked me again to m ove in the same direction what should i do s tudent take a bigger step t ake a bigger step  so can you think or try to imagine how would you do this mathematically s tudent refer time threeten so it i s probably there are a few ways to do it  so let us see  so what i a m doing here is this is my current gradient right so i asked that guy at the signal he asked me to move in that direction  so that is this direction and this is all my history whatever i did till step t minus one ok  so now what i will do is i will  so earlier i was moving like this this is what my update rule was wt plus one is equal to wt minus in the direction of the gradient right i will moving in the dir ection opposite to the gradient now what i have is in addition to that i have this gamma update t minus one so  that means whatever i had done up till step t minus one i wi ll also take that into account  so i wi ll end up taking a larger step is that clear if it i s not clear it wi ll become clear on the next slide refer sli de time foureighteen so let us see what this means right so it basically means that in addition to the current step also look at the history there are three guys who earlier pointed you in the same direction  so maybe this direction makes sense right so start accumulating that and move faster refer slide time fourthirtyone so let us just break this down and see right so this is what the update rule is sorry this is all my update s and this is the update rule  so at time step zero my update is zero be cause not started yet at time step one this is what it will look like right and this is nothing  but just move in the direction of the opposite to the gradient because this minus sign will come later on right in the next equation now what will happen u pdate two so its gamma times update one plus the gradient at the current step  so remember here everything is positive i a m adding the gradients because my final negative sign is going to come in the next equation ok  so do not get confused with that eventually i a m going to move in the direction opposite that opposite will come from this negative sign so what is happening i am moving in the current direction plus a fraction of the direction which was pointed earlier right ok then does this make sen se  so can you tell me in general what is happening here at the t th time step what is happening what kind of average am i taking weighted average  but it i s a dash weighted average this is an exponentially weighted average ok  so let us look at this right so when i am at step four i have most faith in the current gradient right and this gamma is always i wi ll just set it to less something less than one right so i have a fractional trust in the previous gradient even smaller trust in the previous guy and even smaller trusts in the previous guys  so i a m taking an average of all my gradients  but it i s an exponentially weighted average does that make sense m y maximum faith lies in the current guy and then decaying faith in the previous guys and as i move further and further away from the last guy that i checked right i will give lesser and lesser weight age to that  so everyone understands what is happening here anyone who has a problem is just raise your hands if you understand this good so in g eneral this is going to be the formula and you see that as as i form problem here no as t is larger this fraction is going to become smaller and smaller right so you are first the first step that you take will have lesser and lesser weight age as t inc reases everyone gets this fine refer slide time sixfiftyseven so now this is the code for momentum based gradient descent i wi ll just give you a minute to stare at the code and see if it makes sense  so this much part is ok you a re just computing the gr adients with respect to all the points right and now we are keeping this running sum ok which is the previous gradients and the current gradient right and then you are just subtracting that running sum now this looking black curve that you see here t hat is gradient this this guy ok this black curve that you see here that is gradient descent when i have run it for around one hundred iterations now i a m going to run momentum base gradient descent and each click is going to be one step ok and i want you to o bserve what happens ok  so slowly a red curve will start appearing on the figure initially it will not be visible  so do not worry there i s nothing wrong with your eyesight one how many if you already see the red part i see it two three four five six no now y ou can see it as is nothing great about seven eight nine i want you to observe something here eleven twelve thirteen fourteen came back right so gradient descent i ran it four hundred iterations it was just stuck here right this was a point and i ran this for less than like around fifteen or twenty is what we counted right and s o already entered into the valley so momentum base gradient descent is good you see that wicked smile on my face and you know it i s a trick question  so we are moving fast right refer slide time ninetwo e ven in the r egions where the slope was gentle right that is the beginning of the beginning of our trajectory right this was the gentle region even that i was very quickly able to navigate right within five to six steps i was away from that part right so even in the r egions where the slope was gentle i was able to move fast  but is moving fast always good so would there will be a situation where momentum would cause us to run fast ago same thing now instead of walking you are in a car you ask the person at the secu rity whether i should go there he says yes go in the right direction you keep moving there someone else you keep accelerating what will happen eventually you will go fast phoenix market city then what will you do s tudent t ake a take a u turn c ome back again while taking a u turn what will you do s tudent refer time ninefiftyseven o vershoot and come to the signal and then go back again right so you see this you wi ll end up taking a lot of u turns  so let us change the input data a bit and see what h appens to momentum based gradient descent refer slide time teneleven so this is what my data looks like now so this is not what my data looks like this is what my error surface looks like  so earlier we had this error surface something like a flying carpet now i have a very peculiar error surface this is again for the two parameter problem right w comma b  that means i want to learn a sigmoid function where i have these two plateaus at the top the dark red regions that you see and then a very sh arp valley can you tell me how i would have come up with this kind of an error surface what are the points that i would have chosen just hold on to that part so i have this kind of an error surface fine the error is high on either side of the valley now could momentum be detrimental in this case yes no maybe i do not care i do not care fine refer slide time elevenseven so let us see this is the is this the two d equivalent of that three d surface everyone gets it i can perfectly verify that you get it everyone gets it i wi ll assume right so these are the very high plateaus where the error is very high very sharp and narrow valley where the error is low so now again this sorry looking black curve is what i have done with gradient descent after s ome one hundred iterations or something now i a m going to run momentum based gradient descent and you have to help me understanding what is going to happen again you wi ll soon start seeing that red curve appear one two three four five six what will happen now it i s already fa st that is known it was that black curve was after one hundred iterations or  so it i s fa st now tell me what will happen s tudent refer time twelvezero h e will go out is actually almost come out of the valley right it i s almost at the top of the valley now wh at will do take a u turn now what will i do again take a u turn now i will keep doing this i will take now smaller and smaller u turns and it will converge right so what happens here is because of this speedy movement and which is very analogous to that car movement which i described this overshoot your goal you will have to take the u turn come back if you are again careless you wi ll have to keep taking these u turns  but you wi ll finally end up at the location that you want righ t it takes a lot of u turns be fore converging despite these u turns it still converges faster than gradient descent right because gradient descent can just not move at those gentle slopes right it just can no t move from there because the gradient is almost zero because th e slope is flat right and it just can no t move  but even with this lot of u turn and lot of rework after one hundred iterations momentum base gradient descent has reached an error of almost zero w hereas gradient descent is still stuck at the plateau at an error of zero thirtysix ye so see you have reached the minima now s tudent y e n ow you will be navigating there right  but you know that now your loss is very slow low  so you could end that right you know that your loss is very close to zero  so you could have a condition that once you have reached something very close to zero you could end that even if you are making these very small movement s now you could just stop there s tudent b ut in the plateau regions is also zero b ut the loss is high right so if the loss is high and you a re not moving you cannot stop but if the loss is low and you a re not making movements you can just stop there right so you can just end you can define that as your convergence condition refer slide time thirteenfortyfour so let us look at we will com e back to three d now we look at a three d visualization and a very different interpretation of what is happening i really want you to understand what exactly is happening in this example which i had picked up right refer slide time thirteenfiftynine so this is what the three d surface looks like view from a different angle you have these two plateaus and the very sharp valley now this is the corresponding sigmoid function where i started with  so what i a m trying to tell you is that this is a sigmoid function corres ponding to w equal to six oh no sorry w equal to two and b equal to six this is the sigmoid function that i got once i plug that value  so sigmoid is one over one plus e raised to minus w x plus b and i have plugged in the values of w and b and plotted it for all the values of x and this is the sigmoid that i got ok  so that is my starting point is this good how do you define good or b ad s tudent refer time  fourteenfortythree w hat do you expect at the end of training it should pass through all your training points and these are my training points ok is it passing through them no its way off right ok so now let us start this momentum based gradient descent and what just see how my sigmoid function changes  so right now i am on the gentle slope even that momentum base gradient descent i t i s going to be fast  but not dramatically fast because still building up the momentum so it i s you see that these sigmoid that i am drawing here they are almost indistinguishable from each other i ha ve already drawn three sigm oids here  so i will just go back  so there was this initial guy then i draw drew a red one then one more and then one more  but they a re all very close to each other now keep viewing both these sides in parallel what happens here on this figure and wh at happens to this sigmoid ok and i will ask you questions  so still i a m moving a bit slowly because i a m still building the momentum right it takes time to build that moment now i ha ve slowly started building the momentum my sigmoids have started mov ing towards where they should be everyone gets this what is happening here ok now tell me what will happen as i enter the valley i am almost entering the valley what will happen i ha ve gained this momentum now  so my w comma b values are going to ch ange much faster now  so what will happen to these sigmoids they no longer stick to each other we will start seeing a difference they a re already moving away from each other ok  so that is what is happening to the function ok now you see even faster c hanges ok now what will happen i have entered the valley this is how my sigmoid looks at this point now tell me what will happen s tudent refer time sixteenthirtyfour i t wi ll go fast what will happen to your sigmoid how many of you know what will happen to the sigm oid ok i will tell you what happens and then it will be obvious right so now i a m entering the valley all of us know that i a m going to come out of the value of the other side right so let us see what happens when i come out of the valley fr om the other side the sigmoid changes that is why you have this situation that your error is high on both sides right because on this side you have these kind of sigmoids on the other side you have the other sigmoids and somewhere in between lies the so lution where does the solution lie at a very flat sigm oid right so now i start this is where the oscillations will happen  so notice what will happen to the sigmoids they will toggle between these two orientations ok just see what happens to the si gmoids you see it again moves keeps moving keeps moving it keeps oscillating around the solution and then finally you reach the solution  so you see that should i repeat this so when i a m on one side of this valley i have one kind of sigmoids righ t now when i move to the other side of the valley i have this ot hers kind of sigma and take a u turn  so when i u turn take a u turn i again overshoot and go to the other side and this keeps happening and i keep toggling till i reach my final solution so these are all the oscillations that you are seeing  so can you visualize this what is happening do you understand all the s e relates to the actual function that you are trying to learn  so that is why we will end this module this was on momentum b ase gradient descent now we will see a n esterov accelerated g radient d escent"}
{"audio_filepath": "Data/Preprocessed/Beating humans at their own games (literally)_7.wav", "duration": 79.0, "text": "now since i mentioned rl so we will go on to the next chapter which was now becoming much more ambitious with what you can do with deep learning and people started beating humans at their own game quite literally so there was this starting with atari games in two thousand and fifteen where resources from deep mind show that you could train a deep neural network to play atari games and do much better than what humans doso that is something that they were able to show on atari games and then people started looking at other game so then there was this go and this popular tournament and which a lphago which is deep reinforcement learning based agent was actually able to beat the reigning champion at that time one of the best players of go at that time thenevenatpokerweresomethingknownasdeepstackwhichisagainadeep reinforcement learning based agent which is able to beat eleven professional poker players at this game then other games like defense of the ancients since on which is a much more complex strategy based game where again deep reinforcement learning based agents have shown a lot of success in beating top professional players on this game"}
{"audio_filepath": "Data/Preprocessed/Unsupervised pre-training_70.wav", "duration": 1452.0, "text": "so with that we go on to the nex t module in whic h we will talk about unsupervised pre training so this work which i am going to talk about they trying to understand what has changed since the late ninetys or the early two thousand how did deep learning become so popular despite this problem with training them ri ght this problem was there so what happened to them solve it right and this field actually got revived by this seminal work by hinton and others in two thousand and six so let us see what that idea was so this is the idea of unsupervised pre training in the original paper they introduce idea in the context of something known as r b ms which we will do in the last thirtythree percent of the course b ut we could do the same with auto encoders which we have already done so in this lecture i am g oing to talk about t his idea in the context of auto encoders so consider the deep neural network shown in this figure so the a module name and the idea was unsupervised pre training so that itself is a giveaway of what is g oing to happen ok so suppose this is the deep neural network that i have designed for a particular classification task so what it is doing is this taking an input which is the red colored neurons that you see at the input it has four hidden layers that means it is four layer deep and then you have the output layer which tells you whether positive or negative right that is the network that i have and i know that this is hard to train such a network the loss will not converge and i will not get anything m eaningful so what these guys suggested is that forget about the supervised criteria that you have that means you are trying to minimize a classification loss just forget about that just take the first two layers of this network ok which is x and h one righ t so you take the original input x you feed it to some transformations and you get the hidden representation h one and now try to reconstruct x from h one what is this student a uto encoder auto encoder ok what is the objective of the auto encoder s tud ent refer time twonineteen it is exactly this for each of the m training examples look at each of the dimensions of your input and minimize the square difference between the actual input and the predicted input right is that fine that is what an auto enc oder does so this is what they suggested ok so right now i am not telling you why this makes sense and all that that is what we will do later  r ight n ow i am just telling you the trick then we will analyze by that trick works and why is this objective unsupervised student refer time twofiftytwo because we are not using any labels we just giving an input and we just reconstructing the input we only have xs we do not have ys of course eventually we will use the y but at this stage when i am calling i t unsupervised pre training i am not using the y now at the end of this what would happen yeah what would h one learn student refer time threenineteen it will learn an abstract representation of x was that our original task wh at were we interested in student refer time threetwentynine in the classification task but we are doing something very different why we will see ok now guess what would the next step be  d oes this make sense now at the end of t he first unsupervised pre training i have ensured that h one which is this layer has learned some abstract representation of the input right and that i know from the auto encoder i mean the auto encoder which we have learned earlier right that at learns an abstract representation of the input now i have this so that means given an input i know how to compute an extract representation and i am also sure that it captures the important characteristics of the data i will just repeat this process i know th at i have four layers in my original network so i will now take h one try to compute h two and then reconstruct h one from it so the in effect what am i doing in plain english learning and even more student refer time fournineteen abstract representation of the input h one was already one abstract representation now from this i am learning an even more abstract representation and does the objective function makes sense right all i have done is replaced x by h one right in both these places the rest of it is the s ame for all the training examples for all the dimensions and throughout i am assuming that we are n layers i mean sorry n neurons and every layer including the input layer now what would the next step be fix the weights in h one layer fix the weights i n it is two layer and now try to reconstruct h two from this h two right and in this way we will continue and learn all the hidden representations does that look ok right so at least this much we believe it because we know that auto encoder works and you are just using an auto encoder and we are using it incrementally from every abstract representation learn an even more abstract representation now at the end of this what will i do what was my original task student classification classification so wh at will i do student refer time fivetwentythree what is a network that i have when i finish this unsupervised pre training student refer time fivetwentynine no tell me of the diagrams that you see on the slide how much of the network would i have right everyt hing except the green output layer right because the last step would be take h four or sorry take h three and reconstruct h three from it and in the process learn h four right is that clear so i would have learnt till that point and now what i am going to do is some thing very simple i will after this layerwise pre training is done i will add my output layer now all the weights in my network for every layer have been initialized and they have been initialized in a way that that layer le arns a good abstract representation of the input right that is the thing that we have achieved at the end of unsupervised pre training that every layer has learned and more and more abstract representation of the input right now i will keep all these wei ghts initialized to whatever i learned in the pre training setup does that make sense so that means instead of taking this big network with the output layer and initializing the way it is randomly i am just going to use whatever weights i learned using the unsupervised pre training ok so can you tell me what has happened in terms of the error surface and so on o r my movement in the w b plane or in this case this very high dimensional w plane i have reached some configuration for the ws where i kno w that each of these layers is a good meaningful representation of my original input right is that a fair statement in english how many of you agree with this ah anyone has any questions at this point one layer weights that is what you do in answer bec ause if you train all the four then you are again entering the same prob lem which you had earlier right you cannot back you cannot back propagate through all the four layers because now it is a deep network and we know that does not work so at every layer you fix whatever you have learned so far and at a time you are training only one layer so that is one interesting way of looking at it right you know that the deep neural network with four layers was not trainable so now we have reduced it to one layer at a time i knew that one layer at a time works right is that fine now i will add the output layer and what will i do train the weights of the student o utput layer refer time sevenfiftythree i will not just do that i will fine tune the entire network that mea ns i will train the weights of the output layer and i will also fine tune the entire network but now i am contradicting myself i just gave an answer to him that again i am doing this deep training and i know that deep training does not work but this a ctually works do you get the difference right one is that when i start from i take this big network i start from random weight initialization and try to train it that is the story from one thousand nine hundred and eightysix to two thousand and six that in most cases these networks did not converge so no w in two thousand and six we came up or someone came up with this idea of unsupervised pre training where you train the layer network one layer at a time y ou do up till the last layer now you add the output layer and then fine tune the entire network that means back p ropagate over the entire network is a set up clear to everyone how many you understand the setup now again when i am doing the last step which is known as fine tuning i have to back propagate over the entire network because i am saying i will adjust all the weights but suddenly this works as compared to starting from scratch y ou see the problem and you see why this is important then because this has now given you a way of training deep neural network i still not told you why it works we will delve i nto it but not really give any concrete answers because concrete answers do not exist but we will at least try to get some intuitions behind why it works so you get the setup now that this is what was happening till one thousand nine hundred and eightysix to two thousand and six and now with this idea suddenly deep neural networks were being able to train well so in effect what we have done is we have initialized the weights of the network using the unsupervised objective right so now initial starting with random weights we have some weights whic h cater to the unsupervised objective that we had and the unsupervised objective was us layer wise reconstruction so that is what has happened in plain english is that fine everyone gets that now the question is why does th is work better and i give you two options and i want to think about both these options ok is it because of better optimization or is it because of better generalization no that is not an option but i of course we will relate it to that but given these two i want you to think whether there is any difference between these two statements or not that is the first thing i want you to see how many if you get the difference between these two statements not many why is it so what is optimization deal with dash dat a or dash data student refer time tenthirtyfive the answer you can give dash right dash one data or dash two data what is optimization deal with student t raining data training data optimization remains on training data what does generalization depend on stu dent i t as zero it as zero so you get the difference between these two questions fine so let us try to answer this again here right this is two thousand and six to two thousand and nine period that i am going to talk about there are some answers and just bear with me i will give you those a nswers some of them will not look very convincing but what happened after that or as a result of these investigations that is more important right whether these answers make sense or not they will make sense to an extent i am not saying that we will jus t be bluffing but it will not be very convincing because there is no theory behind it right so what is convincing if i give you a proof that this less this is equal to that right then if we give you a proof and everything you do not have any other quest ions that is not what i am going to give you i am going to give you some intuitions because that is all these existing works from two thousand and six to two thousand and nine had and then i will make a commentary on that which will lead us to some other things  so just bear with me for a few minutes right student refer time elevenfortysix that is the optimization problem if that was the case the i will just come to that that is what i want to talk about ok so it is so these are the two questions that we are dealing with right and th e answer is depends so we will see what it is so let us first examine the case when it is because of better optimization so let us first understand what is the meaning of this question when i a sk is it because of better optimization then the question that i am asking you is that the first set up where i was trying to train everything from scratch compared to the second set up where i had this unsupervised pre training is it that the optimizat ion problem becomes easier in the second set up now if the optimization problem becomes easier what do i actually mean by that that i was able to drive the dash to dash student l oss to zero loss to zero right so is it that this is the optimization problem that we were interested in so is it the case that in the absence of unsupervised pre training we are not able to drive the loss to zero for the training data and hence poor optimization right that if you do not do this unsupervised pre training even for t he training data we cannot drive at loss to zero that means our optimization problem itself is not working properly right i mean the problem is fine but the solution is not good you get that do you understand what is the subtle meaning of this how many i f you get this so let u s see this in more detail right so the error surface of the supervised objective of a deep neural network is highly non convex it looks something like this or even nastier than this and in particular it has many hills and plateaus and valleys we saw this even in the toy examples that we were dealing with right and given the large capacity of deep neural networks it is still easy to land in one of these zero error regions on what basis am i making the s tatement which theorem student refer time thirteenthirtytwo universal approximation theorem that is what the universal approximation theorem told us in fact there is a study the paper which has been cited it showed that if the last year has a very large capac ity then you can drive the loss to zero even without pre training do you get the meaning of this what does is mean so i have the input i have a series of hidden layers what do i mean by the last layer has a lot of capacity what do i mean by that it has a lot of dash student p arameters parameters now how do i create these parameters i will just grow the size of the last hidden layer right and using that then i will predict this one y so so that is how i could increase so that is exactly what they di d they took a very deep neural network and made sure that the last layer was given a very high capacity and then they showered that even if you do not do an unsupervised pre training you can still drive the training loss to zero right so this was hinting that maybe this is not an optimization problem this is something it is still not very conclusion but we will just go with these studies we will just all i am saying is that do not shoot the messenger this is what the study says i am just relaying it back to you right and they will have questions on these which will try to address but if the capacity of the network is small then the unsupervised pre training helps so if you do not have these large capacity networks but you have very deep networks in that case unsupervised pre training helps and this is all empirical observation right there is no proof which says that given a capacity k with so much error bound i can guarantee that the loss would be epsilon within the zero loss and so on it nothing like that that is what it should have been ideally the case in which case life is much easier for me but that is not the case this is just an empirical study as are most of the studies done in the period of two thousand and six to two thousand and nine so that te lls us something about what optimization means and whether this was an optimization problem or not so let us look at the other question is it because of better regularization so what does regularization do or you gave the ex act answer it constrains the weights to lie between lie in some regions so it does not allow the weights a lot of freedom right and so you know what l one regulation does it constrains the weights to this box and l two regularization constrains us to thi s circle why no why this i know this but why student refer time sixteenone in why the circle i am pretty sure most of you do not know what you are saying but you are saying the right answers but anyways i will test this in the quiz so i have given yo u another quiz question on camera so yeah so a prevents a loss from taking large values so indeed pre training also constrains to the way to lie in certain regions of the parameter space why am i making this statement what is the meaning of the statement so i told you that what regularization does and from there i am making this jump and saying that even with pre training the same thing happens that your weights are actually constrained to certain regions of the parameter space why am i making this statement  and what are these regions that the weight is constrained to think l theta think omega theta any regulation is of the form l theta plus omega theta let us see so it constrains the way to lie in regions where the characteristic of the data are captured well that is what unsupervised pre training does it is trying to train the network in a way that each layer actually captures the important characteristics of the data and this is based on our understanding and bel ief in auto encoders so you could actually think of this that the unsupervised objective that you had for all these layers that was actually omega theta you are first trying to optimize omega theta so in a normal regulation problem you put l theta and omega theta together and then you try to balance them but here you have done it slightly differently you first gave it omega theta which is the lost of reconstruction and you asked it to minimize this loss across for every layer student refer time one sevenfortyeight no is this fine tuning so now what that means is that see remember that this is a very high dimensional region where you initialize makes a lot of difference so with this unsupervised pre training you are at least ending up in reason so you could think of it as a constraint that ok move wherever you want to but start from here which automatically means that i have i mean i have how to it is some other regions in that parameter space you get that student refer time eighteensixteen as you typica lly that would be one thing and it would also mean that you are starting from there so with this early stopping and other criteria you will not be able to grow much out from here right so just if that makes sense geometrically from here you would not be able to move all the way there you get that everyone gets this question and the answer so you see what the answer per is object was and you also see the difference between a normal regularization and this regularization in the normal case you had l theta plus omega theta put together and then you are trying to minimize the sum of these two it was a joint optimization here you have first done omega theta ensured that the weights that you learn minimize this objective and now you add in the supervise o bjective which is l theta right so now this makes sure that your network cannot be too greedy with respect to l theta because it has been constrained that has to first honor the omega theta because that is where you started and now from there on it has to decide how to do l theta does that make sense  you see how this is acting as a regularizer is that ok and that links back to your weight initialization thing right fine so some other experiments have also shown that pre t raining is more robust to random initializations now what do i mean that mean by that so in these two graphs that you see here so this on the x axis you have the number of layers that you add to your deep neural network and on the y axis you have the error that your network gives when you try different initializations right so this box actually tells you the variance in the error so that means i tried training a network with four layers and i tried different initializations and the error varied in t his range ok is that good or bad what would we want typically something which looks like the plot below right where all these variances are little that means even once you do unsupervised pre training right  i t is more robust to random initialization s random initializations of what student refer time twentyfifteen the original random initializations from which point you started the unsupervised pre training ok because once you have done the unsupervised pre training that is your initialization everyon e gets that so these are some let us see ok so these are some empirical studies and let me just make a comment on these so what happened from two thousand and six to two thousand and nine is people showed that see this is possible you can actually train a deep neural network using some of these tricks we do not have a very clear answer for why this works and you could argue different way so this is optimization this is regularization and so on but i do not have any theory supporting it there is no pro of for why unsupervised pre training works all of these are empirical observations but what it at least established was that it is possible to do this so now if it is possible to do this let me see if there are better ways of doing this do we actually need to do unsupervised pre training oh i think it is better regularization then why not i try better regularization techniques and see whether that helps so that led to the evolution of which thing that you have already seen yeah which regularization t echnique that you saw in the last class student d rop out drop out right so drop out was something specific to neural networks which was introduced in the context of neural networks so this is because people started believing it is possible so let us try even better ways of doing that so that is how dropout came out right then people said maybe optimization is the problem maybe these earlier algorithms which up till that point was which algorithm student refer time twentyonefortyeight gradient refer ti me twentyonefortynine maybe that was not good so let us try to decide and design better optimization methods and that led to the evolution of adam adag ard rmsprop s o on right so although these studies were not so theoretical in what they were trying to prove t hey created this hope which then led to a lot of pr olific work in that field right so at least you get the context now right the some of these might look oh this is one data set people did experiments on m l s but i could have taken a different data set and showed that these results do not hold and so on you could always ask those questions but at least what happened is people started believing these and people started questioning that ok unsupervised pre training is one thing what else can i do and now what has eventually happened is today no one uses unsupervised pre training right that method which led to the revival of this field and you would have hoped that that would actually survive for many years that is out now hardly anyone uses unsuperv ised pre training it is only used in the context of transfer learning so what i mean by that is that if you have a model trained for one classification say classification of images on one data set right now you have a very small amount of data in some other domain so instead of training a network from scratch for this domain you will just initialize it with the weights for whatever you have trained on data set one so that is more of transfer learning rather than unsupervised pre training so that i s still very prevalent but this reliance on unsupervised training to make sure that the network actually trains that is largely phased out because what has happened since two thousand and six and two thousand and nine is that we have better optimization algo rithms which are rms prop  a da grad  a dam even so on right many various and even now that research area is active as i was saying just in december there was a paper w hich pointed out some flaws in a dam and how to improve it and so on we are better regula rization methods the most prominent among those being student d ropout dropout so these two are things which you have already seen today we are going to talk about better activation functions this is again something which evolved that maybe sigmoid tanh are not good so maybe something else is needed and then better weight initialization strategies so then people took this inference oh one way of looking at unsupervised pre training is that it actually initializes the weights in a better way from where on it becomes easier for me to reach convergence so why do not i come up with better weight initialization methods itself instead of relying on this indirect way of initializing the weights so you get this so get the whole picture now what we have been doing in the past few lectures and how it connects to the history and these studies which were done from the period two thousand to two thousand and nine how many if you get the whole picture ok so that is where we are now so today we are going to talk about better acti vation functions and better weight initialization methods"}
{"audio_filepath": "Data/Preprocessed/Attention Mechanism_114.wav", "duration": 1613.0, "text": "so let us go on to the next module which is attention mechanism so let us motivate not the task of attention let us motivate attention mechanisms with the help of machine translation ok so what is happening in the models that we have seen so far the current model that we saw for machine translation by the way all t he models that i have shown you so far are wrong or rather incomplete we will complete all of them right and that is where attention fits in ok that was for the camera a encoder reads the sentence and its computes the encoding once right we read the e ntire sentence and be encoded it and then we have these two options either the pass the encoding at the zero time step or pass this encoding at every time step is this how humans translate a sentence what is the human analogy for this you have read the se ntence once done and now we are going to remember this entire thing throughout and then translate imagine if you doing this for sentences which have twentyfive words which is a typical wikipedia sentence what is wrong with this we have read the input ones and w e have encoded it what is likely to happen you will forget something you are going to lose information not just that is the entire sentence important that every time step student refer time onetwentyeight only certain words are important you see this conce ptually something wrong that we are doing here is is saying ok i have encoded the sentence and then start decoding from there ok thats the conceptual error that we are making so let us see how humans actually try translate it right refer slide ti me onethirtynine so when producing one word in the output suppose my input is the hindi sentence and i have the output sentence when i am trying to produce the first word i actually compute this probability distribution which tells me which of the input word s that i need to focus on at this point it is ok if i dont know what is the translation for ghar or ja or raha or hoon  as long as i know the translation for mai i am done because that is the word which i first need to produce there right so i am goin g to say that at this point i only need to pay attention with the first word in the input and i can ignore everything else what about the second time step i just need to focus on the last word what about the third time step is it always going to be tha t i only need to focus on one word at a time student no no what about the third time step student fl i am sorry i am assuming everyone understands hindi but i think that is this is small sentence i can assume that what will you focus on student  ja rahi ja and rahi right you want to focus on both these things and not an anything else and what about the next one hoon right so just on ghar and not an anything else is this what the model encoder decoder model is doing what is it doing actu ally the every time step is focusing on the entire sentence because that is the encoding that your feeding to every time step that is the problem that we need to correct we need to learn to pay attention to certain important parts of the sentence is th e setup clear to all of you is the motivation fine not your motivation layers is the motivation for this fine or not ok the distribution actually tells us how much attention to pay to each input word at each time step and ideally at each time step we should face pay attention to only certain words in the input so let us revisit the decoder that we have seen so far this is what the decoder looks like in fact i also have the encoder there now suppose sorry so current ly what we are doing is we are either feeding s zero at the i mean we have either feeding the input embedding or the encoder embedding at time step zero or at every time step the suppose there was an oracle which told us exactly which are the words important at time step t right so in our example at time step three suppose it told us that the word going is important actually we need to flip the input and output here also right but you can still understand right so i am saying at time step three certain words ar e important and suppose a oracle actually told us that these are the words which are important what would you do assume that you have already run the encoder what will you do now and say someone told you that only this word is important word why weight ed i am just saying binary weigths right only this word is important what would you do ideally student refer time fourtwentyfour just feed this blue vector to the decoder and do not feed everything else does not make sense suppose i told you that two word s were important send those two words but how concatenate but now at certain time steps four words will be important and you cannot concatenate four words right because then the dimensions will change so what do you do student refer time fourfortyseven a weigh ted student refer time fourfortynine weighted some of the important inputs is that make sense at time stamp three we saw that ja was zerofive important and rahi was zerofive important just a weighted combination of those two blue vectors and feed that to the decoder so  you are not changing the dimensions at each time stamp because the blue vectors have the same dimension i am just taking a weighted combination of those i am going to give you the same dimension does that make sense ok so in fact what i am saying is that i could just take a weighted combination of all the blue vectors that i have at the encoder and the weights of this weighted combination right now i am assuming that someone oracle has given me is that ok if i had his weights does this makes more sense then having the vanilla encoder decoder model everyone agrees with that ok now the question of course us who is going to give us these weights we will come back to that later but at least given the weights this mak e sense so at every time step they just going to focus on the words which are actually important just take a weighted combination of those words and we will just feed that to the decoder and intuitively this should work better because unlike before whe re we were overloading the decoder with the entire sentence remember twentyfive words thirty words entire sentence was being passed to the decoder now you are just overloading it with the amount of information that it actually needs to produce that particular word hence intuitively this should work better right ok now how do you convert this intuition into a model in practice of course there is no angel who is going to coming give as these weights is no oracle the machine will hav e to learn this from the data whenever you need to learn something you need to student refer time sixthirtyfour introduce parameters so i am going to now introduce a parametric form for the from the figure which thing for those of we cannot see these are alpha one alpha two and so on so now from the figure we are going to introduce a parametric form for student alphas for the alphas ok so i am going to introduce i will come to alpha but and what you think this weight should depend on what i am tryi ng to say is that at the tth time step of the decoder so this is e j t at the t th time step of the decoder i want to find out how important is the j th word in the input that is exactly what i am interested in at every time step of the decoder of all the in put words i want to see which of them is the most important right so this is the quantity that i am interested is how important is the j th input word at the t th time step this should depend on what what should it be a function of for one it should d epend on what that word is right the other is should depend on what has happened in the decoder so far what is the decoder produce so what is the input and what is the decoder state at so far right so as the decoder has already decoded the word ghar or home it does not need to look back at home right that is why need to know what is the state of the decoder what captures the state o f the decoder at time step t h t  and what is the state of all the words that we have it is captured by what th e h j s right this is h one h two h three h four does that make sense how many of you have fine at this point please raise your hands high above ok how many of you have questions please ask specific questions if you have a question all i am saying is a couple of things one is at every time step instead of the oracle giving me these weights i want to learn these weights whenever i want to learn something i have to introduce a parametric form and then i learn those parameters ok now what is the quantity that i am interested in i am interested in this for all the input words for the j th input word i am interested in knowing how important it is for the t th time step there are several ways i could write this function i am saying that the two things that are important is one what is this j th word which is captured by h j right and what is the state of the decoder up to this t ime step which is captured by s t one you could think of various other equations at this point i am fine if you by the intuition that this quantity should indeed depend on these two terms it should depend on what has happened in the decoder so far and what is my current word actually look like how many of your fine with that please raise your hands up and high ok now also the other thing t hat i want is that across all the input words this should actually sum to one right i just want a weighted combination i do not want arbitrary weights it just like taking a probability distribution over what which word is important by how much so if i ha ve this e j t how will i convert it to a probability distribution student softmax softmax so i will just compute th e alpha js as a softmax of e j t  e j s is that fine did not get this ok now we have still not seen what the exact form of attention is  of the f attention function is so this is what the equation for the  j t is that we had an alpha j actually denotes the probability of focusing on the j th word at the t th time step ok now we are n ow trying to learn these alphas instead of an oracle telling us what these alphas are so learning is always going to involve some parameters so let us define a parametric form for alphas and just a couple of notations so f rom now on we would not change this we are going to refer to the decoder state as s t and the encoder state as s h j  ok so these blue vectors are s s and these blue vectors are h s  ok given these new notations one among many possible choices for f attent ion is the following i wanted it to depend on the current decoder state i am making a dependent on the current decoder state but i am also adding a parameter in front of it right i also wanted to make a dependent on h j i am making a dependent on that i am also adding a parameter here why do i need this parameter what is the dimension of this let us assume this is also what is the dimension of this remember after multiplying with u attention and after multiplying with w attention the two vectors sho uld be addable is that fine something cross d what about this same thing cross d good ok so let us call that same thing as d one what is this output then the tanh output is vector scalar matrix vector of size student refer time eleventhirtyeight you said m atrix or scalar it is ask r raise to d one what is the quantity on the left hand side vector scalar matrix vector even though it has two indices it is a vector what is this quantity capturing at time step t what is the importance of the jth input t hat is a i will keep asking till everyone replies that is a student scalar scalar ok now you have scalar equ al to something multiplied by r d one  so why do you need this something student refer time twelvesixteen so what is the dimension of that going t o be student r done r done  so that is the dot product so you see why we have these parameters ok so what we have d one is made it dependent on s t one and h j  and also made sure that the output is a scalar that is what these three parameters are doing ok and these parameters will be learned along with all the other parameters of the encoder and the decoder so now this is all fine you would actually someone had given me the true alpha js and i had predicted this alpha js to that fancy equation which i just showed you and then i added a dash function softmax that is the laughter safest choice in this course i want to learn something so what do i what should i add student loss function loss function what would the loss fu nction be student refer time thirteenfour say a squared error loss and then i want to adjust the parameters to minimize this squared error loss then all of this makes a lot of sense right because then you can imagine that your u attention w attention an d v attention will get tuned in a way that the predicted weights are very close to the true weights this we all understand given an objective function we understand that the weights will get adjusted so that you are there to the objective function but th e whole premise was that we do not have the true alphas because in the case of translation no one is going to tell you that the k th word can come came from the j th word or the set of j words do you all agree with that so if we had the true alphas this m akes a lot of sense because then we could have added a loss function which takes the loss of alpha true with respect to alpha prediction and then an addition to our lost theta which was the sum of the cross entropy errors and then we could have jointly m inimize this and we could have hope that the attention parameters would have been learnt accordingly in practice we will not have this in our translation example we would want someone to manually annotate for every word in the output which is the set of input word from is this which it came is not going to be possible this we cannot collect so much annotated data so what do we do why should this model then work they does not have any supervision why should this model work in the absence of such data how many of you get the meaning of the question how many of you see the problem please raise your hands we are not given the true alphas and that is what a problem is then why should this work better this works better becaus e this is a better dash better dash choice language model better dash choice what is the possibility is there better modeling choice why ok i give you the answer this was better because this is a better modeling choice why so so i will give you a nalogy and the reinforcement learning fans will cringe but they can just go out so suppose i trying to learn a bicycle how to ride a bicycle thats why i said they will cringe i already see some of you can see as if you guy have a copyright on bicycles ok so suppose you trying to learn a bicycle and for some reason you in your infinite wisdom you decide that you can learn how to ride it without holding the handle and you start trying to do it its conceivable that you know few years or decades you will actually know how to ride the bicycle right even if you are not holding the handle right people do that and people can ride it before without that now the only thing that i do is i come and tell you that instead of doing this why not you try to ho ld the handle and then try to ride the bicycle right that is all i am telling you i am not giving you any other supervision i have just given you a better model i have said that instead of just trying to adjust the parameters with respect your feet and the pedal and your back position why not you also introduce this additional parameters where you are holding the bicycle which your hands and now try to figure out what kind of weights you need to put on your left hand right hand and so on i am not givin g you any supervision for that that you need to still discover on your own that you will start riding it you might fall on one side you might fall on the other side but you will eventually figure out what these weights need to be right because a second model where you hold the handle is a better model than the first model where you are not holding the hand in the second model you have additional parameters where you could adjust these parameters so that you could learn to drive better that some more natural way more close to human way of learning how to ride a bicycle the same thing is happening here the second model where you have a way of learning these attention on the weights even though i am not all i am telling you that look maybe if you decid ed every time step which were to pay attention on you might be able to do better than feeding the information from the entire sentence at every time step that is all the information that i am giving you which is very similar to saying just hold the hand that is not going to teach you how to ride a bicycle right you still have to do the extra work of learning these parameters but now you are given a chance you are giving the model a chance to learn these parameters they are telling it that this is a be tter way of modeling it with this you should be able to learn better right so there is a hope of doing better because now the model is actually making a more informed choice right it is a more informed way of learning how t o do translation by focusing on certain words at every time step and now these parameters how will they get adjusted they will get adjusted because at every at a given time step you produced a wrong output you did that maybe because this parameter was wr ong which is the v parameter or maybe because these recurrent connections were wrong or maybe because your attention weights are not proper so now adjust the attention weights and that should given sufficient data it should be able to learn which words to focus on just as humans learn how to do translation right even when we are doing learning how to translate or when we learn translating from one language to another we are not given this word by word supervision right we just do a lot of translatio ns or read a lot of translations and somehow understand that while translating i need to focus on certain words and at every time step this is the word that i need to focus on so given enough words it should be able to learn that at least someone gets th e joke good so that is the hope and in practice indeed these models work better as compared to vanilla encode you do not know where the statement comes from so now let us what we will do is so this entire thing hints on h ope only right that is all that is all i am saying but it does makes sense right because you have these additional parameters which you can learn and you can back propagate through them i will just not stop there will actually prove what happens not pr oved by demonstrate what happens in practice right so with this attention model in mind let us look back at the encoder decoder model that we had for machine translation integrate the attention mechanism with it and then let us see the end to end equati on that we get ok so this is what the diagram looks like the input and output still remains the same we have just given the source sentence and the target sentence in particular you are not given which words to pay attention to every time step right that is not given so remember that my input is not changing it still the same source sentence and the target sentence what is the encoder now try to work out the equations i wanted to write the equation for y t which is goin g to be some composite function of x where x is a vector it is a x one x two x three all the words in the input and somewhere along the line it also going to have this attention equation it is going to take a while but at least try to imagine it there is som e hints in the diagram itself you could take a look at it i am just asking you to convert the diagram to a set of equations so encoder part is fine i have computed the representation of each word at time step t so this is a contextual representation the word because it is aware of what the neighboring words are right now what is the decoder going to be what is the first thing at time step one or a time step t in the decoder what is the first thing that i am going to compute the dash weights the l ast time step of the decoder of the encoder sorry what is the first thing that we need to compute a time step t t attention weights speak up please what is the first thing that you need to compute at every time step how what kind of a combination i t ake off the inputs or rather which are the words that i need to focus on from the input who tells us this student attention weights the attention weights how will you compute the attention weights using this fancy equation that we have seen earlier is this enough i need to convert this to a student probability distribution probability distribution right that is just to ensure that everything is a neat combination once i know the attention weights what do i need to feed the decoder a dash combin ation of the inputs a weighted combination how do i take a weighted combination of the inputs summation i is equal to one to capital t student  j t  jt into h j right no j t is the decoder time step j is the input word so at the t th time step of the decoder i am taking a weighted combination of all my inputs the index over the inputs is going from j equal to one to t by the way did that answer your question that is what you are asking right ok is that fine ok now what next now i want to produce a word at the output what is the decoder going to be first thing that i am going to do is decoder is a dash rnn ok what is the input to the rnn every time step the previous predicted word as well a s the weighted combination input that you have given it does that make sense ok and then finally how do i get the probability distribution is that fine yeah i think this should be a distribution right l t does not make sense l t is r max of this rig ht and what is the loss function student cross entropy cross entropy there is no change in the loss function right loss and the algorithm remains the same say seen these set of equations now how many of your confident of going back and modifying all the wrong encoder decoder modules that we have covered in the initial part of lecture modifying them to add the attention equations in it how many of you can do that please raise your hands i am not going to ask you just do it so that i feel happy you can do right any questions at this point very good ok so you can go back and try adding attention mechanisms to all the models that we have seen before right see how will you compute so remember the only purpose of  ok what kind of a network is the attention network it is a single feed forward neural network right this is just transforming a simple linear transformation of the inputs and in a non linearity on top of that and then just again one more transformati on right it is a simple feed forward neural network only these three equation somehow need to be fitted in all the other models that we have seen so far right this is a very generic framework just as the encoder decoder framework or the very generic frame work the encode attend decode framework is also very generic framework you can go back and model all the applications that we saw and you can change them change them to at the other case ok try to answer the same set of questions whats the data what s the encoder whats the decoder whats the loss whats the training function and in particular remember that in when you go back represent all the applications that you have done the data is not going to change no one is going to give us the supervis ion for the alphas that is one thing which is not going to change ok so here is one more thing so this probably tie to this question like how do we be sure that the alphas actually learn something meaningful now what do i mean by this if i have to convince you that alphas are actually learning something meaningful and let us take the context of machine translation what do i need to show you suppose the model has generated an output for a given input sentence it has g enerated a translation what do i need to show you to convince that it is learn some kind of weights at every time step what should i show you student refer time twentyfourfortysix what does the attention weights look like right so let us see refer slide ti me twentyfourfifty this is a common trick or not a trick actually it is a probably a trick only but this is the common thing which is used in several papers and that is why i call it a trick because it is a trick to get a paper accepted that you actually show w hat the attention weights actually look like right so on this is the input document and this is the summary that you want to generate ok and what you see here is that at different time step so look at the last time step terrorism it paid maximum atte ntion to the word terrorism in the input right so we can draw this matrix suppose you had capital l time steps in the output and capital t time steps in the input so you could draw this l cross t time step or t matrix which tells you what was the atte ntion paid to every input word at every output time step do you get that you see what is matrix is this heat map is essentially a matrix of size l cross t and every cell here tells you how much attention you paid to a particular word at that time step an d the darker the cell that means more the attention that you paid everyone get this ok so what this is saying is that probably see when you wanted to generate russia the maximum attention was paid to russian and maybe some other words also sometimes it does not work very well but sometimes it does right so for calls the maximum attention was paid to called and then similarly for front with the maximum attention was paid to front and so on you see some meaningful patterns that it is learning here and here is another example for machine translation so roughly to quickly understand what this figure is right so this is i think english to french is it french yeah its french or french to english translation which is largely monotonic right t hat means at the fourth english word you would end up paying attention to the fourth french word right that means you are almost doing a word by word translation and that is exactly what you see the most of the attention is along the diagonal right so it is learning some meaningful attention weights as always helpful if you are using if you are using an attention mechanism to plot the sense see if it is actually learning any meaningful attentions or attention weights or not right so that is a common t rick which people use"}
{"audio_filepath": "Data/Preprocessed/PCA : Practical Example_49.wav", "duration": 687.0, "text": "so we will in thi s mod ule we will look at practica l example where pc a is used and i just like to give y ou a flavor of why all thi s is importa nt right why do we nee d to throw away some dimensions and then how does it practically help so consider that we are given a large number of human images right  so this is like some faces data set a database that says one of the intelligent agency someone is maintaining one of the governmen t agency or may be a adhar data bases or something like that ok n ow e ach image here is one hundred cross one hundred  that means it i s ten k di mensions right  i t i s a very high dimensional data ok a nd your job is to actually store this on to do some database for a large amount of the population right because you are collecting these images from various people so now we would like to represent and store this data using much fewer dimensions right a nd you would be really ambitious that if you want to store that more than fifty to two hundred dimensions right s o you see the compression that i am looking at you have ten k which is a big storage problem for me and i want to just bring ing out to fifty to two hundred  but i have know that t his is crucial data right i do n o t want to store information which is not able to dis tinguish these faces  i was still be able to reconstruct the faces from this information right d o well i mean mi nimum error r econstruction from this and that is exactly what pca s are allowing us to do right so now we construct a matrix of m cross ten k what is m s tudent refer time onefortysix t he numbers of samples you have the numbers of data point s that we have and each of this is of dimensions ten k o k  s o this is what matrix what do we call this matrix o h it i s already given right it i s the x matrix the data matrix that we always have n ow each row of the matrix corresponds to one image and e a ch image is represented using ten k dimens ions just to reiterate now let us see  so now what would you do this is the original data i want a dimensionally reduced data right  y ou want store this ah is the mike working y ou want this data to be represe nted by a fewer dimensions  so what is your solution d o pca  so what will you do  x transpose x right and i did not get my slide refer time twofortythree refer slide time twofortyfour so we retain the top one hundred dimensions corresponding to the top one hundred eigen vect ors of x transpose x right so basically we do a pca find the one hundred find all the eigen vectors of x tr anspose x and then just retain the top one hundred of those n ow what is the dimensi on of each of these eigen vectors s hould be straight forward take your time it i s early morning s tudent refer time threeten ten k right  so now can you think of a physical interpretation of this so what are you trying to do y ou are trying to store faces and now you have come up with these dimens ions  no sorry we have come up w ith these basis vecto r which is eigen vectors and each of them is also ten k which is as sam e as dimensions of your faces c an you think o f a physical interpretation of what is happening here n one of went you through the slide except perhaps you or i do n o t know just think about it  so what you are trying to do is you are trying to represent any possible face in your database ri ght using a linear combination of some vectors ok n ow these vector should have some interpretations right i t should be connect ed to faces and somewhere otherwise how will you construct a face from taking a linear combination or some random vectors do you get the point so can you think of each of these ten k dimensional vectors which is the same as the dimension of your origina l data as a face and try to plot it can you try to do that at least it make sense ten k dimensional ok that is the same what you are image size was i could just arrange th e s e ten k dimension as one hundred cross one hundred and try to plot it ok  so let us see if you d o that what happens ok we convert eac h eigen vector into one hundred cross one hundred matrix and treat it as an image and let us see what we get t his is what we get  so this is the top sixteen eigen vectors that i have plotted now can you tell me a physical interpretation of this t his is the basis for constructing any face in your data base right  t hat what you are trying to say all the faces that you have in your database or in the world you can combine them by looking at the these elementary face structures right which are your basis a nd then you could scale them up by using these alphas you will be multiply them with the certain alpha right a nd when you c ombine them you will get the base any face that you had in your original database d oes a physical interpretation make sense how many of you get this refer slide time fivetwentyeight so that is what is happening here  so we have constructed this basis now i will come to that later  so these images are actually c alled eigen faces and the form of basis for representin g any face in our database ok i n other words we can now represent a given image as a linear combination of these eigen faces  so this is my original image ok i want to reconstruct it n ow use sixteen or twentyfive of these eigen faces what do you think would happe n w e will get some face which has s ome error there is some error in reconstructing this face so let us see what we get  so i am using only one basis vector and i found out this alpha one i how would i found it out s tudent refer time sixeleven d ot pr oduct of the face vector with the basis vector ok now i instead of one i take two y ou see i have took this two basis vectors scaled them with the corresponding alpha coordinates and added it them up right a nd i am trying to get some face value it does n o t l ook it goes to the original face  that means the dash is very high the reconstruction error is very high t hat means i have still dropped some of the important dimensions i have still drops some of the important eigen vectors right so the value of k which is the top k eigen vectors is something that i need to take care of it and should be in a way i can always construct the reconstruct i can always compute the reconstruction error here right  h ow will you compute the reconstruction error s tudent refer time sevenzero t ake the square error between the original image and this second image that you see here right so you will take the square error between this and this and you will end up with the number which is not acceptable right n ow what i will do is i will go further i have taken four ok s till not quite there  but i can see a shape emerging right i go to eight things becomes better since you are already know what the original face at least you can make sense of it a nd by the time i reach sixteen i am alm ost there right at least i can recognize the face that is probably losing out some subtle things in the face  but i can recognize it now how many of you appreciate what is happening here y eah of course now what is happening here so think in terms of a practical application right what have you done what have you able to be ac hieve how may basis vectors where you able to store or did you store s tudent sixteen sixteen  that means sixteen into ten k values ok and suppose you had a million images in your database how many would you require to store s tudent refer time eighteleven w ait let us we do it step wise forget about pca if you had a million image in your data base and each of them have ten k dimensions how much storage do we need s tudent refer time eighttwentyfour m illion into ten k floating point values ok n ow with if i say sixteen sixteen may be too ambitious may be later on i will say fifty or one hundred is ok but let us say sixty then how much data we need to store s tudent sixteen into ten k sixteen into ten k and you can reconstruct any face s tudent refer time eightfortynine y e a h alpha s needs to be stored right so for every image instead of storing ten k dimensions you will just store the sixteen alphas right a nd you can see that even if i go there to one hundred and its still ma nageable instead of ten k i am going to just store one hundred alphas ri ght  a nd as i go to one hundred what would happen s tudent refer time ninenine t he reconstruction error would become even lesser ok  so is that is the intuition clear ok so this eigen vector storage is a onetime stor age we are going to store this k eigen vectors each of them are ten k dimensional and k i s one hundred or two hundred we do n o t really care because the original data wa s very large right a nd for each image we just need to store these alpha values k of them right so f or each of them instead of ten k we will store one hundred to two zero alpha values and of course it i s significantly reduces right so this is why we need to do all this right and what is the other advan tage of doing this a nything of something else  so what is pca actually allowing you to do i f you again think of it not i mean subtract the math out just think it in terms of physically interpretation a nd what is that it is allowing you to do if you had to say it in e nglish what is it allowing you to do c ompress ion is a loaded word can you just spell it out for me what is this compression mean actually s tudent refer time tenfourteen r ight  so it i s storing all the relevant information in the image and disca rding all the error element information right n ow this also ensures that if you have multiple images of the person then what would happen y ou should take the image under lighting conditions or may be at person had applied some makeup or something like that right what would happen s tudent refer time tenthirtyeight i n the original space the ten k dimensional space what would happen to these images t hey will be very far from each other right because the lighting conditions have changed you see a dark person so have a light person something like that right a nd n ow because pca has helped you to throw away this dimensions right  m ay be the exact terminology which i am usin g may be the lighting condition do n o t do it b ecause you can imagine that there would something right that suppose as some element which is call ing t he image to look slightly different  but that is not the important information right so that would get discarded of f and only the relevant information would stay so then multiple images of the same person w hich were dash in the original sp ace woul d come dash in the new space student refer time eleventwentytwo f ar in the original sp ac e would come closer in the new sp ace right so this is what compression helps you to do s o this is what you want to learn you want to learn the im portant characteristics of your original data and that is what pca allows you to do fine"}
{"audio_filepath": "Data/Preprocessed/CNNs (success stories on ImageNet) (Contd.)_91.wav", "duration": 132.0, "text": "this is wherewe left off in the last classso we look atthreenetworksfor image classificationstartingwith alexnet then zfnet and then vggnetvggnet in particularhadsixteen layersincludingconvolutionsand fully connectedlayersand one thing that we saw that a large number of parameters are there in the first fullyconnected layer because you are connecting a five hundred and twelve   seven seven volume to a four thousand and ninetysix dimensional vector right so that is one thing the other thing that i would like to kind of mention right now so that it bec omes useful for the later part of the lecture is that if i look at any of these pink boxes here right or even these things which are known as the fully connected layers and if i just flatten them out and view them as a vector what does that vector actua lly capture it captures a it captures an abstract representation of the image right so now imagine what would happen is suppose you have trained one of these networks alex net vgg net or any of your favorite networks and by what i mean by training is that you have been tracking the cross entropy laws and you have run it for several epochs with some patience and so on and i was satisfied with whatever training error you are getting and you have stopped training now right now after this if i pass images through these net through this network and i take the representation from any of these boxes or from the fully connected layer what is it that i have essentially got now i have got an abstract representation of the image that i have been feeding i t right so just remember this and this is something that we will use so this is very common to do so you have a trained image net many people have released different models for image net the ones which we have covered being included them and now for him any image task if you want to do some processing then its common to take the strain network pass your image through that so you can train any you can use any image trained image net and pass that image through it or sorry any trained convolutiona l network trained on image net and pass the image through that and you can get a representation for that image and these are known as the fc representations and these are as the convolution representations ok any of the convolution layers fine"}
{"audio_filepath": "Data/Preprocessed/Deep Learning(CS7015): Sigmoid Neuron_18.wav", "duration": 705.0, "text": "we are in lecture three of csseven thousand and fifteen and today we are going to cover the following modules we are going to talk about sigmoid neurons gradient descent feedforward neural networks representation power of feedforward neural networks so let us start so here are some acknowledgments so for one of the modules i have borrowed ideas from the videos of ryan harris on visualize back propagation they are available on youtube you can have a look if you want for module threefive i have borrowed ideas from this excellent book which is available online it is the url as mentioned in the footnote and i am sure i would have been influenced in borrowed ideas from other places and i apologize if i am not acknowledge them probably properly if you think there are some other sources from which i have taken ideas and let me know i will put them in the acknowledgments so with that we will start with module threeone which is on sigmoid neurons so the story i had is that it is enough about boolean functions now we have done a lot of boolean functions but now we want to move on to arbitrary functions of the form y is equal to f of x where x could belong to rn and y could belong to r so what do i mean by this so let me just explain this with the help of an example so i will again go back to our oil mining example oil drilling example where we are given a particular location say in the ocean and we are interested in finding how much oil could i drill from this place and that is what i would base my decision alright whether i want to actually invest in this location or not and then what we are saying is that this could depend on several factors so we could have xone xtwo xthree up to xn right where this could be the salinity of the water at that location so this could be a real number this could be the density of the water it is average density this could be the pressure on the surface of the ocean bed and so on and so forth so each of these values independently belongs to the set of real numbers so each of this is a real number and we have n of these so together they belong to rn so i can read that i have n such real numbers and i could just put them in a vector and say that i have a input x which belongs to r raised to n so we have this x which we can say belongs to rn and in this particular case we want to predict y we want to take this as an input and predictor y and what is y in this case you want to predict the quantity of oil that we could mine so what does ry belong to again a set of real numbers and it could be some gallons or litres or kill of water so this again belongs to r so these are the kind of functions that we are interested in now we want a function which takes us from i am having this x which belongs to rn right it is a vector of dimension n and takes us to a value belonging to r so you clearly see that this is different from the case when we had n variables each of this was just boolean so these were only zero one inputs now we have real inputs and these are the kind of functions that we are interested in now can we have a network which can represent such functions now what do i mean by represent such functions we already spoke about this when we were doing boolean functions so what do we mean by representing the function we mean that if i am given a lot of training data right so i am given these xone xtwo each of these belongs to rn and i am also given the corresponding labels now i want a network which should be able to give me the same predictions as is are there in my training data so it should be able to take any of these xs as input and it should give me the same y i corresponding to it and i am saying approximately which means i am with some error rate whether if it is within some to with as long as it is close to the actual value i am fine with it so that is what i mean by a network which can represent such functions is that working definition of representing clear right so that is a very similar to the definition that we were used for boolean functions we had said that we should be exactly be able to get the truth table the network should be able to represent the truth table exactly so that is very similar to the definition that i am using here and then before we do this right before we come up with a network which can do this for arbitrary functions we have to graduate from perceptrons to something known as sigmoid neurons so please remember this overall context that we dealt with a lot of boolean functions we analyze them carefully and we saw that we could come up with these networks which could represent arbitrary boolean functions and they could represent them exactly as long as we have one hidden layer of course the catch was that that hidden layer could grow exponentially now we want to graduate from boolean to real functions that means you have a real input of n variables and one or more outputs and you should be able to represent this exactly so that is where the transition is where so that is the story that we are looking for so let us start so recall that a perceptron will fire if the weighted sum of it is inputs is greater than the threshold just recall that fine so now i claim that the thresholding logic which is used by a perceptron is actually very harsh now what do i mean by that let us see so let us return to a problem of deciding whether we like or dislike a movie that is the same problem that we have been dealing with and now consider that we base our decisions only on one input which is the critics rating which lies between zero to one and this is what my model looks like it takes the input as the critics rating i have learned some weight for it and my threshold is zerofive what does this mean it means that if for a given movie the rating is zerofiftyone will it predict like or dislike like so then i should go and watch the movie what about a movie for which the critics rating is zerofortynine dislike so now you see what i mean by harsh so both these values are very close to each other but for one i say i like it for the other i say that i would not like it so it is not how we make decisions right you would have probably said something equal for both the movies you would have not given such a drastic decision so why is this happening so you might say oh this is a characteristic of a problem that youhavepickedupmaybethatisthecriticsratingwhichisbetweenzerotooneor something but i want to convince you that this is not a characteristic of the problem that i have picked up but this is something to do with the perceptron function itself so this is what the perceptron function looks like so this sum of all the inputs the weighted sum of all the inputs i am calling it by a quantity z and this is what i am going to plot on the this axis so this is my z axis now what does the perceptron say that when this value of z becomes greater than w naught or minus of w naught it will fire and when it is less than minus of w naught it will not fire that is what it says so this is a characteristic of the perceptron function itself it is going to have this sharp decision boundary that whenever your sum crosses this threshold you will say one and whenever your sum does not cross this threshold you will say zero so in this toy example over the movie critics it just happened that this was zerofive and so it was saying yes for zerofiftyone and it was saying no for zerofortynine so this will happen for any problem that you pick up so to counter this we introduce something known as sigmoid neurons and this is just a smoother function or a smoother version of the step function you see that how many if you know what a sigmoid function what is the formula for a sigmoid function quite a few good and here is one such sigmoid function which is called the logistic function so remember that sigmoid is a family of functions these are functions which have this s shaped logistic function which i have shown here is one such function and the other function that we will see in this course is something known as the tanh function so let me just get into a bit more detail with this logistic function i just want you to understand it properly so this quantity here remember we were writing it as w transpose x which was summation i equal to zero to n wi xi remember this so now i am just going to consider this to be one over one plus e raise to minus w transpose x now i am going to ask you some questions and try answering those whathappenswhen wtranspose x tendstoinfinity whathappensto thesigmoid function one and that is exactly what is happening here as this tends to infinity as this keeps growing so remember this axis is z which is the same as w transpose x right this is w transpose x so as it tends to infinity your sigmoid goes to one what happens if w transpose x is minus infinity zero and that is exactly what is happening here and what happens when w transpose x is equal to zero half so this is that value corresponding to half is that clear so that is how a sigmoid function behaves fine now we no longer see a sharp transition it is a very smooth function and the sigmoid function lies between the values produced by the sigmoid function rate what is the range that they lie between zero to one what is another quantity of interest that you know which lies between zero to one probability so that is one advantage of sigmoid functions so now you can interpret the value given by a sigmoid function as a probability so what does it mean in our movie example again so it just tells me in those two cases that with fifty one percent probability i like the movie or with fortynine percent probability i like the movie so now this is not very drastic or very harsh right i am not saying yes or no i am not committing myself i am just giving you a number which is proportional to how much i like the movie so it can be interpreted as a probability now heres the overall pictureit so thisis the differencebetween the perceptron function and the sigmoid function so notice that here we had this if else condition right which was leading to that sharp boundary now here we do not have that defence condition we just have a function which is a smooth function and here is another picture so this is not smooth not continuous and not differentiable everyoneagreeswith that it isnot smooth here rightit is not differentiablehere whereas this is smooth continuous and differentiable and the contents that we covered today it will be very important to deal with functions which are smooth continuous and differentiable so for lot of this course calculus is going to be the hero of the course lot of the things that we do will be based on calculus and in calculus always if you have smooth and continuous and differentiable functions they are always good so that is why we want to deal with such functions"}
{"audio_filepath": "Data/Preprocessed/Introduction to Autoncoders_51.wav", "duration": 3108.0, "text": "welcometo lectureseven of the courseon deeplearningcs seven thousand and fifteen inthis lecturewe are going to talk about auto encoders and we will focus on their relation with pca then talk aboutregularizationin autoencoders whereinwe will look atdenoisingauto encoders sparse auto encoders and contractive auto encoders so let us begin with the introduction to auto encoders what they are so this is what a typical auto encoder looks like a nd as you can see this is very mu ch like a feed forward neural network you have an input which is x i so you are given some training data you are given some i samples x i to x n so this is your training matrix x which we have seen in the previous lectures so this is one of those tra ining inputs x i and then you have a hidden layer and then an output layer so let u s look at what is the configuration of the hidden layer and what does the output layer actually try to do refer slide time oneeight so it i s a very special type of a feed forward neural network w hat it does is it encodes with input x i to a hidden representation h ok and it uses an encoded function to do this so this is what the encoded function does i t first does a linear transformation so w is a matrix and x i is a vector and you again have the bias b as a vector right so let u s look at these dimensions right  so let u s try to fix some dimensions so suppose x i belongs to r n that i s what we have been considering throughout the course  so far and let us sa y h belongs to r d so it is a d dimensional representation s o in that case what would w be yeah  so w would be r n cross or the d cross n right so it will multiply with the n cross one vector which is x i and give you a d cross one output right a nd simi larly the b would also be d cross one and then on top of that you have this non linearity g which will be operating at element wise just as we had seen earlier  so it could be any of the sigmoid functions the logistic or tan h and so on so the end result is you have taken an input x i and encoded into a hidden represent h by using a linear transformation first and then a non linear transformation right so i refer to w x plus b as a linear transformation because it is a matrix multiplication n ow once you have constructed this hidden representation refer slide time twofortyone t he job of the decoder or the latter half of the feed forward neural network which is this half is to take this encoded representation a nd then try to reconstruct x again from it so again let us first look at the equation s o this is the equation for the decoder where again you first take the hidden representation do a linear transformation and then you again have some function on non linear ity on top of it right so we will see what this function can be so we will refer to it as f for now we will not say whether this is sigmoid or linear or what kind of a function it is we will come back to it later on so now let us again look at these dimensions s o what is x i x i is again r n and your h was r d so you have to go from a d dimensional input to an n dimensional output  so again your w star is going it to be r d cross sorry r n cross d so it will multiply with a d cross one vector and give you an n cross one output right a nd that will pass through some function and it will give you x i hat which is a reconstruction of x i so why are we trying to do this right we took an input x i we computed it is hidden representation by doing some non linear and linear transformatio n and then again we are trying to reconstruct x i hat  so why are we trying to do this s o reason we are doing this is that we want to learn what are the most important aspects or most important characteristics of the input data x i right so if you com pute a hidden representation h which is presumably smaller than your original input data a nd from that hidden representation if you are able to reconstruct x i right t hen that would mean that this hidden representation captures everything that is requir ed or everything that is yeah everything that is required to reconstruct x i from x i from the original input right refer slide time fourthirtyseven so the model will be trained to minimize the difference between x i and x i hat so you want to make sure tha t after passing through this bottleneck which is the hidden representation you are able to reconstruct x i and the reconstructed output is very close to the original input right so can you see an analogy with pca where you are trying to find this hidde n representation or this most important elements of the original input x i  so there we had used this linear transformation where we are taking the original input x a nd transformed it to a new basis and we had used that basis for representing the origina l input right so something similar is happening here we are using this hidden representation h to represent our original input refer slide time fivetwentytwo now let us consider a few cases the first cases when the dimension of h is less than the dimensi on of x i i n this case as i was trying to say earlier if we are still able to reconstruct x i hat perfectly from h t hen what does it say about h it tells us that h is a loss free encoding of x i i t captures all the important characteristics of x i write just repeating what i had said on the previous slide a nd now you can see an analogy with pca because h has all the important characteristics required from the original input data so it has probably got rid of all the noise or all the low variance dimen sions or the correlated dimensions and so on a nd this is just the compact representation which is as good as the original representation and from there you can reconstruct the original representation a nd such an auto encoder where the dimension of the h idden representation is less than the dimension of your original input is known as an under complete auto encoder refer slide time sixseventeen now let us look at the other case where the dimension of the hidden representation is greater than the dimensio nal of the original input ok such an auto encoder is i will tell you what it is called  so we will we are looking at the case where the dimension of the hidden representation is greater than the dimen sion of the original input so now in such a case the auto encoder could learn a very trivial encoding by simply copying x i into h and then copying h into x i right so think of this from a compression point of view right so now suppose you have ten bits initially right and then you want to somehow compr ess it and store it only in four bits a nd now this four bits should be such that it captures everything that was there in the original ten bits because you would want to reconstruct the original input again right so this is what we do typically when we compres s any of our files right we have a larger file we compress into a smaller information while making sure that everything important is there  so that whenever i want to recover it i can j ust recover it from there so this is definitely a hard task  but no w what i am doing in this auto encoder is that i had ten bits i am actually giving it more bits now because the dimension of h is greater than the dimension of the input a nd then from these sixteen bits i want to reconstruct the ten bits now this is a very tri vial task right because all i could do is copy these ten bits into the first ten bits here leave the remaining six blank a nd then from those ten bits just reconstruct the input that i s very very trivial if you give me more storage and what i originally needed then definitely i can easily reconstructed refer slide time sevenfiftyeight so this looks very trivial and this is what it could do right just copy the input to the first the n bits refer slide time eighttwelve so this was n and this was d and we are looki ng at the case where d is greater than n so it will just copy the input to the first n bits and then just take it back to the output just as i said in the case of you have ten bits sixteen bits and then again ten bits it is very trivial to do this so such an identity encoding is useless because you are just not running any important characteristics of the data your h is almost the same as x i it also has all the useless information that x i had in fact it has slightly more because it has these blank units a lso but this is not really useful right why would you want to actually learn such a hidden representation right so it is not clear why would you want to do that so we will take a look at it we will come back to why this is important so such an auto en coder is known as an over complete auto encoder because it has the hidden representation has more number of neurons as compared to the original input now let us look at a case where this would actually be important right so this is a very rough intuitio n for why you would want an over complete auto encoder so let us consider the case where you have as input one of the features that you are looking as bmi so suppose you are trying to find out whether the person is likely to get a certain disease or not right so whether he would have a heart attack or whether he would have a diabetes would have diabetes and so on a nd you are looking at various parameters or various medical parameters of that person and one of them could be height one of them could be weight and one of them could be bmi now for whatever reason you have not computed the height and weight and you have only looked at the bmi so now what has happened in your input and all of you know that bmi is actually body mass index which is a func tion of the height and the weight so now what has happened is that in your original input there was already this compact the your feature space is already compact because you would actually look at you should have actually looked at both the features h eight and weight but for some reason you have only computed bmi and you could think of various some other correlated features which are functions of many other features  but you do not look at all those features and just this final function of those feat ures so now in that case if suppose your prediction is that this person has or has a high likelihood of being of high likelihood of having diabetes at some point in his life t hen you would want to know whether it was the height or whether it was the w eight which was responsible for this so in your original input your features are actually entangled and you would like to disentangle them right so you would want to go from this smaller feature space to a larger feature space where some of these entan gled features get is disentangled so in those cases we reach an over complete auto encoder  however the problem still remains that there is no reason why the machine should actually learn to disentangle these features it could still just simply copy the bmi here and then copy it back here so that i s why when you are dealing with over complete auto encoders you will have to do something special to prevent this kind of identity encoding  so as you just take the input and copy it to the hidden layer an d then copy it back to the output so we will look at what kind of special treatment you need to do to prevent these kind of identity representations refer slide time eleventwentyfour h ere i s the road ahead so first we will talk about the choice of f x i g x i right so we did not say anything about what these functions f and g have to be  so we will talk about those and then we will talk about the loss function  so i have just told you so far that we will train this model in a way that x i i s very close t o x i hat right a nd i have argued that if we are able to actually achieve that that x i hat is the same as x i in which case presumably presumably the loss would be zero t hat means our hidden representation has captured all the important characteristics o f the original data s ame as in the analogy of ten bits to four bits to again ten bits right i f i am able to reconstruct this without any error t hat means loss is zero then these four bits or the hidden representation of my original x was actually able to capture e verything that was important in x so that it can reconstruct x again as x hat without losing any information right so that i s the loss function that we would want now what is the actual mathematical formulation for this loss function that i s what we wi ll see next refer slide time twelvethirtyfive so first let us start with the choice of f f and g so we will consider two ca s e two cases one case when your inputs are binary and the second case when your inputs are actually real numbers right so the first we w ill look at the binary case so now just some notation clarification so remember our original data was this matrix x which was m cross n t hat means you had x one x two up to x n and each of these was r n so now when i am referring to the entire row or entire data instance i will use bold x i as i have circled here a nd i want to refer to one of the elements of this guy then i will use this notation x i j same as what i have written here so what i am saying is that each of these x i j s actually is a binary variable refer slide time thirteentwentyfive now which of the following functions would be most appropriate for the decoder so remember was the input was binary t hat means your output also has to be binary you do not want to produce numbers arbitrar ily large belonging to any or want do not want to produce any real number you want to produce numbers which lie between zero to one so in such a case what would be an appropriate loss function or sorry what would be an appropriate function for the decoder so remember i am asking you what would f be so i am giving you three choices it should be tan h or just a linear decoder or a logistic function w hich of these would be most appropriate logistic why would that be because it will make sure that your outputs ar e between zero to one tan h would give you outputs between minus one to one b ut you do not want that because your inputs were between zero to one so when you are reconstructing  obviously you want outputs between zero to one you do not one minus one to one a nd linear of cour se can give you any real number which is not what you want right so if you produce any arbitrary real number like hundred and so on your loss is going to be very high because your inputs were just zero to one and you are producing these arbitrary real numb ers which are very different from what your input was ok so in this case the logistic function makes the most appropriate choice a nd g is typically t hat means the encoded function is typically again chosen as a sigmoid function so it could either be the logistic function or the tan h function right so the there is you could choose any of these as the encoder function fine refer slide time fifteenone now let us consider the other case where your inputs are real valued t hat means when you reconstr uct something you should again produce real values t hat means your function f should take whatever is the input given to it and map it to some real numbers right so that i s what we want from this function f earlier in the binary case we wanted it to ma p it to binary numbers right so that i s the difference that we have now so in this case which of the following would be appropriate t he second one right because tan h does not make sense because it will just produce minus one to one  but you want to produ ce any possible real number because some of these are actually higher than one greater than one l inear would be fine because it will produce any real number logistic is again not fine because it will produce numbers between zero to one refer slide time fifteenfiftyone so the logistic and tan h as i said would clamp the output to certain ranges  so that is not appropriate hence you should choose the linear function and again in this case also g is typically chosen as the sigmoid function fine ok so the next thing th at we look at is the choice of the loss function refer slide time sixteenten a nd again we will consider both the cases where a case the first case is the inputs are real valued and the second case is when the inputs are binary so let us look at the rea l case first n ow here the objective of the auto encoder is to reconstruct x i hat to be as close to x i as possible n ow we have actually seen this before so something similar before when we were talking about regression so now you want to produce rea l valued outputs and they should match your real valued inputs so what is an appropriate loss function that you can choose the squared error loss function right so what does this actually capture i t says that for all my input data x one to x m f or eac h of these dimensions x one to up to x one n right i want to make sure that my original input i will have a similar x hat reconstructed where i will have x one one hat x one two hat and x one n hat so i want to make sure that each of these pairs of variables are a ctually similar a nd i can capture that by ensuring that the squared error loss between the i j th entry in my output is the same as this or sorry r ather i could capture the squared error loss between the i j th entry in the output and the input ok t hat i s what this function is trying to capture straightforward similar things we have seen while we were doing regression e xcept that there we had y hat and y  but here we are just trying to reconstruct the input so there is no y here we just have the x ok a nd the parameters of the objective function are of course all the variables or all the parameters that we have in a network which is w w star c and b refer slide time seventeenfiftysix a nd the matrix or the vector way of writing this is the following so we h ave x i  so what i am looking at here is i have gotten rid of this summation and i am just written it in vector form  so let me just explain what this means so this is what x i would look like right so this would be x i one x i two up to x i n ok this is the vector a nd then you have the x i hat vector which is going to be x i one hat x i two hat up to x i n hat so taking the difference between these two vectors that i s what this term is so what you will get is essentially x i one hat minus x i one up to x i n hat minus x i n right a nd then you are taking the dot product of this vector with itself which will essentially give you this summation right so the dot product of this vector with itself is actually going to be this summation it is going to be the s um of the squares of the elements of this vector and that i s exactly what we wanted refer slide time nineteensix so this is a more compact vectorial way of writing the same thing a nd now we can just train the auto encoder by treating it as a regular feed forward neural network this is just a like any other feed forward neural network you have find the loss function a nd you can just use back propagation to treatment right but and in this case all we will need is a formula for the gradient of the loss fun ction with respect to with your parameters which are w and w star i have again ignore the biases and the bias is here b and c so we will also need dou l theta by dou b and dou l theta by dou c right so these two gradients also you will need but these are generally the easier ones to handle if you know how to compute this b and c are very easy refer slide time nineteenfortyeight so let us look at this now what we need for back propagation as i said we will need this gradient right a ll these four gradients b ut let us focus on one of these n ow we have already done back propagation and we have looked at arbitrary neural feed forward neural networks here right w e did not have we just said that there are l hidden layers and in this case l is equal to one right o r other we had said there was l minus one hidden layers and the l th layer was the output so in this case l minus one is equal to one  that means there is just one hidden layer so it does not matter we had actually derived it for the general case when l is e qual when the number of hidden layers is l minus one and here we just have one eight n layer so it is much more simpler than what we had learnt a nd even for the number of neurons in the each of these layers we are just assumed general that it could be r n a nd in this case we would have some r d which is less than n or it could even be greater than n right s o but it does not matter because whatever algorithm we had or whatever equations we had derived for back propagation t hey did not care about what thi s n or d was we had just derive it in general terms right and the same for the output layer w e did not assume any number of inputs any number of neurons in the output layer we again said that it has some k neurons but there the catch is in the earlier c ase when we had derived back propagation w e were dealing with classification and we had these k classes that we want to predict at the output a nd in which case our loss function was actually the cross entropy or the negative log likelihood function right w here we were trying to maximize the probability of the correct class out of the k given classes but here our loss function is slightly different it is actually this squared error loss between the input and the output so now given this difference in t he loss function does it mean that everything that we learn in the previous lecture on back propagation we just have to throw it all away because now there i s a new loss function t hat means my gradients are going to be very different from what i had deri ved for the back propagation loss where i was looking at the cross entropy loss as compared to the squared error loss so does it mean that i will have to throw away all the hard work that we had done in that course in that lecture or can we reuse somet hing from them we can reuse so let us look at what we can reuse and i will just give you an intuitive explanation for that so you can think of this as a composite function right a nd you are taking your input passing it through a lot of functions and t hen arriving at the output and then your loss function is actually a function of the output itself so what we have is something like this right we have a situation like this t hat you had an input x you computed some function of it say x square right s o i will call this as y one then you computed some other function of it say y one say log of y one right so they this was log of y one so in effect it is actually log of x square because y one is equal to x square and then some other function and then finally you had the output  so you had this other function which was sign of i am calling this y two so say this was sign of y two and finally you had this function which was e raise to y three so you have a very complex composite function of your original input rig ht and this is your final output function that you are considering which is e raise to y three n ow the way you would do this is if you want to take the gradient of d l with respect to your input d x right i n that case what would you do is you just apply the chain rule you will write i t as dou l by dou y three dou y three by dou y two dou y two by dou y one and then dou y one by dou x right a nd this is something very similar that we are done in the back propagation lecture we had constructed this chain and then we had attack ed every element of this chain and derived how to deal with that right derived an neat expression for that now the question which i am asking you is that in that lecture we had assumed a certain l a nd that l was actually cross entropy but in this lectu re i have actually changed the l what i am saying is the l is actually equal to the squared error loss n ow does that mean that i have to throw away all this work that i had done no right so even in this example if you look at it suppose i change this f unction from e raise to y three to say square root of y three so i have just changed my l but notice all of these other guys are going to remain in the same because y three is still sign of y two so that the derivative of y three with respect to y two is not going to ch ange e ven though i have changed the output function the loss function everything else is going to be remain in the same right s o  that means all these portions i could just reuse from the time when i had computed for this chain i just need to rework o n this final expression and plug it in right so that i s why all the work that we had done in the case of back propagation will not go to waste in particular everything that we had done refer slide time twentyfourfortyfive so let me just go to the next slide so in particular everything that we had done for this portion of the network right which is actually dou a two all the way up to dou w right so if ok  so let me write it like this i want dou l by dou w so i can write it compactly as dou l by dou a two and t hen dou a two by dou w right so this portion is not going to change because i am not change any of the functions here i have just assumed sigmoid or logistic or the same kind of network t he only thing i have changed is something at the output layer so i will just need to recomputed this and the rest of it can be reused right so that i s the intuition which i wanted to give you refer slide time twentyfivetwentyseven a nd that i s exactly what i s written on this slide so i am written it as dou l theta by dou w st ar that i s the first gradient i have interested in a nd i could write it as dou l theta by dou h two dou h two and dou a two by w star right now this portion as i was trying to say is something that we have already seen in the back propagation lecture and no thing has changed in the network in that part so you can just reuse it as it is and this portion is something that we need to recomputed right t hat i s the only thing that we need to recomputed and plug it into our back propagation code or the algorithm which we had in the previous lecture a nd similarly if you want to do dou l theta by dou w it is the same idea here that you could write it as the following chain a nd this part of the chain you already know how to compute from the back propagation lecture a ll you need to do is change the loss function and just try to find the derivative of the loss function with respect to your output layer which is h two t hat i s the final thing that you have changed just as in my toy example i had changed e raise to y three t o square root of y three right t hat i s the similar change that i am trying to do here fine so all we need do is dou l theta by dou h two but dou h two is the same as x i hat right because that i s my output and my output i am calling it as x i hat so i need t o take actually the derivative of this  so i am just using the vector form here i could have also written it as this summation over i equal to one to n x i j minus x hat i j the two whole square right i could have also written it as am i just writing it as t he vector here in the vector form here right  but this quantity ultimately is going to be a scalar because it is a dot product between two vectors which is the scalar so what i am doing here is taking the derivative of a scalar with respect to this vector so what is that derivative going to be i t is going to be a vector refer slide time twentyseventwenty a nd i am just so we have similar stuff in the past  so you can actually easily work this out so this will actually turn out to be the following vector whi ch is to times x i hat minus x i right so this is very simple i have just computed this and all i need to do is go back and change my back propagation code a nd change this derivative of the loss function with respect to the output clear and the rest of the code i can just reuse it as it is so now similarly  so we have both of these ready refer slide time twentysevenfifty now let us look at the other case when we have binary inputs ok t his is the most more this is something different that we will have to d o here so we will now look at the second case where the inputs are binary so first we look at case when the inputs were real numbers and hence your outputs also needed to be real numbers now we look at the case where inputs are binary and hence your outputs also need to be binary ok n ow here so each of these guys is actually a sigmoid functions  so it is in or rather if you look at the output you could divide into two parts so this is the pre activation and this is the activation so your this is actually the pre activation and this is the activation right so this activation is actually chosen as the sigmoid function or the actually the logistic function not the sigmoid function of course logistic is the sigmoid function  but the logistic f unction which was one over one plus e raise to minus z right so logistic of z is equal to one over one plus e raise to minus z a nd remember that this sigmoid function was element wise t hat means this is a is a vector it has elements a one a two up to a n and then you know apply the sigmoid to it you get h which is going to be sigmoid of a one sigmoid of a two and sigmoid of a n right so it is just the sigmoid applied to every element of the activation layer t hat means every element of this vector which have circl ed so now in this case your outputs are going to be between zero to one right because your inputs were also between zero to one and your sigmoid or the logistic function is going to give you clamped outputs between zero to one so since this is between zero to one we coul d actually interpret it as probabilities right so we could say that whatever you are reconstructing is actually telling you that with zero eight s uppose the reconstruction value is zero eight then you could think of it that with probability zeroeight it is telling you th at the output should have been one right a nd if it tells you that the output is zerotwo if the sigmoid gives an output as zerotwo t hen you could think of it that with probably zerotwo the output was actually zero or rather the input was zero because an input is the same as the output so that i s one way of interpreting it and this way of interpreting it why does it make sense  so we will just look at that right  so before at if i do not give you this interpretation a nd remember that the sigmoid is going to produce values between zero to one but not necessarily zero and one right it will try to be as close to zero when the input is zero  but it could also produce zerofive and so on a nd when the input is point nine it could also produce something like zeroninetyfive so at the output also you are goin g to get these vectors which are of which would look something like this right a nd suppose you are input was zero one zero one now can think of a suitable loss function for this yeah  so again these are two vectors these are x hat and x so once again you could h ave just gone with the with a squared error loss right you could have taken the squared error difference between these two and you could have been fine so that i s definitely one way of going about it but whenever we are looking at these binary inputs a nd whenever this probabilistic interpretation is possible we tend to do something better which is look at the cross entropy loss instead of looking at the squared error loss so i am not saying that the square error loss is wrong in this case but you co uld also use this cross entropy loss a nd in practice for our binary inputs the cross entropy loss often works much better than squared error loss refer slide time thirtyonethirtyone so let us see what i mean by the cross entropy loss so remember that you have n outputs right t hat i s why this summation let us not worry too much about what i s written inside for the time being i will explain that but that i s the i just want to explain the summation first so what you are saying is that for each of these green guys at the output you are going to make some loss a nd you just want to some over that loss that i s what we are trying to see now ideally you could have just written it as just done what you had done before a nd written this entire replace this entire bo x by this squared error loss a nd that would have been just fine right of course there should have also have been this summation i equal to one to m here because you are going over all the m training instances and for each of the m training instances you a re trying to minimize this loss so this two summations followed by this squared error loss would just have been fine refer slide time thirtytwotwentysix  but instead of that i have this something special here ok so let us look at what this special quantity is ok a nd now for that remember that i am trying to interpret each of these inputs as a binary random variable i am saying that they can take values zero or one so i can think of it that when i am given that this value is zero i can write it as this determinist ic probability distribution where i have p a nd the probability mass is entirely concentrated out on this zero value and my the probability mass on the value one is zero t his is something similar to what we had done earlier when we are given these labels suppose it was apple orange mango and banana a nd the class label was given to us that this is an apple t hen we could still write it as the probability distribution where all the mass was concentrated on apple and everything else was here so i am saying somet hing similar here right so you could think of it that two possible values can occur here one and zero and if i tell you this is zero right t hen i am telling you that with probability one into it is zero and with probability zero it is zero  so i still write it as a prob ability distribution now the same thing i can have at the output so for this unit when i am trying to reconstruct it and if i produce the output as zerotwo t hen i can or rather let us say zeroeight then i can say that with zeroeight probability i am predicting zero a nd wit h zero two i am predicting a one right so now i can think of this again as two probability distributions a nd once i some have two probability distributions i know that cross entropy is the right or a better loss function to look at right a nd what i s cross en tropy actually in this case it would be given by summation i equal to one to two right or rather i equal to zero to one because if the those are the values it can take p of i right into log of q i plus yeah  so p of i into log of q i that i s how i can write it so let me just since there are only two terms i can just expand this summation right so i can write it as p i or rather p zero log of q zero plus of course it is a minus sign here this is a minus sign at the out p one log of q one i can just open up because ther e are only two terms so i can write it as this is that fine ok now also i know that there is this relation between p zero and p one right t hat p zero is actually one minus p one yeah s imilarly you have this relation between q zero and q one that q one is equal to one mi nus q zero because the sum is going to be one ok now let us look at this sum right so in the binary case this sum becomes interesting because n ow suppose your input x i j right which is the entity that i am looking at suppose that was equal to zero i n which ca se all the probability mass would be concentrated on p zero and p one would actually be equal to zero which means the second term would display o n the other hand if x i j is equal to one then the reverse situation what happen that everything would be concentrated on p one t hat means p one is equal to one and this guy would become zero because p zero is going to be zero right ok so there is this another way of writing it that you could day that instead of x instead of writing p zero and p one you could just write it as x i j right into log q zero plus one minus x i j into log of q one so now let us look at it again  so when x i j is zero first which is the same which happened here just an refer time thirtysixtwenty in same thing right b ecause whenever s x i j is zero p zero is equal to sorry it shoul d have been q one and log q zero sorry i made a mistake here  so it have been x i j into log q so or rather let me just rewrite it so this is going to be actually i can write it as i look at this term first so i can write it as x i j into log q one and th en the second term i am going to write it as one minus x i j into log of q zero right a nd then i am going to simplify this further  but let see what is the consequence of this so now whenever x i j is equal to one this term will remain and the second term will disappear and that i s exactly what was happening in our original formula right so this is just an equivalent way of writing your x i j is equal to zero this term will disappear  but this term will remain  that means log q zero will remain this is exactly wha t was happening in our original formula right so that i s so now i have given you why a i can replace p zero and p one or rather p one and p zero by x i j and one minus x i j a nd now i can make a similar argument for x hat i j also so i can think of q zero as whate ver s predicted at the output right sorry i can treat q one as whatever is predicted out one output so whatever my sigmoid function predicts i can think of it as it is predicting the probability of getting a one right so it is just predicting the heads proba bility or the probability of getting one so i can instead q i q one i can write it as x i j hat and similarly instead of q zero i can write as one minus x i hat i g right so did you get that so these become very messy refer slide time thirtyeightthirteen so let me ju st clean this up and i will just go over this again right so what i was trying to tell you is that in the ideal case you could have just replaced this by the squared error loss but since you are dealing with binary inputs you can do something better be cause you can interpret the outputs as probabilities so when you get a zerotwo here you can interpret it as it is telling you that the probability of this unit being one is zerotwo it is very less a nd that i s the same as saying that the probability of this unit being zero is one right so you can interpret this as a probability n ow if you think of it that way then you can say that at the input you are actually given a probability distribution so which tells you that in the first case your probability distribution looks like one zero right because all the mass is focused on value zero because your input is zero at that case a nd now suppose your output was zerotwo right and zerotwo is what you are treating as a probability of  so this is the probability of one this is the probably of sorry this is the probability of zero oops and this is the probability of one so if your output is predicting zerotwo t hat means it is predicting zeroeight for zero and zerotwo for one n ow if you think of it this way then you can capture the loss function between these two guys using the cross entropy formula w hich is going to be summation i equal to zero to one p i log q i is that fine ok a nd now i just said that the since there are only two terms i can just write it as p zero log q zero plus p one log q one t hen i focused on this rela tion between p zero p one and your input so whenever your input is zero ok your p zero is going to be one so then i can just replace p zero by one minus my input right so if the input is zero then this guy is going to be one and that i s exactly what this expression is also going to be so i can write it as one minus x j log q zero and similarly for this second guy w henever input is one this guy is going to be one whenever my input is whenever my input is one this p one is going to be one whenever my input is zero this p one is going to be zero so i can just replace p one by log by x i j and now you can see that this expression evaluates to the same as this expression right you can substitute value of x i j zero or one you will get the corresponding p zero p one which would be one or zero depending on what yo ur input was and these two expressions will evaluate to the same thing so just as i replaced the p s by x i s x i j s i can similarly replace by a the q s by x i j hats right because once again q zero is nothing  but one minus whatever my output was predicte d b ecause whatever is predicted i am treating as the probability of getting a one so one minus that is going to be the probability of getting a zero  so that i s what q zero is and similarly q one i can replace by x hat i j a nd so that i s exactly what i have done i n this expression here so now this expression every term in these n terms captures the cross entropy for that particular random variable right so this is the original distribution p for this random variable t his is the predicted distribution q for thi s random variable and i have just told you that this the cross entropy between these two distribution can be written in this simple form as the function of x i j and x hat i j so this is the standard thing to do when you are dealing with bernoulli rando m variables  so you can go back and read up a bit about it ah  but for now i guess with this explanation it should suffice to know why this expression is used a nd remember that i am not telling you that this squared error function was bad i am just tell ing you that instead of the squared error function cross entropy loss function works better when you are dealing with binary inputs refer slide time fortytwothirtyfour so with that let us pursuit and the another we have looking at it is the following you can now look at this expression a nd tell me when is this expression going to be minimized so we have x i j and x hat i j you can see that this expression will be minimized only when x i j or rather x hat i j is equal to x i j right so now x i j could take v alue zero or one ok and now x hat i j could take zero one or zero one so you can see that for these two combinations the value is going to be minimized only when x hat i j is actually equal to x i j t hat means if x hat if x i j was zero then x hat i j should also be zero a nd similarly in this case also if x i j was one t hen the expression will be minimized only when x hat i j is equal to one so let us see this so suppose x i j was zero t hat means this term is going to go to zero but this term is going to remain and now if you are x hat i j was not equal to zero t hen you will get some log of one minus x hat i j as the loss right but if x hat i j was also zero then you would get log of one which is zero so this whole expression would then evaluate to zero which is the minimum possible value for this expression right s o  that means if x i j is zero then this expression will be minimized only when x hat i j is also equal to zero s imilarly if x hat i j sorry if x i j is one then this one minus one will give you zero so this term is going to disappear b ut this term will remain  so this will just be log of x hat i j because x i j is equal to one now if x hat i j is also equal to one then this is become log of one which is zero t hat means again this expression will attain it is minimum value when x hat i j is equal to x i j is equal to one right so this expression now attain it is minimum value in two cases when x i j is equal to x hat i j is equal to zero or when x i j is equal to x hat i j is equal to one  so compactly i can say that this expression will attain i t is minimum when x i j is equal to x hat i j that i s why this loss function makes sense refer slide time fortyfourfiftyfour now again we have this problem that we want to use back propagation to train this network a nd once again for back propagation we will n eed the following gradients the gradients of the loss function with respect to w and w star ok t his is what we are going to need and i am going to make this same argument again that whatever hard work you had done in the back propagation lecture you can j ust reuse all of it refer slide time fortyfivenineteen b ecause the only thing your changing is this final loss function so you just need to compute the gradients with respect to this loss function and everything else is going to remain the same right so th at i s exactly what i am going to do on this slide so whatever is in the boxes here these two boxes that is something that you have already computed a nd now what i am going to compute is the stuff which is outside the boxes  so let us look at that so i am interested in computing this dou l theta by dou h two this is the derivative of a scalar quantity with respect to a vector say it is going to be a vector a nd i am going to follow our usual recipe which is h two is actually equal to h two one h two two up to h two n so i am going to consider any of these guys which is h two j i am going to compute the derivative of the loss function with respect to this one entry and since i have that i am going to construct the entire gradient right so now i will have this dou l theta by dou h two j right and once i have that expression i am just going to generalize it to all the other entries in this vector so let us look at that expression first ok so now if you look at this actually it does not have an h two j right but we k now h two j is the same as x hat j or rather x hat i j right for the ith input it is going to be x hat i j b ecause h two is equal to x hat i ok you can just see that the top left corner of the slides say x two is equal to x hat i so this is nothing dou l the ta by dou x hat i j so now i want to take the derivative of this quantity with respect to one particular x i j and remember that this quantity has the sum which is indexed over j  so j goes from one to n i am looking at one particular j s o that means if i expand this sum of all the js possible the derivative with respect to all  but one is going to be zero because they do not depend on this particular j  so if i am looking at j equal to three then the term which has x hat i one is going to the derivative of that ter m is going to be zero so for all these terms in the expression only that term where a j is equal to the j which i am considering is going to remain ok s o that means only one term in the summation would remain and for that one term so let me just rid of t he summation right s o that means only one term in the summation would remain i am trying to find the derivative of this quality x which has a lot x i j s with respect to x i j so now this is of the form a log x so the derivative would a over x right so that i s exactly what i have written here and similarly for the second guy this is one minus a into log of one minus x so the derivative is going to be one minus a over one minus x and of course there is this minus sign here which will then get adjusted app ropriately right so that i s how this expression has been completely that i s very straight forward and now as you need the derivative of h two j with respect to a two j so remember that h two is equal to sigmoid of a two which means it is just an element wise s igmoid right so i just need to compute the derivative of the j th entry of h two with respect to the j th entry of a two all the other derivatives are going to be zero because they do not depend on that particular entry of a two  so now that is just going to be sigmoid of a two into one minus sigmoid of a two right so i have computed these two quantities i can just plug it then back into back propagation code t he rest of the code is going to remain the same and i have the gradients ready with me refer slide time  fortyninefourteen a nd as i said once i have this one guy i can just extend it i can just generalize it  so i just had these j s here right for h two j so i can just replace the j by one two up to n and i will get the same expression so that i s the end of module one where we introduced auto encoders w hat we showed is that they are actually just like any other feed forward neural network accept that they have this special objective t hat they want to reconstruct the input and the reason they want to reconstruct the in put is they about to first create a bottle neck which is this h hidden representation a nd then try to reconstruct from there and just as i gave you that compression analogy that you have this ten bits you want to compress it to four bits and then reconstruct the entire input again so this will happen only if these four bits capture everything that is required or the most important characteristics of your original input right a nd then we could have a loss function which tries to capture the difference between m y original input and my reconstructed input now we argued that this loss function will be dependent on the nature of your input so for the real inputs it was straight forward we just said that we can use the squared error loss function for the binary i nputs we actually did something special w e said that we can actually use the cross entropy and then we had this funny way of writing the cross entropy which was this x i into log of x hat and one minus x i into log of one minus x hat a nd just gave you some intuition that that is the same as writing p log a pi log or rather p zero log q zero plus p one log q one write and the i just gave you some explanation for doing that y ou can go back and check on how do you write the cross entropy for bernoulli random variables a nd you will see that this expression makes sense a nd once we had this expression computing the gradients was easy so the other thing that we relied on is that in the back propagation lecture we had taken care of everything up to this point and in this l ecture we have actually changed the loss function so one loss function was the sum of squared squared loss errors and the other loss function was the sum of sum of cross entropies whereas in the back propagation lecture we had only dealt with cross entro py by the case that we made is that sense you have this chain a ll you have done is change the last function in the chain right you have changed this l function all the other functions you have not changed y ou can just reuse the computations from these o r you can just use the code that you had written for these in the back propagation assignment a nd you just need to change this last guy to adjust for the change in the output layer or the change in the loss layer so with that we will end the introductio n to auto encoders there we have done we have actually covered how to train an auto encoder using back propagation"}
{"audio_filepath": "Data/Preprocessed/Derivative of the activation function_31.wav", "duration": 68.0, "text": "we have that activati on functi on and we were taking the derivative of the activati on with respect to pre activati on and i just pushe d it unde r the rug by sayingwe will write it as g dash s o i need to sho w you wha t g dash is w hat how to compute g dash so this is suppose g is the logistic function ok so that means what is z actually it is one of those a s right so this is the activation that you are going to feed it right and then you are taking the element wise sorry z is actually the pre activation that you feed it and then g is the activation function so i will do element wise activation function now what is the derivative of this s o i will just i will not do this derivation it is there and you end up with a very neat formula which is g of z into one minus g of z so now that bit is also taken care of is there any more spoon feeding that i can do y ou are ready for the assignment now i will do one more bit you will also have used a tan h function  so th is is the derivative of the tan h function it again boils down to a very n eat formula which is one minus g of zd whole square  so we will end this lecture"}
{"audio_filepath": "Data/Preprocessed/Deep Learning(CS7015): Linearly Separable Boolean Functions_16.wav", "duration": 333.0, "text": "so in this module we look at linearly separable boolean functions again and we will try to make some more statements about them so what do we have do so the guiding question that we have is what do we do about functions which are not linearly separable and let us see one such very simple function can you guess what function i am going to talk about all of you are paying attention in the first lecture so here is the xor function now these are the set of inequalities that result from xor function i hope right now let us see the first condition implies that w naught should be less than zero second condition implies this third condition implies this fourth condition implies this just looking at this can you tell me can you find a configuration for w naught wone wtwo such that these inequalities can be satisfied together no right because two and three want it to be greater than minus one minus w naught and when you take an addition of that it has to be less than minus one so that is not going to happen so you see a contradiction so this is a simple boolean function which the perceptron cannot handle because it is not linearly separable it is not linearly separable there does not exist a line if there does not exist a line you cannot find the line in fact you can look at it visually so these are the red points for which the output should be zero or one and the blue points are the points for which the output should be zero if we need to change this i think we were using blue as positive and red as negative and you cannot just draw a line there is no way you can draw a line such that the blue points lie on one side and the red points lie on the other side so it is a simple two input function so it is not that i have taken a very contrived example most real world data is not linearly separable and it always contains some outliers right so here maybe you have some data where you are trying to say that people which live in this part of the world belong to a certain or maybe people who live or work here have a certain qualification people who work in this company may have a certain different qualification and there might be some outliers right it is not that is always going be very clean so now what do i mean and it is not necessary that the points will only be outliers in fact there could be a clear case where there are no outliers but still you cannot find a line such that you separate the positive from the negative can you think of such an example good right this is clear data there is no outliers here as well i mean it is just saying that everyone who lies within this boundary has a certain characteristic and outside that boundary people have a different characteristic right and there is no outlier here but you cannot separate this data with a line so all functions that you deal with will not go or are not going to be linearly separable so we have to work around those right and while a single perceptron cannot deal with this we will show that a network of perceptrons can indeed deal with such data so that is where we are headed so before going there we will discuss some more boolean functions in more detail and i will try to see what kind of nonlinearly separable boolean functions are there so first of all how many boolean functions can you design from two inputs how many can you design sixteen looks like a good number from three inputs two hundred and fiftysix how many if you understand this let us see so let us begin with some easy ones that you already know right so these are two inputs xone xtwo what is this function always off the other extreme is always on and i have already given you the answer f sixteen so then you have the and function and or function then some other functions right so why did you reach sixteen actually because with two inputs we will have these four values to take care of and each of these are again binary so you actually have two raise to two raise to n right so for three inputs two raise to two raise to three would be two hundred and fiftysix now that is the easy part of these how many are linearly separable i will have to do any actually stare it in and seriously try to find the answer when you cannot really do that so turns out all of them except xor and in not of xor ok so for the two input cases therearetwofunctionswhicharenotlinearlyseparableforninputshowmany functions would be not linearly separable it is an arbitrary n is not the answer you are not going to disappoint me not n ok but what is the answer so for n inputs we will have two raise two n functions of these we do not know how many are going to be not linearly separable that is not a solved problem although i encourage you to go and find the answer i am looking for a good will hunting kind of a moment but all it suffices to know is that there exists some which are not linearly separable and that everyone agrees that there exists some right and as n grows probably that number will increase and so on but it is not known exactly you cannot write it as a function so what we have done so far is looked at boolean functions how many boolean functions can exist and of that we just have concluded that there would be some which are not linearly separable"}
{"audio_filepath": "Data/Preprocessed/Recurrent Neural Networks_104.wav", "duration": 543.0, "text": "so we have seen sequence learning problems now we are interested in the question of how to model these right so we look at something known as recurrent neural networks and our question that we are interested is how do you model tasks which involves such sequences ok so here is the wishlist that we have what will model will come up with should account for the dependence between inputs because that is the strong case that we have made that the output actually depends on multiple inputs and not just a single input you should also account for variable number of inputs because a video could be three hundred seconds it could be twenty seconds twentyfive seconds a sentence could be of arbitrary lines and so on and it also makes sure that the function at each time step is the same right but every time s tep they are trying to do the same activity ok so we will focus on each of these items from our wishlist and then try to arrive at a model for dealing with such problems so first let us ask this question what is the functio n being executed at each time step what is the function being executed at each time step either should come after dealing you have an input your ability to go blank on me is just amazing actually you have a hidden representation and then you have an ou tput the first time we are seeing this situation in the entire course where we have an input a hidden representation and an output what is the function being executed remember the output is always a function of the input lecture two or three i do not know definitely not lecture fourteen so what is the function can you write y i as a function of x for my sake if not for god sake you can ok what is it ok first tell me what is s one u s one no nonlinearity no bias which all that plus bias then no nonlinearit y who cares about nonline arities ok and then what is y one  ok some output function ok for some reason i have written sigma here i will just call the output function as o always maybe in this case sigma would work but o is what i will call it just to m ake the this thing clear ok is that fine so this is the function being executed at this every time step we can just write it using these two equations which are seeing for a first time and i is a time step since we want the same function to be execut ed at each time step we should share the same network at every time step that means what do i mean by share the same network share the same parameters good so this is the same as this because u v and b and c are the same ok so that is an easy way of taking care of the requirement that i want the same function to be executed at every time step and this parameter also sharing also ensures that the network becomes agnostic to the length of the input because now whether i have a word which has ten characters or twenty characters it does not matter because at every time step i am going to execute the same function that is why it is important that at every time step we have the same function so since we are going to complete t he same function the number of times it does not matter and we can just create multiple copies of this network that we have and for any arbitrary length n we can still compute the output ok still not quite there we still need to take care of a lot of th ings but we are just slowly addressing each item from our wishlist now how do we account for dependence between inputs or rather actually the right way of asking this is how do we account for the case that the output actual ly depends on multiple inputs and not just the current input ok how do we account for that feed in the ok good so let us first see an infeasible way of doing this ok so you are given the first time step x one  you have a network which predicts one from x one  you know at the same second time step you also want to look at the previous in puts so why not just feed it x one and y x two both and then try to predict y two at the third time step feed in x one  x two  x three and predict y three and so on forever is this fine proba bly the word infeasible is there so y is so what is the problem with this yeah good so i am looking in terms of the conditions that we have on the wishlist which condition does is violet what is the function being executed at each time step ok so  let us see the function being executed at ever y time step is different so y one is function of x one  y two is a function of x one x two  remember that this is not just saying that you are passing to inputs everything changes bec au se you now you need to have u one and u two here you need to have u one  u two  u three  right so everything changes it is not the same function how many of you get this it is a different function being executed at every time step so now if i have a sequence of le ngth one hundred what happens i need y one hundred which takes f x one to x one hundred as inputs and has how many parameters u one to u one hundred right you could you could share u one to u ninetynine for y ninetynine and y one hundred but we still need those many of that right so that network is now sensi tive to the length of the input and on the length of the input goes you will have to construct more and more functions right and imagine that if the training time the maximum sequence length that you had seen was twentyfive and suddenly a test time you get a se ntence which is of length thirty you do not even know how to compute that because you have not train any parameters for doing that so then the final solution is actually to add a recurrent connection in the network why does this work ok before that now can you tell me what is the function being executed at every time step assume there is a s zero here these are a s one  s two  s three up to s n and there is a s zero now what is the function being executed at every time step can you writ e it down if you it would help if you think in terms of y two and not in terms of y one  y one is the boundary case was special case the thing in terms of y two or any other of the ys and first think of what s two is from s two y is straight forward how many of y ou can write the function so s i in general s i is u into x i plus w into s i minus one plus b how many of you get this and then what is y i again this has to be output functions ok but how does this solve our problem does it take care of everything on the wishlist one the way we have written it in terms of i which is the time step definitely the same function is getting executed at every time step there is no doubt about that right modulo this boundary case of s one where will assume that there is an s zero ok so same function being executed at every time step can you deal with inputs of arbitrary length yes as long as you ensure that the same function is executed its fine does it ensure that the output is actually dependent on the previous inp uts how student refer time sevenone s i through s i minus right so that is an interesting thing that this guy actually depends on this guy which depend on the previous input and also on this guy which in turn depends on the provision inputs so rec ursively you can see that you depend on all the previous inputs that you had ok that is a very neat way of ensuring that your output depends on all the previous inputs and you do not blow away the parameters blow of the parameters by sharing this recurre nt connection and that is this is a compact way of writing is that your y i is now function of x i s i and has these parameters w u v and b and c so s i is called the state of the network at times step i and what is see here this is just for the sak e of completion this is known as a recurrent neural network because of this recurrent connection and s i is a state of the network at time step i so as when you start working in deep learning and you are dealing with sequence problems state of rnn or sta te of lstm or state of grv something that you will be hearing or reading often so this is what you mean by the state of the recurrent neural network this is the current state which kind of encode everything that is happened so far right it has a encodi ng of all the inputs that you had seen so the parameters of the network are w u and v which i shared across time steps i obvious forget the biases and the same network is getting executed every time steps i do not need to worry about whether i am comput ing y one  y two  y three or y one hundred  right so everyone agrees that this solution takes care of all the things that we had on our wishlist how many of you agree with that and this is a more compact way of representing that that you sa y that you compute s i and then you are feeding it back so this is just more compact way of representing a recurrent neural network so let us now revisit the sequence learning problems that we have seen so now just correc t each of these networks so what would happen each of these things i was thinking of all the inputs as independent so now what will i do what is the only thing to be done just add the recurrent connection right so once you add the recurrent conne ction now you can go back and relate to all these problems that one i am trying to predict the character which appears after e i also have the information of d and e and same argument you can make for all the other examples that you have but i am tryin g to predict this final state i have the information of all the previous inputs here ok"}
{"audio_filepath": "Data/Preprocessed/Train error vs Test error_58.wav", "duration": 639.0, "text": "so we would start the next module where we will talk about training error versus test error and before that we will see this bias variance tradeof f so now what have we done so far in these complex models and the simple models we have trained them using the dash data training data and what are we interested in always a test data right i already know what was the oil amount of oil mined from the training data locations that i was given and i am not interested in predicting those i am less interested in learning those so that if you give me a new location i should be able to do the right prediction so i am always interested in the test data so now consider a new point which is not seen during the test data and there are several such points that you could see now  if you use the model f hat x to predict the value of y then the mean square error is given by you get this it i s just the expected value of this squared error that i will get so what is the randomness here y expected value because the x that i am going to feed at test time is going to vary for each of these dif ferent xs i will get a dif ferent error  so hence that is a random variable do you get that so please focus on these things right i mean just do not take a formula for granted just see what is it trying to see so whenever you see an expectation over something always question what is a random variable here so what is the random variable here it is the squared error loss why is it random it is because it changed the input x you are going to try it over a multitude of test examples you will take one thousand text examples tenzero text examples and so on right for each of this you will get a different squared error  that is the randomness so you want to see what is the expected value of this or very loosely speaking the average value of this nowit turns out that this now  just try to remember that this is also some expectation and you had the terms f x and f x hat here this also had some expectation and term f x and f x hat and so on right if you do not remember the exact formula it is ok but you do remember there were some expectations inside and the terms f x and f x hat whether they are so this is just simple you are dealing with a minus b the whole square on the left hand side if you if you open it up rearrange some terms you will get this right so you can show that the mean average or the expected square error on the test data is actually the bias square plus the variance that is a small amount of irreducible error  you can go back and work this out and actually the proof is given here on the link ok but i hope you get the intuition you have this a minus b the whole square if you open it up and rearrange the terms you should be able to get this now  what does this tell you what happens if the bias is high the squared error is going to be high what happens is if the variance is high it is going to be high so that is why you do not want a very high bias you do not want a very high variance also you want this sweet spot in between where the bias and variance are just about optimal you get that that is why there is a tradeof f between bias and variance you cannot rely on simple models which have high bias you cannot rely on complex models which have high variance you want something in between now  the parameters of f hat x remember that they are trained using the training data which consists of these end points that you have at test time we are interested in evaluating the model on a validation set which was different from the training data this gives rise to the following two quantities one is the training error which you deal with at dash time training time that is the error that you are trying to minimize right but a test time you have a different error which is the test error and that is the error that you care about typically these two errors exhibit a certain trend do you know what the trend is now  on the x axis i have model complexity and on the y axis i have error  as a model complexity increases what would happen to the training error it will go to almost zero that is exactly what happened from the linear function to the polynomial function this is how it will behave as the model complexity increases as the model complexity increases what would happen to the validation error it will decrease up to a certain point right because you are still not over fitting on the training data your answers are still generalized so you had this degree one polynomial degree twentyfive polynomial if i take in something in between then probably this is where i would have ended up with the training error and that would not have been too bad for the test error  you see this ok now  you see i will mark two points two regions rather one of this corresponds to high bias the other one corresponds to high variance tell me which one is which do this i cannot understand so let me ask this is this is ok good so you see that there are these two extreme and we want somewhere to be in between ok at least you get the intuition behind this fine ok and you are looking for this sweets spot which is the perfect tradeof f between the bias and the variance right so now everyone gets why there is a tradeof f and how this relates to model complexity and therefore we are looking for the ideal model complexity  how do we achieve the ideal model complexity well we cannot really  ideal is ideal but we try to do this using dash what is the title of this lecture student regularization regularization i will try to use regularization to achieve this ok so let us formalize this a bit more and remember that this curve is actually because of this equation that you see right high bias you will be in this region i am actually inserting it ok fine ok so the intuitions that we have developed so far is that if there are n training points and m test points then we have a train error which goes over the training points and we have a test error which goes over the m test points ok so i am just taking a total of n plus m points the first n is training the next last m is test now  as the model complexity increases what happens to the training error  it becomes very optimistic and gives you a very wrong picture of how close the predicted function is to the true function whether it makes you feel that you have done a perfect job this you have actually discovered the true function but that is not correct it i s giving you a false picture of that therefore we should always look at the dash error  student validation error  validation error so now you see that why you always do this train validation and test split test is unseen you try to optimize on the training error ok but you should always tune for the validation error  your optimization algorithm is going to take in the training error  it is going to be very optimistic it is going to try to drive to zero but you should look at the validation error and try to see that you are not over fitting on the training data everyone gets this intuition so now this is all intuition we will have to concretize this mathematically  so that is what we will do now so that d be these training test points that we have we know that this relationship holds we do not know what f is but we know that this relationship holds so what am i trying to say here that we know that there is a true relation between y and x which is given by the function f but i am also willing to admit some noise that may not be a very neat function but a small noise might exist that i s the epsilon i ok and i am going to assume that epsilon comes from a normal distribution with zero means so on average the noise is going to be zero but there is a small variance everyone gets this this is a true relation but i am willing to admit some noise in the relation ok fine and of course we do not know f we never know f right now  going by our paradigm where we have these five components we use f hat to approximate f f hat will have some dash which will i try to learn from the training data what is this dash parameters right which will try to learn from the training data the training data t is a subset of your total data which is thus those endpoints right and we are interested in knowing this quantity  this is what we are actually interested in can we compute this quantity how many of you say yes how many of you say no we cannot why cannot we compute it we do not know f so why cannot you raise your hands if you all can answer in chorus so we do not know what f is then how do we compute this quantity right but what do we actually know  so now we are going to see something which is true expectation and something which is empirical estimate expectation how many of you know this what is the difference between the two most of you should but it is not confident about it ok so we do not know what fxi is the true thing but what do we know  we are given some training data right we know these yi s for was training data and we know these yi hats for those training data so this is something that we can estimate yes or no this is given to us so this expectation is going to be an empirical estimate right because we are going to look at some one thousand ten thousand twenty thousand training points and estimate this right it is an empirical estimate how many of you get that now  i am just going to rewrite some of this so what i have done is i just defined that yi is equal to fxi plus epsilon i so i have just replaced yi by that ok is that fine now  this is of the form a minus b the whole square so i am going to treat it as that and just open up the bracket so i will have a square minus two a b plus b square and now this is a sum or difference of expectation so i can push the expectation inside so this is what i get this is this fine ok now  i am just going to rearrange the term so remember this was the quantity that we were actually interested in but this is the quantity that we had a handle over because these were the data points given to us so i will just rearrange the terms and i can write this which was my quantity of interest as this can you estimate everything on lhs on rhs this this what is this variance sigma square we assumed it came from zero sigma square distribution and this can estimate the answer is no for the same reason we do not know what f of x is ok"}
{"audio_filepath": "Data/Preprocessed/Backpropagation: Computing Gradients w.r.t. the Output Units_27.wav", "duration": 983.0, "text": "now we go to the next module where we w ill first see how to c ompute the gradient with respect to the output u nits well that was the first guy in our chain right that is the first p erson that we need to talk to so that is the part that we are going to focus on re fer slide time zerotwentysix so this is the output and when i say i want to compute the gradient with respect to output unit  what do you actually mean what is the quantity that i am looking for i will help you out actually what i meant by output unit is t his entire thing right so i actually meant al s ok but it is it is a fair answer and even y hat is a fair answer ok in fact am going to start with y hat and then go to al  so i will have to start with this guy and then come to this guy refer slide time zerofiftyfour so this is the loss this is y hat which is equal to y one hat y two hat up to yk hat so these are the k values that we have here and we are looking at cross entropy that means we are looking at the classification problem right so we ha ve got a distribution over the k classes that is what y hat looks like and we know that one of these guys is the right class maybe say y two so the loss function is minus log of y hat two because two is the correct class in this toy example that i am consid ering ok so the loss function i am just repeating the definition right tha t is how the loss function is now oh god so again this is what our y hat looks like ok now i want to compute the gradient with respect to any of the output units right so it could be y one y two y three y four up to yk right so this i actually can take values from one to k in this case one to two right ok now can you tell me what is this loss ok this much is fine can you tell me what is this derivative minu s one by minus one by y hat l if y is equal to l student refer time twoeight and zero otherwise how many of you get that cool ok so it is a very simple thing that you can think of this as z and this is y only if z is equal to y then the derivative would ex ist otherwi se it is going to be zero right ok so how do i write this fe part using student refer time twotwentyseven how many of you have seen indicator variables before good so this i s what you are telling me right it is going to be minus one by y hat l if i is equal to l ok and if i is not equal to l then these two things are not related it this is a function of something else and you are taking a derivative with respect to a different quantity so it is a constant with respect to that quantity and the a nswer would be zero ok now i am go ing to write this as this right so this is the same as saying so this variable actually this is known as the indicator variable it takes on the value one if the condition in the bracket holds otherwise it takes on the val ue zero so this is exactly i am writing exactly this but in a more compact manner ok  is that clear to everyone so this is what the quantity this is the quantity that we have computed with respect to one of the output units ok so this is what derivative partial derivative gradient how many of you say derivative no one likes derivative partial derivative that is always the safest choice partially fl right and gradient oh there is one brave soul who say is gradient d o not worry well fix that ok so this is the partial derivative y because my y hat is actually a vector and i am taking the derivative with respect to one of those guys ok now if i want the gradient with respect to y hat what would that look like a ve ctor which is a collection of student refer time fourone p artial derivatives so let us see this is the quantity that i am interested in am interested in the gradient of the loss function with respect to the vector y hat so remember the vector y hat is y one hat y two hat up to yk hat right so this gradient is going to be a collection of the partial derivatives with respect to y one hat y two hat and so on now wh at is each of these quantities so it is simple right so this quantity the derivative is e ither going to be zero or is it going to it is going to be one by y one hat right if l is equal to one right and that is exactly what i have done so now how many elements here are actually going to be nonzero at a time how many of these going to be nonzero one  which one student  refer time  fivefour the one corresponding to l right everything else is going to be zero so this is a dash vector y not vector ok so now am going to write one hot vector like this what have we done ok where el is what one hot vector such that it is l th entry is one ok that is what am going that is how am going to define e l is that fine with everyone ok and so you see the story how did how we went about computing this we started with a partial derivative with respect to one of t g uys right we found a formula for y i we saw that this formula is generic enough and so now we can compute the gradient which is a collective of all these yis where i ranges from one to k right and then we just put that in a gradient vector so this sto ry is going to repeat throughout the lecture where we try to compute the gradient with respect to one guy and then generalize oh sorry we compute the partial derivative with respect to one guy and then generalize and try to find the gradient fine ok refer s lide time sixeighteen so what if i what do i have so far i have this quantity what does till which part of the diagram am i currently the dash green part dark green part i am till here i need to go till the light green party that is collectively the output unit ok although i have divided into two halves but when i say output unit i mean that output neuron right complete neuron so what i am actually inter ested in is these quantities or more specifically ok this is what i am interested in what is this one of those gu ys right this al is actually alone up to al k right so this is one of those guys  so  this is going to be the gradient or this is going to be the derivative a partial derivative sorry ok now what do how do we p roceed from here now i will again have to compute this you already know that good but before that i want you to answer on e question right so y hat l  what is y hat l it is the output corresponding to the correct class do es it depend on an arbitrary al i so in the previous thing we saw that only when i is equal to l there is a connection in this case is there a connection always or only when i is equal to l student refer time eighttwo always why softmax so student  refer time eightzero four denominator has all the ali s right so this is there it is y hat l in the numerator of course it only has this unit which corresponds to the l th probably did not choose my variables very well so l th component of a capital l r ight and but in the denominator you have the entire sum which means that every output guy here each of these dark green guys depends on each of the dash green guys light green guys good so that is at least settled that we always the we can always com pute this partial derivative we do not n eed an if else here there is no thing like l is equal to i then what will happen it will always have this partial derivative so we will now derive the full expression for this so this is what we are interested in is this fine so this is a function of the form so you are taking how do i say this so this is log of a function so first you will take the derivative with respect to log and then push the partial derivative inside right so that would be minus one by y hat l and then the derivative with respect to y hat l now what is y hat l the softmax function right so it is the l th entry of the softmax function applied to that output vector what is the output vector al right so it is the lth entry of the softmax or l th entry of the function applied to the output vector so this was our al what is our output right so now one of these guys here is the l th guy and one of these guys here is the l th guy right so what you do i s you take this you apply the softmax function to it which again gives you a vector and now you are interested in the l th component of that vector that is what this quantity means it should be clear now now i will just do so me simple math stuff here and we should be able to derive this is it fine am just replaced by the actual softmax formula this is a derivative of the form u by v right what is the formula for that yeah it perfectly right yeah so this is what it would be right i mean it is you all know this i am not go ing to spend time on this so now am just going to substitute the values here yeah it is getting a bit nasty but it is not very difficult right so so this so this is our g of x so am taking the deri vative of that then this is this one over h of x you can just figure it out right anyway it everyone just read this for a few seconds and let me know if this is not clear this is g this is h in this formula right have just substituted the gs and hs in this now what is this quantity going to be it is derivative of the form e raise to x right so it is e raise to x always student refer time eleventhirtythree if i is equal to l right  s o now we have this dependence because we are looking at a numerator but the num erator only depends on the l th entry right so now you are trying to take the derivative of the l th entry with respect to some arbitrary i th entry so only if l is equal to i yo u will get the derivative right now what abo ut this how many terms in the summation would remain student refer time twelvetwo one which one student refer time twelvefour where i dash is equal to i right so the i th guy would remain the rest of it is straightforward right this square i have jus t divided into two parts ok ah now let us see can you simplify this because i cannot ok can you simplify this what is this student softmax softmax and which entry of the softmax student refer time twelvethirtysix l th entry i th entry l th entry with t he saw with the indicator variable but what is this this is our input hidden layer output  so ok now let us see what is the next step t his is should have been y hat i but y hat is equal to f of x right so we can fix this unit so ok fine so we ha ve actually what do we have now we have the derivative of the loss function with respect to the i th unit of the output layer right and which part of the output layer the pre activation pattern ok now what am i going to do i have a formula which tell s me how to compute this what was i actually interested in so now how am i going to go from here to there i just put all the partial derivatives into a student vector vector and that vector is the student refer time thirteenfiftyeight gradient good refe r slide time fourteenzero so we have this one formula it is ok if some of you did not get this derivation right it is very very straightforward if you go back and look at it i am pretty sure you will get it is nothing in this is very simple elementary s tuff right except for some degree here and there ok so now what would this look like we should add actually l theta here this would look like a collection of all the partial derivatives we have a generic formula what will we do now what is the fir st entry minus in indicator l equal to one minus y hat one which is the variable that we are indexing over i right not l oh god oh we are indexing or ok have i goofed up oh that is wrong is it oh yeah that is wrong fine t hen this is fine we are indexing over i and then we can do this now can you simplify this i am looking for ok this is the element wise difference of two student refer time fifteenthirtyeight of the indicator vector and student y hat y hat oh hey we should change all this y hat is equal to f of x right but i just want it to be consistent as y hat so is this fine this is a simplification fine right so we have come a long way right you have finish this part ok we have got the gradients with respect to the out put units ok this much part is a clear to everyone moduler bit of the math which you can go back and look at it this entire derivation is fine but you get the concept right that we start with one unit from there grow the gradient then keep goin g applying the chain rule so we started with the dark green guys and then went to the light green guys now we have the derivative with respect to the entire light green vector and that is what we had started off with that we wanted the gradient wit h respect to t he output units"}
{"audio_filepath": "Data/Preprocessed/Perceptrons_12.wav", "duration": 615.0, "text": "now let us go to the next module which is perceptron  so far the story has been about boolean input but are all problems that we deal with we are only dealing with do we always only deal with boolean inputs so yeah so what we spoke about is boolean functions now consider this example this worked fine for a movie example where we had these as actor so much and his director and so on but now consider the example where you are trying to decide you are in oil mining company and you are trying to decide whether you should mine or drill at a particular station or not now this could depend on various factors like what is the pressure on the surface on the ocean surface at that point what is the salinity of the water at that point what is the aquatic marinaaquaticlife at that point and so on so these are not really boolean function the salinity is a real number density would be a real number pressure would be a real number and so on right and this is a very valid decision problem companies would be interested in doing this so in such cases our inputs are going to be real but so far mcculloch pitts neuron only deals with boolean inputs so we still need to take care of that limitation now how did we decide the threshold in all these cases i just asked you you computed it and you told me right but that is not going to work out i mean it does not scale to larger problems where you have many more dimensions and the inputs are not boolean and so on so we need a way of learning this threshold now again returning to the movie example maybe for me the actor is the only thing that matters and all the other inputs are not so important th en what do i need actually i need some way of weighing these inputs i should be able to say that this input is more important than the others now i am treating all of them equal i am just taking a simple sum i f that sum causes a threshold i am fine otherwise i am not fine  but maybe i want to raise the weight for some of these inputs or lower the weight for some of these inputs so whether it is raining outside or not maybe does not matter i have a car i could go or i could wear a jacket or an umbrella or something so that input is probably not so important and what about functions which are not linearly separable  w e have just been dealing with the goody stuff which is all linearly separable but we will see that even in the restricted boolean case there could be some functions which are not linearly separable and if that is the case how do we deal with it so these are some questions that we need to answer so first we will start with perceptron which tries to fix some of these things and then we will move forward from there so as we had discussed in the history lecture that this was proposed in one thousand nine hundred and fiftyeight by frank rosenblatt and this is what the perceptron looks like d o you seeanydifferencewiththe m ccullochpittsneuronweightsyouhaveaweight associated with each of the input otherwise everything seems so this is a more general computational model than the mcculloch pitts neuron t he other interesting thing is that of course we have introduced these weights and you also have a mechanism for learning these weights so remember in the earlier case our only parameterwasthetawhichwearekindofhandsettingright but nowwiththe perceptron we will have a learning algorithm which will not just help us learn theta but also these weights for the inputs how do i know that actor is what matters or director is what matters g iven a lot of past viewing experience past given a lot of data about the movies which i have watched in the past how do i know which are the weights to assign this so we will see an algorithm which will help us do that and the inputs are no longer limited to be boolean values t hey can be real values also so that is the classical perceptron but what i am talking about here and the rest of the lecture is the refined version which was proposed byminskyandpapertwhichisknownastheperceptronmodelsowhenisay perceptron i am referring to this model so this diagram also corresponds to that so now let us see what the perceptron does t his is how it operates i t will give an output of one if the weighted sum of the inputs is greater than a threshold so remember that in the mp neuron we did not have these weights but now we have these weighted sum of the inputs and the output is going to be zero i f this weighted sum is less than threshold not very different from the mp neuron now i am just going to do some trickery and try to get it to a better notation or a better form so is this i have just taken the theta on this side n ow is this notice this here the indices were one to n n ow i have made it zero to n and the theta is suddenly disappeared so what has happened student w zero is minus theta right and xzero is one d oes anyone not get this right if i just start it from one to n then it would be summation i equal to one to n wi xi plus wzero xzero but i am just saying wzero is equal to minus theta and xzero is equal to one which exactly gives me back this right so very simple xzero equal to one and wzero is equal to minus theta so in effect what i am assuming is that instead of having this threshold as a separate quantity i just think that that is one of my inputs which is always on and the weight of that input is minus theta so now the job of all these other inputs and their weights is to make sure that their sum is greater than this input which we have does not make sense so this is how this is the more accepted convention for writing the perceptron equation so it fires when this summation is greater than equal to zero otherwise it does not fire now let me ask a few questions so why are we trying to implement boolean functions i have already answered this but i will keep repeating this question so that it really gets drill in w hy do we need weights a gain we briefly touched upon that and why is w naught which is negative of theta often called the bias so again let us return back to the task of predicting whether you would like to watch a movie or not and suppose we base our decisions on three simple inputs actor genre and director n ow based on our past viewing experience we may give a high weight to nolan as compared to the other inputs so what does that mean i t means that as long as the director is christopher nolan i am going to watch this movie irrespective of who the actor is or what the genre of the movie so that is exactly what we want and that is the reason why we want these weights now wzero is often called the bias as it represents the prior so now let me ask a very simple question s uppose you are a movie buff w hat would theta be zero  i mean you will watch any movie irrespective of who the actor director and genre now suppose you are a very niche movie watcher who only watches those movies which are which the genre is thriller the director was christopher nolan and the actor was damon then what would your threshold be three high in this case i always ask this question do you know of any such movie always takes a while i nterstellar so the weights and the bias will depend on the data which in this case is the viewer history so that is the whole setup that is why you want these weights and that is why you want these biases and that is why we want to learn them now before we see whether or how we can learn these weights and biases one question that we need to ask is what kind of functions can be implemented using the perceptron and are these function any different from the mcculloch pitts neuron so before i go to the next slide any guesses i am hearing some interesting answers which are at least partly correct so this is what a mcculloch pitts neuron looks like and this is what a perceptron looks like t he only difference is this red part which is weights which has added so it is again clear that what the perceptron also does is it divides the input space into two halves where all the points for which the output has to be one would lie on one side of this plane and all the points where which the output should be zero would lie on the other side of this plane so it is not doing anything different from what the perceptron was doing so then what is the difference you have these weights and you have a mechanism for learning these weights as well as a threshold w e are not going to hand code them so we will first revisit some boolean functions and then see the perceptron learning algorithm so now let us see w hat does the first condition this condition if i actually expand it out then this is what it turns out to be and what is that condition telling me actually w naught should be less than zero clear so now based on these what do you have here actually what is this a system of l inear inequalities right and you know you could solve this y ou have algorithms for solving this not always but you could find some solution and one possible solution which i have given you here is wzero is equal to minus one wone equal to oneone and wtwo equal to oneone so just let us just draw that line so what is the line it i s oneone xone plus oneone xtwo is equal to one that i s the line and this is the line and you see it satisfies the conditions that i have is this the only solution possible n o right i could have this also as a valid line if i could draw properly right all of these are valid solutions so which result in different wone w naught and w zeros so all of these are possible solutions in fact i have been tellingyou thatyou had to set thethreshold by hand for the m cculloch pitts neuron but that is not true because you could have written similar equations there and then decided what the value of theta should be so you could try this out for the mcculloch pitts neuron also you will get a similar set of conditions or i mean similar set of inequalities and you can just say what is the value of theta that you could set to solve that"}
{"audio_filepath": "Data/Preprocessed/L2 regularization_61.wav", "duration": 1408.0, "text": "refer slide time zerothirteen so let us start with ltwo regularization  so i have seen th is before refer slide time zerofifteen so all of you see that t his is ltwo regularization right what does ltwo regularization does now tell me in the context of things that we have discussed today what is this empirical estimate of the train error ok and w hat is this is that fine right so everything that we are going to write is l because of its w  but fine right ok now why does this relate to model complexity what am i doing here actually by adding this so they are going to see a very detailed anal ysis of this  but i just want to see first whether you get an intuition behind this  so by doing that what you are trying to do not allow the model to become very complex right you do not want a model where your weights can take any possible value you just want the weights to be small  so you are reducing the freedom on the model right less freedom less complex you get the intuition at least we will see this in more detail  but at least you get the intuition why we are doing this so we are using o mega remember that we are using this omega theta as a surrogate for model complexity  so if you add something in all omega theta just make sure you understand that this relates to model complexity ok fine and now for sgd what would i need for gradient descent just in case you have forgotten what sgd is what do we need n othing we have done it fl gradient of this which is a sum of the derivatives of the two quantities of which you know one right you know this already and what is the other guy alpha w right refer slide time onefiftynine so you see this ltwo regularization right one reason why it is preferred is now imagine you have already written code for gradient descent all you need to do is change it at one place add this to your update rule that is all you need and you can think of the vector form of this where you have a vector of parameters you can think of the matrix form of this variable vector matrix of parameters all you need to do is add one term to your update rule  so it can be done with very minimalistic change and this would be your update rule now let us see geometric interpretation of this refer slide time twothirtysix now from here onwards some of you will start getting a bit uncomfortable with some of the math because of these assumptions that it only works for squared eggs in a vacuum right so you will see those kind of things i will not tell you upfront what is the assumption i am making because that will just spoil the analysis you will just not enjoy it as much a s you would ignorance is bliss right so if you do not know what the assumptions are you will probably enjoy it more b ut for some of you will pick it up just keep it to yourself at the end i will tell you what are the assumptions i had made ok there are some tricky assumptions that i want to make  but just live with it and just try to enjoy it while those assumptions last right ok so now let us assume that w star is the optimal solution for l w what is l w the train error not our regularized err or just the train error a nd  so if w star is the optimal solution what can you take tell about the derivative with respect to w star or derivative at w star sorry it i s going to be zero from basic calculus right so which i say minimize x square the mini ma is where derivative of x squared with respect to x is equal to zero right so now consider one point which is ok  so what i actually want to consider is that let me just see how to see this  so let us see my w star ok and i want to consider some point in the neighborhood of w star ok that is what i want do  so one way of saying it is that h is equal to w minus w star is that fine ok  so that is what i am going to use in the next few steps refer slide time foursixteen so suppose i have such an h whi ch is equal to w minus w star  that means i can move from w star to some point in its neighborhood by using h and what does taylor series tell us this is what taylor series tells us right that the value of the function at this neighborhood point is equ al to this all of you know taylor series well no w it is that fine i do not need to really go over this right this is approximation up to the second term second order derivative now what was h actually w minus w star  so i will just substitute that and this is what i get is that fine what is this quantity one minus zero infinity minus infinity zero right we just did that ok  so that term will disappear what am i left wit h this quantity ok and i have forgott en what is next now again i am interested in th e derivative of this ok  so what will happen if i take the derivative what would i get i am interested in computing grad l w what will the r h s be how many of you fine with this remember this is a quadratic form right so this is of the form x squar e that i s i mean that is roughly how i remember it is not correct because of the form x square  so when you take the derivative one of the x is will disappear and this quantity will remain ok  so everyone gets this ok so now what do i have is i have the formula for the gradient with respect to l w and it is in terms of the gradient with respect to or rather the gradient at l w star that is what i have achieved  so far  but what am i actually interested in the regular ized loss i am what i am stil l dealing with is the non regularized loss this is just the empirical estimate of the training error that is not what i am interested in i am interested in the regularized loss how many of you lost at this point h is the second order derivative oh  s o these are brackets just for clarity  but i see it i s making it more unclear y e a h actually we should have used u and then call it u transpose h u  so it i s the brackets here are not indicating function ok this is just h transpose h now let us say it i realize how bad it is  so last step what are we taking gradients with respect to is w right is it fine so we have a so i mean do not get too confused right so up till this point we have a formula for l w right and i am just interested in the deri vative of that ok and all i have achieved by this is that i have ok  in fact i have one more step right refer slide time sixfifty w hat is this quantity zero ok  so we now know that the derivative of the loss function with respect to w can be written as this quantity is it ok and i have just derived it step by step there is nothing great about it anyone is can why i am doing this is not clear that will become clear hopefully  but what i am doing is clear right is that fine can i move ahead now what we are actually interested in is this quantity because this is the true loss that we are going to deal with right and we just saw in the previous slide that this quantity which is on the l h s is equal to this thing on the r h s this is what we saw on the previous slide can i just go back to the previous slide because the derivative of this was just alpha w now let us start with this  so on the next slide let me just see if there is anything else that i need to see here ok so far everyone is clear what i have derived so far why is not clear  but what is clear what is being derived so far  so i have said that the derivative of the loss function or the regular is loss function can be written as this quantity ok is that fine where w star is t he optimal solution for with respect to the un regularized loss function ok a nd now i have what i am interested in this solution with respect to the regularized loss function ok refer slide time eighteight now let w tilde be that solution for the regula r ized loss function so  that means the derivative of the loss the regularized loss function at w tilde is going to be zero nothing great about this  but i just told you on the previous slide that i can write this quantity as this quantity that is what we derived on the previous slide ok just take my word that is what we derived on the previous slide ok let just no confidence in me ok that is fine now can you are you if i write it as this just rearranging some terms oh sorry so i am just grouping al l the w tilde some terms and this is a matrix is needed here right because i need to i can only add two matrices so what i am just doing is putting the elements across the diagonal everyone understands this everyone gets this step ok refer slide time ninenineteen so now i have a formula for w tilde in terms of w star ok i am going to go a bit further and be a bit bold and compute the inverse also  so now i have a exact formula for w tilde in terms of w star  so what is this actually what is th is relation that i am trying to establish suppose i know the solution with respect to the un regularized loss and now i have added regularization what happens to the new solution so i am telling you the new solution would be smaller weights and so on t hat is what l two regularization tells you now you are just trying to make an interpretation for that  so i have given you a closed form solution that w tilde is actually equal to this quantity that you see on the right hand side ok why you are doing this is still not clear but r ight now i just focus on the what part of it this is just some mathematical steps that i am doing anyone who is not comfortable with this now notice what would happen if alpha tends to zero what would be w tilde be w star what do yo u mean by alpha equal to zero no regularization right so that is just one corner case that i want to do  but that is not what we care about anything what that is stupid to do all this and tell you that if you do not use regularization you will get the same solution  but that is not what i am going to tell you right we are interested in the case when alpha is not equal to zero ok  so let us look at that case refer slide time tenthirtyfive now i am going to assume that h is a symmetric positive semi definite ma trix squared egg in a vacuum ok  so if that is the case then i can write h as this i have just done the dash of h eigenvalue decomposition all right ok and i know that since it i s a squared symmetric matrix the eigenvalues are going to be eigenvalues a re going to be orthogonal yes eigenvalue vectors are going to be orthogonal and that is why i can write this that q transpose is the inverse of q now let us start with whatever we had on the previous slide and substitute what what i am going to substit ute instead of h i am going to use q lambda q transpose ok  so i am doing that  so is that ok i will just go over the steps and let me know at any point if you have a problem what i have done is i have replaced this i by this and its valid because q q transpose is just equal to i i have just taken q and q transpose as common right so this is a c b plus some a z b  so i have taken a and b out right is that fine ok now what is the next thing i am going to do this is of the form a b c inverse  s o i am going to write it as an d the inverses are neat right refer slide time twelveeight t his is fine what will happen to this quantity i what is this quantity q and this is what i am left with  but there is still something more i can do i guess let us see ok  so i can write this entire thing as a diagonal matrix how many of you see that it is a diagonal matrix because lambda is a diagonal matrix i of course is a diagonal matrix i is multiplied by a scalar which is also going to be a diagonal matr ix and the whole thing is again multiplied by some diagonal matrix ok what is the inverse of a diagonal matrix the reciprocal of the diagonal elements so i its fine  so i have a very neat formula for what w tilde looks like in terms of w star ok aga in why am i doing all this and g od knows  but and here d is equal to this quantity refer slide time thirteenzero so what exactly is happening here in terms of linear algebra or in terms of geometric interpretations  so let me just see if i have to do som ething first ok  so what is happening to w star is getting s tudent refer time thirteenfifteen r otated remember what happens when a matrix where hits a vector it gets rotated and scaled also and then what is this diagonal matrix going to do scale it elemen t wise scaling actually everyone gets this operation ok and then i am again rotating it by q again the same stupid question if alpha is equal to zero what would happen q transpose would rotated by something and then q would rotate it back way  that means you will end up getting the same solution ok if alpha is equal to zero we understand now if alpha is not equal to zero first let us see wh at does this matrix look like  so what is this matrix actually it is a diagonal matrix what are the diagonal elements the what is the first element in the diagonal one by everyone agrees with this what is the second element ok fine and what is the other matrix that i have lambda  so d is equal to the product of these two things right so what is d going to be what is the first element of this matrix is going to be how many i f you say lambda one by one lambda one plus alpha this much is clear everyone gets this so this is a diagonal matrix of the form a b c let us consider a three by three matrix now i am going to mult iply it by another matrix x y z which is also a diagonal matrix right because this is also it  so this matrix i have already told you what it looks like the other matrix is also a diagonal matrix now what is this product actually a x b y c z and everythi ng else has zero now everyone gets it now can you say what would this product look like if you can actually make out it would be a diagonal matrix and what would the diagonal elements be refer slide time fifteeneighteen so now what is happening so first t his rotation is happening that no one is denying after rotating what is happening this is a this product is actually a vector that is fine ok w hat are we doing to every element of the vector scaling it scaling it by what quantity these quantities th at every element is getting scaled by the corresponding entry in the diagonal in this diagonal right so the first entry is getting scaled by this the second entry is getting scaled by this and so on ok i just want you to take some thirty seconds and try t o figure out where i am headed from here refer slide time sixteenthree let us see if i can y e a h maybe look at this sentence and see first of all everyone agrees with this sentence right is there anyone who does not agree with the sentence i am just tryi ng you to figure out the implication of the sentence you get it some people are nodding their heads j ust in because if you scale it right then there is no guarantee that what the vector has changed ok what happens in the following case  that means th at dimension will be left as it is ok  but if the eigen if this condition holds what would happen that dimension is almost getting multiplied by a zero right so see these two extremes when the eigen value is very large you will end up staying where you we re  so those dimensions will not be affected if the eigen value is very small then you are almost getting scaled down to zero so now what will happen is actually only the significant directions larger eigen values will be retained  so what is the effect ive number of parameters in your model now s ee remember that this w vector is a vector of all the parameters what am i telling you that some of these are going to disappear when which condition holds the third can the third bullet hol d s some of these are going to disappear that means the effective number of parameters which remain in your model is going to be less right and you see that it is going to be given by this quantity right so that is sometimes known as the effective number of parameter s in a neural network if the effective number of parameters in your neural network is decreasing  that means what you are doing making the model less complex right so that is what we have achieved you see that ok refer slide time seventeenfifty now let me end with a pictorial interpretation of this you see two figures here and there is only one figure  but you see two different things here can you tell me what this is and what this is that is the first question i want to ask you the hint is that in this lecture we care about the other hint is what was w star the solution for the s tudent refer time eighteentwentythree u nregulated loss which means which loss l theta you need any more hints s orry this box is the contours of l theta this box contours of om ega theta  so this thing j ust ignore this part of the figure for now ok this i have marked as w star w star was the solution when i only had the un regularized loss ok there is the solution when i had the un regularized loss ok so remember the contou r maps that we had seen  so this is the minimum of that particular function  so this is the contour map for l theta that is clear now what probably is not clear is why is this the contour map of omega theta let me just go ahead actually refer slide time nineteentwenty please do not read this this is the prestige ok  so do not read that so this is the contour map of omega theta right because omega theta in the what is the minima for the omega theta it i s a function of the form w square what is the minima zero and what does that function look like and what is this point zero the origin right so that is why this is the contour for omega theta ok now what is happening this was the solution when you had without regularization and now this is w tilde w hich is a solution with regularization  so can you make some commentary on this with respect to not just general commentary with respect to the things that we saw in the derivation we talked about rotation scaling dimension specific scaling  so what is happening this was my original solution vector this was my original solution vector when i did not have the regularization term now what has happened the rotation has happened and we saw that there is a rotation operation happening more importantly what has happened scaling has happened more importantly what has happened dimension specific scaling is happening right one dimension has not this dimension has scaled down this dimension has not scaled down enough that is exactly what we wanted righ t we wanted the less important weights to go down and the more important weights to stay there we did not want a uniform scaling down we wanted a dimension specific scaling down so the weight vector has been rotated yes each dimension after rotation has been scaled some dimensions have been scaled down more the other dimensions have been scaled down less how many of you can make this interpretation from the figure now that i have told you this interpretation refer slide time twentyonethirtythree now still if you do not how mean if you can still have a doubt with this you still have a doubt what is doubt fine s o  so this was the original solution vector right the map told us that what actually happens is when you add this omega theta the solution vector gets rotated ok at the same time there is also some scaling down and that scaling down is for dimension how many dimensions do you have here two dimensions right so this is one dimension this is the other dimension now in the original case both thes e weights actually seemed almost equal right i mean if you look at the w one coordinate and the w two coordinate they were same now after this regularization what has happened is what are the new coordinates for w one and w two this is the coordinate for w one r ight this is the value of w one and this is the value for w two both of them are admittedly smaller than the original values for w one and w two in the absence of regularization or both of them equally smaller no they are being scaled differently one rate has been scaled down more the other weight has been scaled down lesser right and that is exactly what the math was telling us that they get scaled in proportion to those lambda one by lambda one plus alpha and that is exactly what we see in the figure i s that fin e how many if you get this interpretation now is that ok  so all of its elements are shrink oh you have a question  so this final resultant right it i s  so what would have happened is that there would have been first rotation then scaling down and the n again rotation  so what you are seeing here is the final rotation right so it i s not it should have been showed in three steps by just shown the final step so its question was that we first had a rotation then had a scaling and then again a rotati on  but i even as explained in the figure i spoke only about one rotation  so i basically clubbed both the rotations and so what you see finally is rotations scaling down and again rotation"}
{"audio_filepath": "Data/Preprocessed/Train error vs Test error (Recap)_59.wav", "duration": 1036.0, "text": "so we spoke about bias and variance and we saw that simple models have a high bias but low variance and complex models h ave a low bias high variance and so on and we saw it some illustrative examples that what that is what that means and then the important thing to note was these two formal definitions of bias and formal definition of variance which you all know anyways and then the important concept that we spoke about was the train error versus test errorright so this was the curve that we were interested in and one corner of this curve was related to high bias low variance and the other corner was related to low bias high variance right so i am looking for something in the middle that is what our quest is in this lecture right and we want to find ways of falling somewhere in middle and this led to the definition of two quantities of interest or training error and test errors so training error i s computed from the training points th ese are the points that you actually look at while you are solving this optimization problem so the training always involves solving an optimization problem which is the objective that you want to optimize or maximize and the test error is something that you want to use it for at eventually  so you all have these two quantities of interest that we design and we realize that the training error is more optimistic whether the test errors actually gives us the real picture of what we do and we tied those back to things that you have done previously in the machine learning or other courses that we always split the data into training valid and test training i t on the training data do some validations on the validation data but never look at the test data that is for the final evaluation so thats the this is this intuition which i have been trying to build with these two curves is the explanation for why we do things that way  now  we are interested in doing a more mathematically rigorous analysis of this intuition right so that is where we left off so what we are interested in so now i wi ll just start from this point is that we are given some data which is m n m training points and n testing points and we know that there is a true f unction between the outputs and the inputs and we are also expecting or accepting some noise in this relation just as in any other relation so which means that y is related to xi but by some true f unction but there is also this noise and f or simplicity we assumed as this noise comes from a normal distribution with zero mean and some small variance and as usual we never know f right but we are trying to approximate this f hat and we come up with some parametric form for f hat and then try to learn the parameters of f h at from the training subset of the data that is given to us so this is what we always do and we have already seen different variations of f hat one of them being the deep neural network and what we are actually interested in i s this quantity  the expected difference or square difference between the predictions made by our model and the true value of the output with respect to the true function right then we asked i asked you whether we can actually estimate this quantity and all of you said no why it is because you do not know what f of xi is right so we will see h ow to estimate this empirically  so then we started off with this information that we have we know what y i hat is because that is the prediction that we make and we know yi what yi is we do not know the function but we see the output of the function in the form of the training data points given to us or any data points given to us so we wrote this by making this particular substitution where we notice that yi that we see is actually the true f unction plus some noise and then we did some trickery and try to simplify this and then we just realize that this is the term that we are interested in so we moved it to the other side of the equation and came up with this neat left h and side or neat right hand side that we need to analyze nowso far everything is clear  this is where we ended the last class right you just went to it very quickly  but i assume everything is clear at this point ok fine so we are left with a bunch of expectations right and we have i am assuming we have no clue how to estimate this right i mean so and remember that when you are dealing with expectations as always this true expectation and then there is this empirical estimation right so what we are going to move towards so these all equations when i write e here capital e here i am talking about the true expectation now  we will see how to approximate the true expectation with an empirical expectation and then based on that we will make some observations so that is what we will do now  so we will just take a small d two and i will just tell you what expectations are or what empirically expectation is how to compute them so suppose we have observed the goals scored in k matches there is some k f ootball matches that we have seen and we have seen that the goals scored were the following now  if i asked q what is the expected value of the goal n ow the number of goals for what will y ou do take the average of this this is what you will do so what is it that you are doing here you are taking a dash estimate of the expectation empirical estimate you are making some observations these are the observations given to you these are the k matches watch as much as many football matches as you want after the semester ends and then notice the number of goals that were scored in them and then you can compute this expectation right and this is how y ou do empirically  so there is something that we do on a regular basis but i just want you to realize that what you are doing is actually an implicit estimate of the true expectation now  can you relate this to the quantity that we are interested in we are interested in computing a certain expectation which is this can you take an analogy and tell me how you would do this the hint is we have done this a million times in the course already  fine so this is how we will do it and h ave actually done this a million times i n the course so when you compute this we are actually doing an empirical estimate of the data so let us just take a minute to understand this we are given some data we are interested in this to expectation which we cannot compute so we will take this data we will assume there is enough of this we are given m samples which are enough and from that we will make an empirical estimate and just as in the case of these goal scored right as you see more and more matches you will have a better understanding of how many goals can be scored when two particular teams are playing in the same analogy goes here as you see more and more data your estimate would become better  but that is how you will do the estimation so now we will come back to so now do not get surprised when i am going to replace all these es by this all the es that we had in our original equation i am going to replace them by these summations ok fine so this was our original equation that we had derived and we were interested in this left hand side quantity which is a sum of some terms on the right hand side so now this expectation i told you that we can estimate it from data but which data t raining data or test data both so we will try to estimate it from both and see if there is any difference which arises when you estimate it from one data and the other data ok so the first thing that i am going to do is i am going to use test observations to estimate this so can you tell me what are my summations going to look like it is summation over n plus one to n plus m right we assume that the first endpoints are training points and the remaining points are test points so the quantity on the left hand side is true error  remember that because that has f x which we do not know quantity on the right side the first thing is empirical estimation of the error  ok the second thing is a small constant h owever  the epsilon i square and we assume that comes from a normal distribution with a small variance what i s the third quantity actually i have given you the answer already  but i want y ou to think about it i am saying it is the covariance between two things when i say it is the co variance between two things what is the first thing that i need to prove is that the two things are dash random variables i mean first thing we need to see is that the two things are random variables epsilon i clearit is a random variable what about this other thing or rather epsilon is a random variable what about the other thing and depending on the training instance that you have sampled this ongoing difference is going to differ right you are having your training or test instance whatever is this x i this is going to differ because these xs are different they are all random variables so there is difference between these two quantities also going to be a random variable is that fine ok but still is this the so then i have told you this i s x and this is y and what i am saying i s that the co variance between x and y i s just e of x x into yis that correct that is how you define co variance what is the definition of co variance if you have bothered to look at the prerequisites no expectation in the form of e so co variance is e of x minus mu of x into y minus mu of phi what is our x epsilon and what is our y what is mu of x zero so i will just simplify this a bit ok i will open up the product what is mu of y into e of x what is e of x what is the expected value of the noise zero so then this turns out to be as that is that fine that is why we are writing the co variance is just the product of the two things so let us just take a minute to again understand this the true error is the empirical estimation of the error plus i mean plus or minus a small constant ok and then this nasty quantity that we do not know what to do with it so let us look at this quantity and see what we can say about it now  what is the co variance between these two i a m trying to compute this expectation f rom the test data just remember that so each i here is a test instance are these two random variables dependent or i ndependent is the question that i am trying to ask it is independent so let us look at it piece wise so remember that we had said that y is equal to f of x i plus epsilon i right this epsilon i h ad no relation to f of x i i mean i could choose any x i but this noise is going to be random so there is no relation between these two now  is there a relation between f hat of x i and epsilon i we are doing tests so how did we come up with f hat of x i how did when i say how did we come up with f hat i s i mean how did we learn the parameters of a f hat using the training data and what are we computing expectation with respect to now test data these these epsilon improve influence the parameters that we had learned further f rom the training data n o since there is no dependence between these two guys so that is why epsilon i i s independent of the other random variable that you see in this expectation is that clear do you get the intuition f hat x i further no but this is the mean this noise is what i s present in the test data and you have not seen this add training time w hen you are training the parameters you did not look at this noise you are looking at the noise in the training data so this is not participated in the estimation of the parameters of f hat but that was for the training data right but this now i am doing the expectation from a test data so these two random variables are i ndependent that means i can write this as is this fine what will happen to this zerook so what did we eventually conclude that the true error i s equal to empirical test error plus a small constant right so what does this tell you n ow tell me forget the math tell me in e nglish right what does this take what does this mean can you relate it to now why you do this training error validation error  test error so what does this tell me this tells me that if i h ave trained a model and now if i take an estimate of the error on some data which i had not used for the training then that error which i see is actually very close to the true error  it only differs by this small constant how many of you get that that is why when i look at the validation error  it is not being overly optimistic it is giving me a true picture of what the actual error is right so there are two things that you need to understand here one this is the quantity that we are interested in which we cannot estimate we are trying to estimate it by using this we are trying to make an approximation so we are trying to see how good this approximation is what this derivation is telling us is that i f you are approximated it using the test error or the test data then this approximation is actually very close to the true error and how close it is actually it just differs by this small constant so you get the importance of what we are seeing here right ok now  to truly appreciate this i need to tell you what would have happened if you had used the training data for this estimation right it is largely dependent but that is again a normal assumption that you make so this is ok good that you asked at this point i will be doing a couple of things today where we will be deriving some things we will try to prove some things mathematicallybut all of these would have underlying some assumptions so if you remember the adam derivation with this we did there also we had made this funny assumption that the gradients are actually coming from a stationary distribution which will not happen in practice so this reminds me of this joke from big bang theory  which says that i have a solution but it only works for squared eggs in a vacuum right so it is basically all these things always have some assumptions underlying them but the idea is to kind of ignore those assumptions and see what happens i n a neat setting and at least see whether in a neat setting everything works fine or not so that is what i s happening here so is a valid point that you are assuming that the noise comes from a zero mean distribution now  if the noise did not come from a zero mean distribution then this would have not gone down to zero and the mean would have been higher than this is no longer a small constant and so on so those things are there so this is going to happen in some of the other derivations that i do today  it is not that i am teaching you something wrong it is just that you have to take it with a pinch of salt in the sense that these assumptions are there and the original derivations these are not my assumptions and they work only under those assumptions so you h ave to be careful about that but the i dea is that still with these assumptions can we at least make something meaningful out of it right is that fine with everyone can we all work with that basic premise so what i have done so far is told you that if you are estimating the errors from the validation data you are doing a good job now  let us see if i would estimate the error f rom the training data take a guess what would happen what would my argument for this be now  this will not disappear  right because these two are not independent now i cannot write i t as a product of two expectations that means it will not go down to zero so that is the ar gument which i am going to make so hence actually the true error if you see right it is equal to the empirical estimation plus some quantity  that means the true error is dash as compared to the empirical error  that means the empirical error that we see is pessimistic or optimistic optimistic that is what i started with that you gave a very optimistic estimation of your error if you are looking at this empirical estimation f rom the training data because you have i gnored this quantity  is it fine so what i s missing in the story let us see now what was this quantity  so far all our discussions l theta right but now suddenly i have realized that my true error is actually l theta plus something else right you see where i am headed with this ok so that is what we need to see now ok now think it would be we should but i am pretty sure it is positive i cannot work it out right now but i am pretty sure it is positive and you can see and if you find it is not then let me know  so how is all this related to model complexity we started off with this idea that model complexity tells y ou how much is the bias how much is the variance and because of that you get these two curves that you are not happy with one curve being very optimistic and the other curve being a bit pessimistic now  h ow does this discussion tie up to model complexity"}
{"audio_filepath": "Data/Preprocessed/The Deep Revival_3.wav", "duration": 433.0, "text": "when this deep revival happened so in two thousand and six a very important study was or a very important contribution was made by hinton and salakhutdinov sorry if i have not pronounced it properly and they found that a method for training very deep neural network effectively now again the details of these are not important we will be doing that in the course at some point but what is the important take away here is that while from one thousand nine hundred and eightynine to two thousand and six we knew that there is an algorithm for training deep neural networks and they can potentially be used for solving a wide range of problems because that is what the universal approximation theorem saidbut the problem was that in practice we were not being able to use it for much it was not easy to train these networks but now with this technique there was revived interestand hope thatnow actuallycan trainvery deep neuralnetworks for lot of practical problems this sparked off the interest again and then people started looking at all such of thing right that even this particular study which was done in two thousand and sixwill actually be very simple to something done way back in ninetyoneninetythree and which again showed that you can train a very deep neural network but again due to several factors may be at that time due to the computational requirements or the data requirements or whatever i am not too sure about that it did not become so popular then but by two thousand and six probably the stage was much better for these kind of networks or techniques to succeed so then it became popular in two thousand and six thenthistwo thousand and sixtotwo thousand and ninepeoplestartedgainingmoreandmoreinsightsintothe effective ness of this discovery made by h inton and others which is unsupervised pre trainingrightthatiswhat i spoke about on thepreviousslideunsupervised pre training and they started getting more and more insights into how you can make deep neural networks really work so they came up with various techniques some of which we are going to study in this course so this was about how do you initialise the network better whatisthebetteroptimizationalgorithmtousewhatisthebetterregularization algorithm to use and so on so there were many things which were started coming out at this period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep neural networks of artificial neural networks started becoming popular that is when people realised that all this all the negative things that were tied to it that you are not able to train it well and so on have slowly p eople have started finding solutions to get by those and maybe we should start again focusing on the potential of deep neural networks and see if they can be used for large scale practical application  so this two thousand and six to two thousand and nine was again a slow boom period were people were again trying to do a lot of work to popularize deep neural networks and get rid of some of the problems which existed in training them now from two thousand and nine onwards there was this series of success is which kind of caught everyone which made everyone to stand up and take notice right that this is really working for a lot of practical applications starting with handwriting recognition so around two thousand and nine these guys won handwriting recognition competition in arabic and they did way better than the competitor systems using a deep neural network a nd then this was a success so this was an handwriting recognition and then there was speech so this shown that variousexistingsystemstheerrorrateofthesesystemcouldbeseriously be significantlyreducedbyusingdeepneuralnetworksorplugginginadeepneural network component to existing systems right so this was handwriting and then speech then again some kind of pattern recognition which was on hand written digit recognition for mnist this is a very popular data set which had been around since ninetyeight and a new record was set on this data so this is the highest accuracy that was achieved on this data set around that time in two thousand and ten sorry and this is also the time when gpus entered the same so before that all of the stuff was being done on cpus but then people realised that very deep neural networks require a lot of computation and lot of this computation can happen very quickly on gpus as opposed to cpus so people started using gpus for training and that drastically reduced the training and inferencetime so that wasagainsomethingwhich sparked a lot of interestright because even though these were successful they were taking a lot of time to train b ut now the gpu s could even take care of that a nd this success continued so peoplestartedgainingorgettingsuccessinotherfieldslikevisualpattern recognition so this was a competition on recognising traffic sign boards and here again a deep neural network did way better than its other competitors and then the most popular or one thing which made neural networks really popular was this i mage net challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when thisalexnetwasone of theparticipatingsystemsinthiscompetitionmost of the systems were non neural network based systems and this competition was basically about classifying a given image into one of thousand classes so this could be an image of a bird or a dog or a human or car truck and so on say you have to identify the right class of the main object in the image  so i n two thousand and twelve this alexnet which was a deep neural network or a convolutional neural network based system was able to actually outperform all the other systems by a margin of sixtyseven percent so the error for this system was sixteen percent and this is a deep neural network because it had eight layers the next year this was improved further and something known as zf network propose which was again eight layers but it did better than alexnet t he next year even a deeper network with nineteen layers was proposed which did significantly better than alexnet t hen google entered the scene and they proposed something which is twentytwo layers and again reduced the error t hen microsoft joined in and they proposed something which had one hundred and fiftytwo layers and the error that you see here is actually better than what humans do so even if a human was asked to label this image because of certain law certain noise in the image and so on even a human is bound to make more errors than threesix per cent  that means even if you show hundred images to humans he or she is bound to may go wrong or more than three or four of these images right there is this system was able to get an error of threesix per cent over the large test set so this two thousand and twelve to two thousand and sixteen period were there was this continuous success on the image net challengeaswellassuccessesinotherfieldslikenaturallanguageprocessing h andwriting r ecognition speech and so on so this is the period where now everyone started talking about deep learning and lot of company started investing in it a lot of traditional systems which were not deep neural network based was now started people started converting them to deep neural network based system so translation system speed systems image classification object detection and so on therewerelotofsuccessinallthesefieldsusingdeepneuralnetworks andthis particular thing that we are talking about which is image net and the success in this was driven by something known as convolutional neural networks"}
{"audio_filepath": "Data/Preprocessed/Occlusion experiments_95.wav", "duration": 336.0, "text": "    ok the another thing that you can do is to figure out whether things are working properly or not so you can do something known as an occlusion experiment so these are all your debugging tools sort of to say if you are working in vision or computer vision where you are using a convolutional neural network and this is to gain more insight said most of you will get away by just taking an off the shelf convolution neural network training it on your data getting some accuracy and reporting it but for those of you who want to really understand what is happening and how can improve things further so this could actually tell you for example if you want to  compare whether a fivefive filter would have been better than a sevenseven filter then you could have observed what these filters are actually learning and in your data does it make sense to have a fivefive filter versus sevenseven filter because maybe the fivefive filters are not being able to distinguish enough but if you had used a smaller or a larger filter things would have been different right so this is for people who really want to get into the know how of how things are working otherwise most of you i do not really expect you to do this is but this is an important set of tools to have and i would strongly encourage everyone to experiment with them and some of this you will do in the assignment ok so here is the idea of behind occlusion experiments so we are interesti ng knowing that what patches in the image are actually causing the output to belong to a particular class right so i have here the figure of a dog and the class being probably predicted is a pomeranian and i want to know that what patch of the image act ually resulted in this output right so have you tried doing this in any other context if you want to know if you have several features or several things several factors and you want to decide which actually influenced the output how would you do it s o what you could do is you could drop one factor right and see whether your output would have drastically changed if it goes from positive negative then that maybe that was the factor which really mattered right so if for example it is a movie revie w classification right so when you drop certain words from your review so you drop the word amazing great and so on and keep everything else the same now it is quite likely that your probability of the review still being tagged as a positive review will at least drop earlier maybe with these words it was getting tagged as a positive review with zeronine probability it would come down to zerosix but now if you drop words like the and for and so on then you do not expect the output to change much because th ese words are not really important indicators of positive or negative ok so the similar thing that you do here is you occlude certain patch patterns a certain patches of the image so i have shown one occlusion here so i have replaced that patch by a gray patch and i again feed the image to the convolutional neural network and i see what is the probability of the pomeranian class right now ok and i do it for all such patches in the image i can do for as many patches as i want refer slide time zero threethree and i create something known as a heat map so the red portions here are the ones which do not cause a large drop in the output probability if you occlude them and the blue portions are the ones which cause a large drop in the output probability if you occlude them and it is pretty obvious because what is happening is when i cover the face of the dog the probability drops drastically and that is what you would expect right so this is also an indicator that your network has actually learned som ething meaningful it is being able to detect this based on the facial features and not just randomly guessing that this is a dog right and see the similar experiment so for example if there is a car sometimes these results are not very at least to me it does not look very intuitive so i would have expected that the wheel would have been one of the deciding factors right so if i occlude the wheel the probability should drop drastically but the other way of looking that it is that its really learn ing a lot of redundant features so it is not heavily relying on the wheel unlike in the dog case even if the wheel is occluded it is relying on certain other features which look like cars and hence the probability is not dropping drastically right so this allows you to interpret what kind of things it is running so if its heavily for example for face detection if its heavily relying on nose to detect the face to say that this is the face the moment you block the nose it will drop its probability of detecting this as a face but thats not good right because you want these redundant features remember we had discussed this at some other point where it should try to detect the face not only from the nose but also from the ears from the hair from the eyes and so on so if your occlusion is not drastically reducing your probability that means it has learned some redundant features which are still allowing it to operate well even though certain portions of the image are not there that means it is m ore robust noise in that case right and here it looks like it is not so robust because it is probably heavy this is the rearview mirror of the car so it is probably heavily relying on that feature to detect a car ok then this is another thing where the true label is an afghan hound and for some reason if you occlude the face of the woman its probability decreases now let us not comment on that but if you go back and look at the image you might be able to make some observations right so these are things so this is an indicator that is probably not really learnt it well maybe all the afghan hound images that it saw maybe a woman was carrying the dog always right so its learn this wrong association that when i see a woman with some object it tha t is the portion which is the dog which is bad right so now you can see that your network has not learned something interesting and you would want so if you look at one network which is predicting a dog based on this kind of a occlusion and another n etwork which is predicting a dog based on this occlusion then you would prefer the other one and so this is a very interesting experiment to do"}
{"audio_filepath": "Data/Preprocessed/Applications of Encoder Decoder models_113.wav", "duration": 1009.0, "text": "so we are going to see a lot of applications of the encoder decoder models refer slide time zeroone seven and for all these applications we are trying to answer the following questions what kind of network can we used to encode the input in the previous application what do we use cnn what kind of network can be used to decode the output what did we use student rnn rnn what are the parameters of the model we will see that and what is an appropriate loss function right so let us again go back to this task which was image captioning what is the input what is the trai ning data given to you what is x what is y x is the image y is the description right so this is what is given to you given n such training pairs where x i is the image and y i is the description and y i in itself is a sequence right so you have y i one to y i capital t everyone gets the input and output now what is the next thing model can you write down the model equations i want an equation which starts from x and goes all the way up to y and since we have several time steps i want an equati on for y t this is generic for every time step right say can you write that equation and feel free to use shortcuts so you do not need to write the entire rnn equation just say rnn of something dont even try to write the vgg sixteen cnn equations just say c nn ok so we will go ahead the first thing that i am going to do is i am going to write the equation for the encoder so the encoder gives me cnn of x i whatever is the input given to me x i is the i th training image given to me so i will just pass i t through cnn i will get a representation for that and i am just being cryptic here it could be the f c seven representation or the con five representation or the max fool five representation or whatever you want right and it is going to denote all of this as cnn of x i run this cnn take whichever representation you want to take now what is the decoder going to be decoder is the following rnn rem ember the equation of rnn was s t one comma x t what is the input of the t th time step whatever we are predicted at the previous time step just the embedding of that so e means embedding if you want take one odd embedding if you want take wordto vec embedding is that fine ok and then what is the output it is the soft max function of the following how many of you get this now please raise your hands how many of you can say that y can be written as a function of x is that pretty straight forward ok so you have an encoder you have a decoder and remember that this final y is a composite function of the original input x ok just that you are doing too many computations along the way but there is a path which exists ok what is the loss function everyone at this point should be able to say it student sum of cross entropies sum of cross entropies they just wai t for me to say two more sentences what are the parameters u v w b c student refer time threetwelve a b c d e f laugher everything right what is that all the parameters of the student refer time threeseventeen convolutional neural network that means a ll the filters that you have all the parameters of the rnn which is w u and the parameters of the output layer which is v right all of these is that fine i am i may have missed some biases but ok the objective function as you said is a sum of cross entropies where l t is the true character at time step true word at time step t and what is the algorithm that you are going use back propagation through time and with back propagate all the way through the cnn also which is an end to end thing in pract ice of course you do not do that yes you could just said both to be the same do you get that question is that ok ok now let us look at another task we look at the task of textual entailment what textual entailment does is that i give you a input or premise the premises that it is raining outside and you need to tell me a hypothesis the hypothesis that the ground it is wet ok with basically means that it is raining outside implies that the ground is wet ok now what is the encoder decoder architecture that you will use for this problem what is the input here student a sequence a sequence what is the output sequence it is all the hint that i am going to give you so what will you do what is the encoder equation g oing to be student rnn rnn what is the decoder equation going to be student rnn rnn how will it become end to end by setting what to what student refer time fourfortynine szero of the decoder to to what of the encoder student refer time fourfortynine las t time step of the encoder how many if you get that really we are on the same page first time in i do not know how many lectures by finally it happened so here is what training data is right it is a collection of premises and hypothesis and you have n of these there are two options for the model the first option is that you encode the input using an rnn feel free to replace and by an lstm if you want then you have the decoder where and you set the zero time step to whatever you got from the encoder then every time step you computed using the rnn where remember the input at every time step is whatever you predicted at the previous time step and then the output is just the soft max function is that fine and what is the loss function going to be los s function student refer time fivefortyfour sum of cross entropies training algorithm student back propagation back propagation through time and really it is through time right all the way back ok so we will see that let me see if i had any other quest ion ha ok i will ask it parameters i am not going to bother about ok now this was option one i have just clearly written what is option two what is the set of equations look like for option two which of these equations will change and how remember option two was maybe pass the input at every time step which equation will change s t what will it become but s t can take only i mean rnn take only two inputs right s t minus when you need to gave embedding you need to gave so how will you fit in the third i nput this animation has it is own mind so this is how back propagation will happen right so let us it is finish that so will actually back propagate all the way back through time fine really all the way back through time and same task textually entailment i want model two option two so this is what will happen we will just concatenate h capital t which is this guy along with the input at every time step right how many if you get this the rnn is still taking just two inp uts one is the previous state the other is the concatenation of the current input as well as input that we got from the encoder everyone get this ok so this is model two i am going forward i am not going to do both model one and model two it is model two is j ust a very simple variation of model one a parameters loss function training algorithm everything remains the same ok let us look at machine translation what is the input an english sentence what is the output a hindi sente nce what is the encoder going to be student rnn rnn what is the decoder going to be student rnnsixrnn what is the loss function going to be student refer time seventhirtyseven soft max who said soft max student refer time sevenfortytwo what is the loss fun ction going to be student sum of sum of cross entropies training algorithm all the way through time right ok so let us can you draw can you write the equations just copy it from the previous slide right actually copy it from the previous slide ri ght if you have the rnn you have the rnn as a decoder again in option when you will set s zero to h t you have the loss function the parameters and your training algorithm ok and for option two it is back propagation will fine and for option two what will ha ppen option two what will happen student refer time eightseventeen this will change right so just focus on that we just passed in the last time say belong with that now transliteration what is transliteration what is transli teration if you do not know it at least see it from the example what is it student refer time eightthirtyfour writing the same word in another language right so this is typically done for named entities very when you are when you are translating from one language to another you do not often for thomas you do not come up with an indian translation right you just say thomas in devanagari right you just right thomas in the devanagari right so for names you typically just do a transliteration that means from the english script you just write it in the native language script ok what is the input one word the input is the word right what is it a sequence of student characters characters what is the output student sequence of characters a sequence of characters what will you use for the input student rnn rnn what for the output student rnn rnn so i will becoming too easy right can you write the equations for this yes you will copy it from the previous slide yes ok everything remain s the same right so you see why this framework has become so powerful you do not see it still maybe let us look at something else image question answering tell me what is the data here what is the input image and stude nt question question and what is the answer what is the output answer so for simplicity we are going to assume that the answer here is a finite vocabulary we are not generating descriptive answers we are not being overly dramatic let us going to sa y one word what is the colour white we are not going to write i think the colour of the image is white now ok just white so all these outputs are going to be single words and we have v possibilities and we are going to predict one of those v possibili ties ok now give me a model for this now things are getting slightly complicated you have one image as the input one sequence as an input and a dash as the output oh god now think why would you generate the sequence of characters as the answer i sai d that the answer is going to be come from a finite vocabulary that means you need a student probability distribution a distribution probability distribution is here enough said now tell me what is the model a model should connect the input to the output you have two inputs here i see some people doing this laugher i do not know what that means but let us just do it let us make a train simple formula simple recipe and whatever input you are given just encode it depending on the type of input you know what is the encoding is going to be for images what is encoding sequences student refer time tenfiftyfive now what do you do with these two separate things student refer time tenfiftyeight concatenate them ok and then student refer time elevenone after that student refer time eleventwo can you think of all the equations can imagine all the equations along the way student yes of course yes laugher right i mean imagination laugher you can always do that laughter now just think about it can you write the output as a function of the input where the input is actually a pair now it is image comma question what is the model going to look like let us see so model will first have an encoder for the image let us call that as h i it is going to have an encoding of the question let us call it as it as h  i am going to concatenate these two as someone rightly gestured and then what am i going to do after that pass it through a student feed forward network feed forward network and predict a probability distribution what are the parameters of this network parameters of the feed forward network the parameters of the recurrent neural networks and the parameters of the cnn right so everyt hing that we have done so fine because ok how do you train it back propagate through time and space ok also go back to the image also fine is that ok ok document summarisation what is the input sequence what is the ou tput rnn rnn everywhere fine i will not even bother to ask you video captioning sequence of images i want to hear the choice of phrasing that you use i just want to hear that i love hearing that every time student ref er time twelvethirty rnn of cnns whatever that means what rnn of cnn every time i do this everyone says rnn of cnn laugher i do not know what that means but it is the right answer what does it mean what is the video it is a sequence what is the sequen ce of student images images so what will you do encode every image and then pass it through a student rnn rnn can you imagine the equations ok let us see ok and in this case what is the output again the sequence so what is the decoder going to be student rnn so here is the model so first what you do is for every time step you compute the cnn encoding of the frame then you pass it through an rnn to get the final time step t and then you feed it to a decoder and generate one word at a tim e is that fine so this thing apparently is called rnn of cnns ok and so that is and loss function would again be the same sum of cross entropies and back propagation through time and space ok good please do not quote me on this thing also this is g etting a recorded but ok the next one video classification what is the decoder decoder is probability distribution okay what is the decoder student feed forward neural network feed forward neural network refer slide t ime thirteenfiftyfour ok this one dialogue how are you i am not fine ok input student rnn output student rnn right so you see this why this has become so popular we took a wide range of problems different modalities right we took images we took video s we took sequences a combination of these right image question answering has a combination of images and sentences and the output you have a probability distribution all of this could be model by this unified end to end network all of the components are neural network based components whether it is a convolutional network or a feed forward network or a recurrent neural network right now let me stretch this right what if you have video question answering what is the input going to be sequence of i mages and sequence of student words words the output is student refer time fourteenfortysix no just a word right we will pick from a fixed vocabulary how are you going to model the input student refer time fourteenfiftythree rnn for the question rnn of cnn for the video then student concatenate concatenate and student feed feed it to your feed forward network right so all of these become to what extent that work is a separate question but all it was not even possible to model all of this as an end to e nd network right but now i just because possible to model it as an end to end network with all the components being neural components right and this is this story still not complete everything is not as easy as it looks they still and very crucial co mponent that we have missing in the architectures that we have seen so far and we will talk about that soon ok now let us just continue that i challenge you to do this pick up any problem there are student from relevant diffe rent departments pickup any problem do not say that i want to design some gear for certain aeroplane and all that and i want to use neural network to do so no something which involves machine learning right not problem is does not involve machine learn ing and see if you can model it using the encoder decoder frame work just try to do this take problem from biotech right for example they given a sequence of genes and you want to predict whether this person is susceptible to a certain disease what wil l you do conduct a blood test ok do not do not laugher do not go and do neural networks for that but if you had to do that this is what you will do it will take a sequence you will treat the sequence of sequence at the given dna as a sequence of genes and then you will try to predict something as the output try to predict a probability distribution over disease it a possible right so you can think of many applications from many domains and all of that you could problems involving machine learning wi th potentially model than using the neural encoder decoder architecture but there is a very important part missing from this whole story which is attention which is a very important idea and we will spend some time on that in the remainder of the lecture s so we will first motivate why do we need attention and from there we will see that how do you make how do you integrate attention with all these encoder decoder architectures that you have seen so far"}
{"audio_filepath": "Data/Preprocessed/Backpropagation (Intuition)_26.wav", "duration": 787.0, "text": "this lecture is on backpropagation and feed forward neural networks so we i ntroduced a feed forward neural networks we saw the input layer hidden layer and the output layers and we saw that the output layer actually the output function depends on the task at hand and we considered two main tasks one was classification the oth er was a regression for regression it made sense to use a linear layer at the output because we did no t want the outputs to be bounded they could be any range a nd for the classification problem we realized that we want a special kind of output because we are looking for a probability distribution over the output and for that we use the softmax function and in both cases we used a different kind of a loss  f or the regression problem the squared error loss made sense because we predict some values and we want to see how far we are from those values but for the other case the classification we realize that it is a distribution  so maybe we could use something which allows us to capture the difference between the true distribution and the predicted dis tribution and therefore we had this figure emerging which was depending on the output whether it is real values or probabilities y ou will have different types of output activation functions and different types of losses and of these combinations today we are going to focus on softmax and cross entropy and our aim is to actually find these gradients remember there are many of those we have seen this large matrix which had many such partial derivatives and we want to find t hat entire matrix i hopefully do it in a way that it is not a repetitive we could compute a large number of partial derivatives at one go so before we look at the mathematical details we just get an intuition for backpropagation refer slide time twoone one and then we will get into the gory details of how to actually compute these gr adients and partial derivatives so this is the portion that we are in we are intended to ask these two questi ons and this is where we are so now this is what our network looks like this is clearly much more complex than that single neuron that we had and which had only two weights w and b that was very easy to compute the gradients there now imagine that i want to compute the gradient of the l oss function and let us assume it is a classification problem then what is the l oss function minus log of y hat so this is the loss function and we want to compute the derivative of this with respect to one of these weights in the network and am delib erately taking something which is much farther away from the loss  b ut why do you say why do i say it is much farther away it is right at the input layer right and the loss is s omewhere at the output layer so we want to compute this gradient refer s lide time threeten now to learn sorry you want to learn this way to learn this weight we know that we can use gradient descent w e are all convinced that this gradient descent algorithm which i have shown here as long as we put all these variables or a ll these parameters that we have into theta we can just run the gradient descent algorithm and compute them the only thing that we will need is this partial derivative with respect to all the weights in the network a nd in particular with respect to this weight that i am interested at now so we will now see how to calculate this we will first this is o nly to get the intuition so we will first think of a very simple network which is a very deep but the thin network it ha s many layers but it is a very thin network here you see what i mean by a thin network ok now this is what i am interested in can you tell me how to compute this this looks like a chain so it is justified the user c hain rule of derivatives so wha t would the chain rule look like you want to compute the derivative of this with respect to this and you have done this in high school right so you have functions of the form of sine of cos of tan of e raised to sine of x and this is exactly how this chain is right you have some function of x followed by another function of x another function of x function of x function of x and so on y ou just keep making a composite function of the input we actually wrote down that function if you remember i t wa s just one function applied after the other function or a very composite function so you just need to apply the sa me idea here s o we take we go step by step so i am almost accountin g for every shade of color here so dl theta by d y hat then dy h at by d a l one one there is only one neuron here  t hen this with respect to the sorry h two one then h twentyone with respect to a twentyone a twentyone with respect to h eleven h one one with respect to a eleven and t hen a eleven with respect to w eleven so i just traversed down the chain in the r everse order this is how the chain rule works right anyone has a problem wi th this it is straight forward r ight and now what i have done is for convenience i have just compressed the chain you see the red part and the green part i have just compres sed this weight so that and this is again something that you have done in high school if you have this you could just write the chain as the first and the last it so this is what you can do and i also compress this other chain ok and am going to use these kinds of compressions later on so what am trying to impress on you is that if i want to go from here to here right that is what my intention is if somehow i have already travels from here to here then i can just reuse that computation that is the idea which i am trying to impress on it i do not need to follow the entire chain every time i can do these partial computations up to a point have you seen this something similar idea somewhere else dynamic program is something like that so you h ave just computed up to a certain point and then it is reuse the value for further down the chain so that is what we are going to do and same for all the weights right for each weight the chain size would be different depending o n where it lies in th e network right for the weights which are very close to the output layer the chain w ould be very small makes sense ok s o this is the intuition and we wi ll see the intuition a bit more so let us now understand this in the t erms of the wide co mplex network that we are using so what actually is happening is that we are at a certain stage that means we have some values of ws and bs ok at the initial stage we just have these w knots and b knot s but let us assume that we have done some training and we are at a certain level we are at wt at tim e step t and bt at time step t right for all the weights inverse now we feed it a training example we do this entire compute computation what do we g et at the end we get y hat which is a function of this x that we have fed it but we also know this true y we know the true value we know y hat so we can compute the loss function so we compute the loss and to our surprise we see that the loss is not zero  we are getting a non zero loss that means the netwo rk has not yet learnt properly  right the weights and biases are still not in the right configuration that we want them to be in right so now what do we do we go on this path of investigation we wan t to find at who in th is network is messing up things  there is someone who is causing this problem because of which i am not getting the desired output and we are on our quest is to now find out who this guy is who is responsible for this so what woul d you do where would you start the output layer because the output layer is the guy who give you the output right so go and talk to him and we say that hey what is wrong with you why are you not producing the desired outp ut right now what is the output layer going to tell you in very civil language i will say i cannot do anything boss i mean i was just given some weights and inputs from the previous layer and those weights and inputs were messed up so there is not hing which i can do go and talk to them so who will it directors do it will say that i am just as good as wl hl minus one and bl because these are the guys that i completely depend on if these guys were ok then i would have been fine so we then go an d talk to these guys that what is wrong with you so now they say ok fine wn and bl take the responsibility they are the nice guys they say we are the weight s we are supposed to make a  we are the ones who are responsible fo r the adjustments in the network so we have failed to do our job properly and i think we should get adjusted right but then hl will resist it will say it is not my fault why will it resist because it against again depends on the previous activ ation layer so till then point as to what the ws and b s in the previous layer right and you see how the investigation is now proceeding where will we reach well keep going down the network we are talking to everyone in the network we are talking to eve ry dark green guy every light green guy every dark blue guy every light blue guy we are also talking to all these weights and biases and in the end what do we figure out the responsibility lies with all the weights and all the biases they are the on es who are responsible for this now but now we find out that this is also one of those weights which is responsible and this is also one of these weights that is responsible but it was have been very difficult for us to talk to them directly so then wh at are we going to do instead of talking to them directly which is this we will talk to them through the chain rule so we will talk to the output layer that is exactly how what we did maybe went to the first guy that we knew that guy pointed out to th e previous hidden layer that guy pointed us to the previous hidden layer a nd then finally we get to the weights right so this talking to is fine but where do derivatives figure out in this why are why is the language derivatives why are we not talk ing in english or hindi or something else what does the derivative tell us s o talking about gradient descent like what we saw in gradient descent but in general what does the derivative tell us if i change this a bit how much does my loss change righ t so that is how much this guy is responsible for the loss b ecause if this is very sensitive even adjusting a bit of this i co uld drastically reduce the loss right so that is what the derivative tells us that tells us how sensitive is the loss funct ion to the weight or any quantity with this with respect to which am taking the derivative right that is why the language is of derivatives right is that clear is the intuition fine to everyone so now will convert this intui tion into actual math and try to figure out how to compute every guy along the way right and we will use this idea that we have made some partial computations and then well use it for the rest of the chain so we have made this much at some point we will reach where we have made this much and then you could use it for the rest of the chain in fact we will start right from here well start with this guy an d then keep expanding the chain so the rest of the story is going to be about computing three qua ntities can you tell me which are these three quantities gradients with respect to the output units gradients with respect to the hidden units and then gradients with respect to the parameters so these are the three things that we need to do if we do this w e have everything in the chain and we are done and the other thing that we need to do is we cannot sit down and compute this for every element right we want to have it in a generic fashion where instead of talking about w one one one w one one two and so on we should at least be able to talk about w one w two and so on  so that means we have only three matrices and three biases right at least at that level so we have to do a collective computation instead of just computing for every guy so instead of looking at sca lars which is what we are doing when we are doing gradient de scent for w naught and v naught  we were just computing the update rule for w and b we want to now do it for vectors and matrices so that is that is the transition that is going to happen a n d our focus is going to be on what cross entropy and softmax why is that important  b ecause that is the loss function so that is the quantity that am going to take the derivative if i change the loss function all the gradients are going to change are all the gradients going to change only the first guy will change in the change all this should remains still same right modulus some conditions but largely it should remain the same right unless you change something in between"}
{"audio_filepath": "Data/Preprocessed/Recap: Learning Parameters: Guess Work, Gradient Descent_33.wav", "duration": 506.0, "text": "welcometo lecturefive of the courseon deeplearningand so todaywe look atsome variantsof gradientdescentso we will just quicklydo arecapof gradientdescentand then look atsomevariantsof it or somewaysof improvingit whichis momentum basedgradientdescent nesterovof acceleratedgradientdescent stochasticgradient descent adagrad rmsprop and adam so just to set the contextso we started with thi s gra dient desce nt algorith m for a single sigmoidneuron andthen wesawhow to extendto networkof neuronswith back propagationso we realizedthat allwe needis thegradientsor thepartialderivatives with respec t to all the weig hts a nd biases once we compute that we ca n just use the gradient descent update rule now today what we are going to see is are there better update rules which lead to faster conversion or better performance in various ways so that is why we are going to look at all these differe nt varian ts or methods of improving on gra dient descent s o that is the context re fer slide time oneeighteen i will just quickly rush through  so for most of the lecture i have borrowed ideas from the videos by ryan harris on visualize back propagation and some content is based on this course by andrej karpathy and others when i talk a bout some tips for learning rate and so on  so yo u can just look at those also so we will just quickly rush through the first two modules which we have already done refer slide time onefortysix w hich was we were interested in learning the weights and b iases for this very toy network with just one input and one output and we started by doing something known as guesswork where we were just trying to adjust these weights and biases by hand refer slide time onefiftytwo refer slide time onefiftysix refer slide time onefiftyeight a nd we realize d that its clearly not good and  but we still try to do a very smart guess work where we were driven by this loss function  which was telling us whether this guess the current guess is better than the previous guess or not a nd we just kept following our guess work and try to reach to some solution and for this toy network it was very easy to do that refer slide time twosix refer slide time twoseventeen a nd what we were actually doing is there is this error surface which exists which can be plotted for all possible values of w comma b and what we were trying to do with this guesswork is trying to find path over the error surface  so that we enter into the better regions  so red is bad blue is good  the darker the sha de of blue the better and this of course becomes intractable when you have many parameters and so on refer slide time twothirtyeight so we wanted to have a better way of navigating the error surface  so this is exactly what we were doing with the guesswor k algorithm refer slide time twofortyeight so then this better way actually we realized that we could arrive at it from a very principled solution from starting from taylor series refer slide time twofiftyfive a nd we went to this derivative where we finall y came up with this rule that move in the direction opposite to the gradient refer slide time twofiftyeight refer slide time threethree so that is the rule that we have been sticking to since then and we also along the way realize some of these things whic h we defined carefully which was what is what exactly this quantity means which is the partial derivative with respect to w evaluated at a particular weight comma bias configuration a nd because this is an iterative process you are at a certain value o f weight and bias and you need to change it from there refer slide time threethirty a nd we then created an algorithm out of this and when we ran this we actually derived the full derivative and so on refer slide time threethirtyeight a nd then when we finally ran this algorithm  so this is where now i will slow down so when we ran this algorithm  so let us see what was happening here right  so i will just start the alg orithm from the beginning so we are now going to run this code and you tell me somethi ng that you observe ok so i am just clicking  so there is no change in the pace at which i am clicking this right so every click of this is one time step and i am just continuously clicking this i will start now do you observe something  fl  ok do y ou observe something it was initially slow then suddenly picked up and then it again became slow why did this happen the slope is small why ok how many of you completely understand why this slow and fast moment was there please raise your hands good so that is what we will focus on now right so we will try to see this refer slide time fourthirty so we will i hope this has been fixed ok  so let us take a simple function which is f of x equal to x square plus one right this is how it will look lik e now in these portions of the curve the curve is actually very steep right and in these portions the curve is a bit gentle and of course it becomes very gentle over here right all of you can see the pen marks properly so now let us see what this mea ns this steep and fast and small  so let us look at a region which is steep ok now what i am going to do is i am going to change my x by one i move my x from one to two how much did my y change all you need to do is just substitute in this formula right f or two it evaluates to five for one it evaluates to two  so when you move from one to two your function changed from two to five ok s o there is a large change in the function for one unit change in your value of x everyone sees that now let me do the same at a gentle portion of the curve i will do it here now when i changed the x by one unit again one unit right it i s the same change which i did earlier i changed from zero to one how much did my y change s tudent one one ok now actually what is this quantity delta y one b y delta x one s tudent slope i t i s the slope it i s the derivative at that point  so what are you inferring from this what happens to the derivative when you are at steep slopes s tudent it i s high d erivative is high because the change in y is much fa ster than the change in x what happens to the derivative when you are at the gentle slopes s tudent smaller s maller because the change in y is small or relatively smaller as compared to the change in x or it could also be missing  but just these two ar e relatively different is what i am trying to impress upon right and so  that means the derivatives at the steep slopes are larger in magnitude whereas for the gentle slopes they are smaller in magnitude now can you relate it to the observation that you had on the previous slide when we were at the plateau it was a very dash slope gentle slope what would the derivatives be s tudent small s mall now what are our updates you have w is equal to w minus the derivative right now the derivative is smal l what will happen to the updates s tudent small t hey will be small what would happen if the derivative is large s tudent the updates would be large t he updates would be large therefore in the gentle areas you are moving slowly and in the steep area s you are moving fast you get this picture very clearly now this is going to be the basis of a lot of things that we do today  so it i s very essential to that you understand this perfectly all of you get this properly good refer slide time seventhirtytwo now now you might say that this was only that special point again and i always get those questions  so let us see what happens if you start from a different point refer slide time sevenforty so now again the same gradient descent algorithm i am going to run  but instead of starting at this point which was my random initialization i just happened to choose a very different random initialization which is here everyone sees that now let us see what happens what do you expect initially fast movement because the steep the slope is a bit steep now what would happen i t will become slow because you have entered a gentle slope region and then again fast right  so and then again it will become slow so see in this gentle region right the changes in w are so small that all your black points are actually indistinguishable from each other it i s almost like a snakes body w hereas in these steep slopes you can see a large change in the w you can see gaps between the values of w right so this is irrespe ctive of where you start from gentle means slow movement steep means fast movement that is the basis"}
{"audio_filepath": "Data/Preprocessed/Relation between SVD and Word2Vec_85.wav", "duration": 211.0, "text": "nowand late r on actually the same guy s they a lso came up with thi s forma l relation between svd and wordtwovec whic h is again under some assumptions but i am not going to do the proof here i am just going to give you the intuition so recall that svd does a matrix factor ization of the co occurrence matrix levy et al showed that wordtwovec also does such a implicit matrix factorization so what does this mean so recall that wordtwovec gives us w context and w word it gives us these two parameters so they say that there exi st a matrix m such that ok this is wrong just be the product of two matrices right this is the product of two matrices it should be w context transpose w word or just see which way the transpose should be so it is actually a product of these two matrices t hat we have learnt ok and what is m m is actually nothing but the pmi matrix minus this log k where does the k come from what was k the negative samples that you have taken so  they actually showed that whatever representations wordtwovec runs it is ac tually doing a factorization of this matrix where this matrix has a strong connection to the pmi matrix and sv d also w orks with the pmi matrix if you take svd matrix and do these modifications to it that means you take every value which is the pmi and then subtract this log k from that and then just do an svd of that you will essentially get back the same word representations as wordtwovec there was some certain assumptions made in the paper but that is i mean i do not want to go into those but the key idea here is that you can actually show that svd and wordtwovec are actually connected and if you think about it at an intuitive level though these methods are relying on the same underlying principle that words appear together based on that the wor d representations get updated or in svd based on there the counts get updated and you then eventually end up with certain representation next the underlying principle is the same so there has to be a connection right it is not that they are doing fundam entally something different both of them are relying on the idea of co occurrence or the idea of distribution right so they have to at some leve l be similar in some ways right so that is what they finally showed and so now but still in most applica tions wordtwovec is preferred so one reason for that is that this is an iterative training procedure right as compared to svd and i come back to your question right how do you do that how do you compute the eigenvectors of x transpose x and the answe r is there is no simple way of doing that and you have to do that expensive matrix multiplication and then rely on various very smart libraries for computing the eigenvectors which are still order n raise to two point something or something like that they not order n cube but they are still order n raise to two point something means they are still expensive and then of course you have this memory issue that if you have a very large vocabulary your pmi matrix is going to be very high dimensional and then you need to do the factorization of that high dimensional vectors so that runs into these computational efficiency issues on the other hand wordtwovec by design is an iterative algorithm because you are going to grade gradient descent which is that ev ery times that you are going to update some parameters of the model you are not learning all the parameters together you are only dealing with some parameters at every time set right so that is more computationally efficient especially if you do the contrastive divergents or the negative sampling or the hierarchal sample so  that is why perhaps it is still more popular than svd"}
{"audio_filepath": "Data/Preprocessed/Biological Neuron_1.wav", "duration": 373.0, "text": "hello everyone welcome to lecture one of csseven fifteen which is the course on deep learning intodays lecture is going to be a bit non technical we are not going to cover any technical conceptsorwe only going to talk about a brief or partial history of deep learning so we hear the terms artificial neural networks artificial neurons quite often these days and i just wanted you take you through the journey of where does all these originate from and this history contains several spans across several fields not just computer science we will start with biology then talk about something in physics then eventually come to computer science and so on so with that let us start so just some acknowledgments and disclaimers i have taken lot of this material from the first people which i have mentioned on the bullet and there might still be some errors because its dates as back as one thousand eight hundred and seventyone so maybe i have got some of the facts wrong so feel free to contact me if you think some of these portions need to be corrected and it would be good if you could provide me appropriate references for these corrections so let us start with the first chapter which is on biological neurons as i said its spans several fields will start with biology and we will first talk about the brain and neurons within the brain so way back in one thousand eight hundred and seventyone one thousand eight hundred and seventythree around that time joseph von gerlach actually proposed that the nervous system our nervous system is a single continuous network as opposed to a network of many discrete cells so his idea was that this is one gigantic cell sitting in our nervous system and it is not a network of discrete cells and this theory was known as the reticular theory and around the same time there was the some breakthrough or some progress in staining techniques where camillo golgi discovered that a chemical reaction that would allow you to examine the neurons or the nervous tissue so he was looking at this nervous tissue using some staining technique and by looking at what you see in this figure on the right hand side the yellow figure even he concluded that this is just once single cell and not a network of discrete cells so he was again a proponent of reticular theory so this is about camillo golgi and then interestingly santiago cajal he used the same technique which golgi proposed and he studied the same tissue and he came up with the conclusion that this is not a single cell this is actually a collection of various discrete cells which together forms a network so it is a network of things as opposed to a single cell there so that is what his theory was and this was eventually came to be known as the neuron doctrine although this was not a consolidated in the form of a doctrine by cajal that was done by this gentleman so he coined the term neuron so now today when you think about art hereaboutartificialneuralnetworksorartificialneuronsthetermneuronactually originated way back in one thousand eight hundred and ninetyone and this gentleman was responsible for coining that and he was also responsible for consolidating the neuron doctrine so already as you saw on the previous slide cajal had proposed it but then over the years many people boughtthisideaandthisguy wasresponsiblefor consolidatingthatintoa neuron doctrine interestingly he is not only responsible for coining the term neuron he is also responsible for coining the term chromosome so two very important terms were coined by this one person so now here is a question so around one thousand nine hundred and six when it was time to give the nobel prize in medicine what do you think which of these two proponents say there are two theories one is reticular theory which is a single cell and then there is this neuron doctrine which is a collection of cells or collection of neurons that a nervous system is a collection of neurons so what do you think which of these two guys who are proponents of these two different theories who would have got the actual nobel prize for medicine so interestingly it was given to both of them so till one thousand nine hundred and six in fact way later till one thousand nine hundred and fifty also this debate was not completely set settled and then the committee said both of these are interesting pieces of work we yet do not know what really actual what the situation is actually but these conflicting ideas have a place together and so the nobel prize was actually given to both of them and this led to a history of a some kind of controversies between these two scientists and so on and this debate surprisingly was settled way later in one thousand nine hundred and fifty and not by progress in biology as such but by progress in a different field so this was with the advent of electron microscopy so now it was able to see this at a much better scale and by looking at this under a microscope it was found that actually there is a gap between these neurons and hence it is not a one single cell it is actually a collection or a network of cells with a clear gap between them or some connections between them which are now known as synapses so this was when the debate was settled so now why am i talking about biology why am i telling you about biological neuron and so on so this is what we need to understand so there has always been interested in understanding how the human brain works from a biological perspective at least and around this time the debate was more or less settled that we have this our brain is a collection of many neurons and they interact with each other to help us do a lot of complex processing that we do on a daily basis right from getting up in the morning and deciding what do we want to do today taking decisions performing computations and various complex things that our brain is capable of doing now the interest is in seeing if we understand how the brain works can we make an artificial model for that so can we come up with something which can simulate how our brain works and what is that model and how do we make a computer do that or how do we make a machine do that so that is why i started from biological neurons to take the inspiration from biology"}
{"audio_filepath": "Data/Preprocessed/ From Spring to Winter of AI_2.wav", "duration": 779.0, "text": "we will start talking about artificial intelligence and this is titled as from the spring to the winter of ai so i am going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what eventually happened to the initial boom and so soletus startwithone thousand nine hundred and fortythreewhereasisayingthattherewasalotofinterestin understanding how does a human brain work and then come up with a computational oramathematicalmodelofthatsomccullochandpittsoneofthemwasa neuroscientist and the other one was a logician no computer scientists or anything at that point of time and they came up with this extremely simplified model that just as a brain takes a input from lot of factors so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be our factor xone you could think about is a weather good to go out is it raining would it be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on so being kind of presses all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that i want to probably go for a movie so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with the binary decision right so this is what they proposed in one thousand nine hundred and fortythree so now we have come to an artificial neuron so this is not a biological neuron this is how you would implement it as a machine right so that was in one thousand nine hundred and fortythree then along and then thiskind of ledto a lot of boom in our interestin artificial intelligence and so on and i guess around one thousand nine hundred and fiftysix in a conference the term artificial intelligencewasaformallycoinedandwithinaoneortwoyearsfromtherefrank rosenbergcameupwiththisperceptronmodelofdoingcomputationsorwhat perceptron model of what an artificial neuron could be and we will talk about this in detail later on the course and not tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages do you find anything odd about this statement yeah so learn and make decisions make sense but why translate languages why is so specific why such a specific interest in languages so that you have to connect back to history so this is also the period of the cold war and there was always always a lot of interest there was lot of research and translation was actually fuelled by the world war and evens that happened after that w here these countries which were at loggerheads with each other they wanted to understand what the other country is doing but they did not speak each others language that is why there was a lot of interest from espionage point of view or from spying and so on to be able to translate languages and hence that specific require and lot of this research would have been funded from agencies which are interested in these things right and the defence or war or something so and this work was largely done for the navy and this is an this is an extract from the article written in new york times way back in one thousand nine hundred and fiftyseven or fiftyeight where it was mentioned that the embryo often this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of it is existence so i am not quoting something from two thousand and seventeen or eighteen this is way back in one thousand nine hundred and fiftyseven fiftyeight why i am that is why i like the history part of it so recently there is a lot of boom or a lot of hype around ai that ai will take over a lot of things will take our jobs it might eventually we might be colonized by ai agents and so on so i just want to emphasize that i do not know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one thousand nine hundred and fiftyseven one thousand nine hundred and fiftyeight right this not something new that people are talking about now it is always been there and to what extent this promise will be fulfilled is yet to be seen and of course as compared to one thousand nine hundred and fiftysevenfiftyeight we have made a lot of progress in other fields which have enabled ai to be much more successful than it was earlier for example we have much better compute power now we have lots of data now and all thanks to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from it so we have made a lot of progress in other aspects where which ai is now at a position where it can really make a difference but just wanted to say that these are not things which i have not been said in the past it has always been the it has always been considered to be very promising and perhaps a bit hyped also so that is about one thousand nine hundred and fiftysevenfiftyeight then now what we talk about what is all the for the past eight to ten years at least when we talk about ai talking about deep learning and that is what this course i s about largely about deep learning i am not saying that other and what deep learning is largely about if i want to tell you in a very layman nutshell term is it is about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal so this is like a schematic of what a deep neural network or a feed forward neural network would look like now this is again not something new which is up in the last eight to ten years although people have started discussing it a lot in the last eight to ten years look at it way back in one thousand nine hundred and sixtyfive sixtyeight opposed something which looked very much like a modern deep neural network or a modern feed forward neural network and in many circles he is considered to be one of the founding fathers of modern deep learning so that is about the springtime for ai and what i mean by that that everyone was showing interest in that the government was funding a lot of research in ai and people really various applications health care defence and so on and then around one thousand nine hundred and sixtynine an interesting paper came out by these two gentlemen minsky and papert which essentially outlined some limitations of the perceptron model and we will talk about these limitations later on in the course in the second or third lecture but for now i will not get into a details of that but what it is said that it is possible that a perceptron cannot handle some very simple functions also so you are trying to make the perceptron learn some very complex functions b ecause the way we decide how to watch a movie is a very complex function of the inputs that we considered but even a simple function like xor o r is something which a perceptron cannot be used to model that is what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and lot of government funding actually subsided after one thousand nine hundred and sixtynine all the way to one thousand nine hundred and eightysix actually this was the ai winter of connectionism so there was very little interest in connectionist ai so there are two types of ai one is symbolic ai and the other is connectionist ai so whatever we are going to study in this course about neural networks and all that probably falls in connectionist ai paradigm and there was no interest in this and people i mean hard to get funding and so on for these seventeen to eighteen years and that was largely triggered by this study that was done by minsky and papert and interestingly they were also often misquoted and what they had actually said in that papers so they had said a single perceptron cannot do it t hey in fact said that a multi layer network of perceptrons can do it but no one focused on the second part that a multilayer network of perceptron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on right so that is what happened for a long time and this known as the winter the first winter then around one thousand nine hundred and eightysix actually came this algorithm which is known as back propagation again this is an algorithm which we are going to cover in a lot of detail in the course in the fourth or fiveth lecture and this algorithm actually enables to train a deep neural network right so deep network of neurons is something that you can train using this algorithm now this algorithm was actually popularized by at rumelhart and others in one thousand nine hundred and eightysix but it is not completely discovered by them this was also around in various other fields so it was there in i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rumelhart other and others in one thousand nine hundred and eightysix were the first to kind of popularize it in the context of deep neural networks and this was a very important discovery because even today all the neural network so most of them are trained using back propagation right and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network right so something this was discovered almost thirty years back is still primarily used for training deep neural networks that is why this was a very important paper or breakthrough at that time and aroundthesametime so againinterestingly so backpropagationisusedin conjunction with something known as gradient descent which was again discovered way back in one thousand eight hundred and fortyseven by cauchy and he was interested in using this to compute the orbit of heavenly bodies that is something that people care about at that time today of course we use it for variousotherpurposesone of thembeing discoveringcatsandvideosor evenfor medical imaging or for describing whether certain have of cancer is being depicted in a xray or things like that there is a lot of other purposes for which deep neural networks enhance and hence back propagation gradient descent and other things are being used for it but again these are not very modern discoveries these are dated way back thirty years and even gradient descent is almost one hundred and fifty years and so on so that is what i wanted to emphasize and around the same time in one thousand nine hundred and ninety or one thousand nine hundred and eightynine there is this another interesting theorem which was proved which is known as the universal approximation theorem and this is again something that we will cover in the course in the third lecture or something like where we will talk about the power of a deep neural network so again the importance of this or why this theorem was important will become clear later and when we cover it in detail but for now it is important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision so what it means in very layman terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many laymen terms that is what it means and if i have to hype it up a bit or i have to say it in a very enthused and excited manner i would say that basically it says that deed neural networks can be used for solving all kinds of machine learning problems and that is roughly what it says but with a pinch of salt and a lot of caveats but that is what it means at least in the context of this course sothisisallaroundone thousand nine hundred and eightynineanddespitethishappeningsomeimportantdiscoveries towardsthelateendofeightyswhichwasbackpropagationuniversalapproximation theorem people were still not being able to use deep neural networks for really solving large practical problems and a few challenges there was of course the compute power at that time was not at a level where it could support deep neural networks we do not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural network so when people try it training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation off the shelf at that time because of which it was not very successful so again despite these slight boom around eightysix to ninety where some important discoveries were made and even follow up in ninetytwo ninetythree and so on t here is still n ot a real big hype around deep neural networks or artificial neural networks and at time again a slump a slow winter right up till two thousand and six"}
{"audio_filepath": "Data/Preprocessed/The Deep Revival_3.wav", "duration": 433.0, "text": "when this deep revival happened so in two thousand and six a very important study was or a very important contribution was made by hinton and salakhutdinov sorry if i have not pronounced it properly and they found that a method for training very deep neural network effectively now again the details of these are not important we will be doing that in the course at some point but what is the important take away here is that while from one thousand nine hundred and eightynine to two thousand and six we knew that there is an algorithm for training deep neural networks and they can potentially be used for solving a wide range of problems because that is what the universal approximation theorem saidbut the problem was that in practice we were not being able to use it for much it was not easy to train these networks but now with this technique there was revived interestand hope thatnow actuallycan trainvery deep neuralnetworks for lot of practical problems this sparked off the interest again and then people started looking at all such of thing right that even this particular study which was done in two thousand and sixwill actually be very simple to something done way back in ninetyoneninetythree and which again showed that you can train a very deep neural network but again due to several factors may be at that time due to the computational requirements or the data requirements or whatever i am not too sure about that it did not become so popular then but by two thousand and six probably the stage was much better for these kind of networks or techniques to succeed so then it became popular in two thousand and six thenthistwo thousand and sixtotwo thousand and ninepeoplestartedgainingmoreandmoreinsightsintothe effective ness of this discovery made by h inton and others which is unsupervised pre trainingrightthatiswhat i spoke about on thepreviousslideunsupervised pre training and they started getting more and more insights into how you can make deep neural networks really work so they came up with various techniques some of which we are going to study in this course so this was about how do you initialise the network better whatisthebetteroptimizationalgorithmtousewhatisthebetterregularization algorithm to use and so on so there were many things which were started coming out at this period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep neural networks of artificial neural networks started becoming popular that is when people realised that all this all the negative things that were tied to it that you are not able to train it well and so on have slowly p eople have started finding solutions to get by those and maybe we should start again focusing on the potential of deep neural networks and see if they can be used for large scale practical application  so this two thousand and six to two thousand and nine was again a slow boom period were people were again trying to do a lot of work to popularize deep neural networks and get rid of some of the problems which existed in training them now from two thousand and nine onwards there was this series of success is which kind of caught everyone which made everyone to stand up and take notice right that this is really working for a lot of practical applications starting with handwriting recognition so around two thousand and nine these guys won handwriting recognition competition in arabic and they did way better than the competitor systems using a deep neural network a nd then this was a success so this was an handwriting recognition and then there was speech so this shown that variousexistingsystemstheerrorrateofthesesystemcouldbeseriously be significantlyreducedbyusingdeepneuralnetworksorplugginginadeepneural network component to existing systems right so this was handwriting and then speech then again some kind of pattern recognition which was on hand written digit recognition for mnist this is a very popular data set which had been around since ninetyeight and a new record was set on this data so this is the highest accuracy that was achieved on this data set around that time in two thousand and ten sorry and this is also the time when gpus entered the same so before that all of the stuff was being done on cpus but then people realised that very deep neural networks require a lot of computation and lot of this computation can happen very quickly on gpus as opposed to cpus so people started using gpus for training and that drastically reduced the training and inferencetime so that wasagainsomethingwhich sparked a lot of interestright because even though these were successful they were taking a lot of time to train b ut now the gpu s could even take care of that a nd this success continued so peoplestartedgainingorgettingsuccessinotherfieldslikevisualpattern recognition so this was a competition on recognising traffic sign boards and here again a deep neural network did way better than its other competitors and then the most popular or one thing which made neural networks really popular was this i mage net challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when thisalexnetwasone of theparticipatingsystemsinthiscompetitionmost of the systems were non neural network based systems and this competition was basically about classifying a given image into one of thousand classes so this could be an image of a bird or a dog or a human or car truck and so on say you have to identify the right class of the main object in the image  so i n two thousand and twelve this alexnet which was a deep neural network or a convolutional neural network based system was able to actually outperform all the other systems by a margin of sixtyseven percent so the error for this system was sixteen percent and this is a deep neural network because it had eight layers the next year this was improved further and something known as zf network propose which was again eight layers but it did better than alexnet t he next year even a deeper network with nineteen layers was proposed which did significantly better than alexnet t hen google entered the scene and they proposed something which is twentytwo layers and again reduced the error t hen microsoft joined in and they proposed something which had one hundred and fiftytwo layers and the error that you see here is actually better than what humans do so even if a human was asked to label this image because of certain law certain noise in the image and so on even a human is bound to make more errors than threesix per cent  that means even if you show hundred images to humans he or she is bound to may go wrong or more than three or four of these images right there is this system was able to get an error of threesix per cent over the large test set so this two thousand and twelve to two thousand and sixteen period were there was this continuous success on the image net challengeaswellassuccessesinotherfieldslikenaturallanguageprocessing h andwriting r ecognition speech and so on so this is the period where now everyone started talking about deep learning and lot of company started investing in it a lot of traditional systems which were not deep neural network based was now started people started converting them to deep neural network based system so translation system speed systems image classification object detection and so on therewerelotofsuccessinallthesefieldsusingdeepneuralnetworks andthis particular thing that we are talking about which is image net and the success in this was driven by something known as convolutional neural networks"}
